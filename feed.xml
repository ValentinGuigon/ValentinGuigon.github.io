<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://valentinguigon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valentinguigon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-11T03:52:56+00:00</updated><id>https://valentinguigon.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Valentin Guigon. </subtitle><entry><title type="html">Intelligence is easy; cognition is hard</title><link href="https://valentinguigon.github.io/posts/2026-01-24-2026-01-24-intelligence-is-easy-cognition-is-hard/" rel="alternate" type="text/html" title="Intelligence is easy; cognition is hard"/><published>2026-01-24T18:10:41+00:00</published><updated>2026-01-24T18:10:41+00:00</updated><id>https://valentinguigon.github.io/posts/2026-01-24-intelligence-is-easy-cognition-is-hard</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2026-01-24-2026-01-24-intelligence-is-easy-cognition-is-hard/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Distinguishing intelligent systems from cognitive systems]]></summary></entry><entry><title type="html">The mind, the brain and the network</title><link href="https://valentinguigon.github.io/posts/2025-10-14-2025-10-14-the-mind-the-brain-and-the-network/" rel="alternate" type="text/html" title="The mind, the brain and the network"/><published>2025-10-14T03:01:14+00:00</published><updated>2025-10-14T03:01:14+00:00</updated><id>https://valentinguigon.github.io/posts/2025-10-14-the-mind-the-brain-and-the-network</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-10-14-2025-10-14-the-mind-the-brain-and-the-network/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[What's modular? Learning under topological constraints]]></summary></entry><entry><title type="html">Can machines think?</title><link href="https://valentinguigon.github.io/posts/2025-09-06-2025-09-06-can-machines-think/" rel="alternate" type="text/html" title="Can machines think?"/><published>2025-09-06T20:36:59+00:00</published><updated>2025-09-06T20:36:59+00:00</updated><id>https://valentinguigon.github.io/posts/2025-09-06-can-machines-think</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-09-06-2025-09-06-can-machines-think/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Assessing machine vs human cognition]]></summary></entry><entry><title type="html">Some forecasting on the next decade of Neuroscience</title><link href="https://valentinguigon.github.io/posts/2025-08-27-2025-08-27-some-forecasting-on-the-next-decade-of-neuroscience/" rel="alternate" type="text/html" title="Some forecasting on the next decade of Neuroscience"/><published>2025-08-27T23:10:23+00:00</published><updated>2025-08-27T23:10:23+00:00</updated><id>https://valentinguigon.github.io/posts/2025-08-27-some-forecasting-on-the-next-decade-of-neuroscience</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-08-27-2025-08-27-some-forecasting-on-the-next-decade-of-neuroscience/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Based on The BRAIN Initiative 2025 Report and the BRAIN Initiative August 2025 Meeting]]></summary></entry><entry><title type="html">AI Agents in the Lab: Concept Paper</title><link href="https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper/" rel="alternate" type="text/html" title="AI Agents in the Lab: Concept Paper"/><published>2025-05-08T11:22:03+00:00</published><updated>2025-05-08T11:22:03+00:00</updated><id>https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper/"><![CDATA[<figure><img alt="There are 16 different small square images in this collage. Each of them have a grid background and different neon coloured square patterns in the squares. One looks like a flower, another looks like a random array of pixels, and the other is 4 blue squares." src="https://cdn-images-1.medium.com/max/1024/1*ThUOQjc9vYzhBkTlGomqMw.jpeg"/><figcaption>Elise Racine / <a href="https://betterimagesofai.org">https://betterimagesofai.org</a> / <a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></figcaption></figure> <ul> <li>Why This Matters</li> <li>When Is It Time</li> <li>Blueprint</li> <li>Use Case 1: Research Discovery &amp; Critique</li> <li>Use Case 2: Multi-Agent Simulation Sandbox</li> <li>Use Case 3: Research Documentation</li> <li>Use Case 4: Reproducibility Audit &amp; Open Science Automation</li> <li>Use Case 5: Research Collaboration &amp; Memory Assistant</li> <li>What This Means for Research</li> <li>Now</li> </ul> <p>Modern academic research faces structural inefficiencies that hinder quality, slow progress, and exacerbate inequities between large, well-funded labs and smaller, resource-constrained ones. Reproducibility remains a persistent crisis, while high cognitive load, bureaucratic demands, and fragmented tools siphon time away from high-level reasoning.</p> <p>As in any domain shaped by financial pressure, limited time, and high competition, AI agents have the potential to directly alleviate these structural burdens. At the same time, modern research demands constant high precision. Human error is the most common source of slowdown, inconsistency, and missed insight whenever repetition is at the heart of a process. On the other hand, <a href="https://arxiv.org/pdf/2412.14161">benchmarks</a> show how far from sufficient quality AI agents achieve on complex processes. Instead, by offloading non-critical, simple but essential tasks, agentic systems should increase both the clarity and the reliability of scientific output.</p> <p>Specifically, AI agents and multi-agent systems (MAS), when properly orchestrated, would optimize a meaningful portion of research pipelines, flag methodological flaws early, and promote consistent documentation and quality control.</p> <p>These systems would lower the barrier to high-quality science. Smaller labs and early-career researchers, who often lack institutional support or internal review infrastructure, would be able to compete more equitably in a publish-or-perish ecosystem. The end goal is to increase both speed and quality, while leaving the trade-off between them to humans : more precision, less friction, faster iteration.</p> <p>In this document I outline use cases for integrating AI agents and MAS into academic research workflows. Each use case is structured around functional value and estimated technical feasibility. Specifically, the use cases are grounded in the context of computational and cognitive neuroscience, where these perspectives are most familiar to me. These examples should be understood as <strong>conceptual designs</strong>, intended to guide real-world lab experimentation.</p> <p>While MAS are promising, their current capabilities come with limitations. They are not yet mature enough for full task automation. However, they can already provide valuable assistance, particularly in structured environments where feedback-based correction allows for progressive refinement or post-hoc corrections. In this role, they function as <strong>intelligent scaffolds</strong>: supportive systems that enhance repetitive work prior to human oversight.</p> <p>As emphasized on each use case feasibility, Now is time for small-scale use cases; Later is time for complex use cases.</p> <p>Each section below describes a use case, the corresponding implications, the agentic solution (tools, tasks, agents, workflow), sample outputs, estimated weaknesses and recommendations, and the estimated feasibility.</p> <p>Each use case follows a nine-part structure:</p> <ol> <li>Define a) research needs that can be addressed and b) their implications for the practices.</li> <li>Propose a high-level pipeline composed of a) agents, b) tools, c) decomposed tasks and d) workflows.</li> <li>Assess a) potential weaknesses and b) suggest recommendations.</li> <li>Estimate feasibility based on current tools.</li> </ol> <p>I emphasize that this is a tentative, conceptual work, meant to guide future experiments, and that my expertise lies primarily in cognitive and neurocomputational sciences.</p> <p>General recommendations</p> <ul> <li> <strong>Use Structured Schemas (Pydantic)</strong>: Ensure all outputs (reports, metadata, environments) are structured and machine-parseable. This supports integration into continuous review pipelines and version tracking.</li> <li> <strong>Robust Workflow Orchestration</strong><br/> Use DAG-style control flows (e.g., LangGraph) to manage iteration, branching, and error handling for complex crews.</li> <li> <strong>Human-in-the-Loop Validation</strong><br/> Periodically calibrate agent behavior against real subject data to avoid overfitting to flawed prompt assumptions.</li> </ul> <p><strong>Purpose</strong></p> <p>Automate high-quality literature discovery and structured critique of scientific papers — assessing theoretical alignment, modeling validity, methodological bias, funding relevance, and ethical risks. This system will be <strong>augmentative</strong>, not replacing human experts. <strong>Internal documentation</strong>, including scientific papers, must be heavily leveraged to ensure the critiques and evaluations align with established research standards.</p> <p><strong>Implications: </strong>Faster, structured literature reviews; improved reproducibility via consistent critique schemas; early detection of theoretical or methodological flaws; template-driven alignment and drafting support.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Enforces structured schemas for outputs.</li> <li> <strong>External APIs</strong>: SerpAPI, PubMed, CrossRef, Semantic Scholar API for multi-source academic retrieval.</li> <li> <strong>Internal Documentation</strong>: Internal scientific documents, guidelines, style guides and domain-specific knowledge bases to ground evaluations in domain expertise.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Search &amp; Retrieve</strong>: Query APIs and retrieve relevant papers using natural-language queries and filters.</li> <li> <strong>Indexing &amp; Semantic Search</strong>: Build vector indices from retrieved papers and perform RAG-based retrieval to enhance search relevance.</li> <li> <strong>Classification</strong>: Tag documents by field, method, population, and relevance.</li> <li> <strong>Summarization</strong>: Extract structured summaries with citations and metadata, ensuring completeness but with limitations.</li> <li> <strong>Multi-Lens Critique</strong>: Run modular evaluation passes (theoretical validity, compliance risks, modeling rigor, demographic bias, etc.).</li> <li> <strong>Funding Relevance</strong>: Assist in grant application preparations by matching paper topics to relevant funding calls using past grant data and basic NLP.</li> </ul> <p><strong>Agent</strong></p> <ul> <li> <strong>Semantic Retriever</strong>: Interfaces with APIs and vector stores to surface relevant literature.</li> <li> <strong>Classifier</strong>: Labels documents based on methodology, field, and population studied.</li> <li> <strong>Summarizer:</strong> Extracts structured summaries with citation metadata.</li> <li> <strong>Synthesizer</strong>: Generates structured outputs, focusing on summarizing themes, key findings, and proposal elements.</li> <li> <strong>Modeling Critic:</strong> Assesses alignment of model with theoretical hypotheses or inappropriate model usage (theoretical alignment checklists, not full critique).</li> <li> <strong>Bias Auditor</strong>: Recommends analysis strategies (e.g., simulations, effect size calculations, power analysis) to assess bias and improve model robustness.</li> <li> <strong>Ethics &amp; Compliance Critic:</strong> Flags potential IRB, GDPR, and dual-use risks based on internal policy documents and rule-based checks.</li> </ul> <p><strong>Workflow Structure</strong></p> <ul> <li>CrewAI delegates task-specific agents across a modular critique pipeline.</li> <li>Optional chaining (e.g., LangGraph) routes papers between agents based on content type.</li> <li>State memory passes outputs (e.g., summaries, classifications) across stages.</li> <li>Pydantic schemas enforce structure for all agent outputs.</li> <li>Outputs are reviewed and refined iteratively using feedback from researchers.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li>`.bib`: BibTeX with relevance tags and critique annotations.</li> <li>` md`: Structured markdown synthesis and summaries.</li> <li>`.json`: Machine-readable JSON objects of evaluations and labels.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Automated Critique Limitations</strong>: AI cannot yet replace domain experts in assessing deep theoretical soundness. Critiques may be shallow or inconsistent.</li> <li> <strong>Retrieval Limitations</strong>: External APIs have incomplete coverage and rate limits. Missing full texts can degrade critique quality.</li> <li> <strong>Classification Accuracy</strong>: Automated tagging can misclassify boundary cases.</li> <li> <strong>Workflow Complexity</strong>: Multi-agent memory and coordination can create brittle pipelines.</li> <li> <strong>Expectation Management</strong>: Risk of users assuming full automation rather than augmentation.</li> <li> <strong>Prompt Engineering &amp; Schema Validation</strong>: Improper prompt structure can lead to inconsistent outputs.</li> <li> <strong>Internal Document Grounding</strong>: Failure to integrate internal documentation may reduce context accuracy and increase hallucinations.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li>Use checklist-driven critique templates and flagging systems to maintain transparency and guide human review.</li> <li>Prioritize summaries from abstracts and metadata. Integrate internal documents to fill gaps and improve accuracy.</li> <li>Implement hierarchical taxonomies and confidence scoring in classification. Allow manual corrections.</li> <li>Modularize agent roles to localize failure. Implement error logging and fallback mechanisms.</li> <li>Set clear user expectations through documentation and UI cues about AI’s supportive role.</li> <li>Use Pydantic schemas with early validation checks to ensure format integrity.</li> <li>Align all evaluation prompts with internal documentation, policies, and domain knowledge to improve grounding and reduce drift.</li> </ul> <p><strong>Feasibility</strong></p> <p>Many components are already in use individually: semantic search, structured summarization, and rule-based bias detection. Integrating them into a modular, agent-based workflow is feasible with current frameworks (e.g., CrewAI, LangGraph) and good prompt/schema design. Success depends on well-grounded internal documentation, careful agent calibration, and realistic scoping of critique depth. While LLMs can flag obvious issues, domain-specific judgment and iterative refinement are essential to avoid overreliance on superficial outputs. Mostly, systems seem not mature enough to finalize into a full-scope workflow. The entire proposition has a high degree of complexity. Can be broken down into systems of small scope.</p> <p><strong>Purpose</strong></p> <p>Create an <em>in silico</em> environment to simulate subject-like behavior across experimental conditions. The sandbox enables researchers to test and benchmark task designs (decision-making paradigms, game-theoretic setups, social phenomena, maybe abstracted mechanisms) prior to data collection. Simulated data can aid in estimating effect sizes, stress-testing models, and refining behavioral hypotheses. Cognitive modeling could be combined with AI-driven behavior to generate realistic trial-level data.</p> <p><strong>Implications</strong>: Cheap pilot testing, design optimization.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Cognitive modeling libraries </strong>(e.g., custom Bayesian inference engines, reinforcement learning models).</li> <li> <strong>Data validation frameworks </strong>(e.g., Pydantic for schema enforcement).</li> <li> <strong>Data science and visualization </strong>(e.g., pandas, seaborn, matplotlib).</li> <li> <strong>Simulation state stores </strong>(e.g., Redis, or in-memory Python structures).</li> <li> <strong>Notebooks or dashboards </strong>for exploratory analysis and parameter tuning.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li>Generate cognitive profiles with varying priors, learning rates, and strategies informed by explicit computational models or prompt-driven approximations.</li> <li>Execute trial-based simulations where agents perform decisions, produce confidence ratings, reaction times (RTs), and outcomes according to task rules.</li> <li>Record detailed trial logs with structured data schemas.</li> <li>Aggregate and analyze population-level behavior, producing statistical summaries and visual reports.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Persona Generator: </strong>Samples parameter sets from computational models or approximates cognitive traits using structured prompts.</li> <li> <strong>Task Executor:</strong> Performs trial-level decisions using a hybrid approach combining explicit model computations and LLM-generated behavior constrained by structured prompts.</li> <li> <strong>Interaction Logger:</strong> Validates and stores each trial’s outputs and metadata using formal (e.g., Pydantic) schemas.</li> <li> <strong>Results Summarizer:</strong> Generates statistical summaries, behavior plots, and variability estimates.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Simulation is initialized with N synthetic agents.</li> <li>Each agent executes a set of trials under specific experimental conditions.</li> <li>Central state tracks agent properties, task variables, and simulation history.</li> <li>Data is validated and logged continuously.</li> <li>After simulations, analysis agents produce visualizations and reports.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Structured logs (CSV, JSON): </strong>Trial-by-trial behavior with metadata.</li> <li> <strong>Visualizations: </strong>Learning curves, RT distributions, choice matrices, etc.</li> <li> <strong>Summary notebooks: </strong>Auto-generated for post-hoc analysis and reproducibility.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Cognitive Modeling Limitations: </strong>LLMs alone cannot precisely execute complex cognitive models; they approximate behavior based on textual descriptions and tend to suffer from WEIRD biases. Pure prompt-driven agents may produce inconsistencies; detecting the source of variability may not be possible.</li> <li> <strong>State and Interaction Complexity: </strong>Managing multi-agent state across iterative trials is challenging and prone to errors without robust centralized state management. Complex workflows increase architectural overhead and debugging difficulty.</li> <li> <strong>Scalability Constraints: </strong>Large-scale simulations require parallelization and resource optimization not inherently provided by CrewAI or LangGraph.</li> </ul> <p>· <strong>Output Dependence on Model Fidelity: </strong>The quality of logs and analyses depends heavily on the realism of underlying agent behaviors.</p> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Hybrid Modeling Approach</strong><br/> Combine explicit computational cognitive models with LLM-driven behavior generation.</li> <li> <strong>Structured Prompt Engineering</strong><br/> Guide agent responses using prompts that request structured outputs (e.g., JSON), ensuring parsability and reproducibility across trials. Pydantic schemas validate trial logs and agent outputs.</li> <li> <strong>Modular Agent Roles</strong><br/> Maintain separation of concerns (i.e., persona generation, task execution, logging, summarization) to ease debugging and extensibility.</li> <li> <strong>Centralized State Management</strong><br/> Employ memory stores to persist simulation state, agent parameters, and historical trial outcomes across loops.</li> <li> <strong>Parallelize Simulations</strong><br/> Run agent trials in parallel where feasible to boost throughput and scalability.</li> <li> <strong>Automate Statistical Reporting</strong><br/> Leverage Python data science tools (e.g., seaborn, pandas, scipy) to auto-generate behavioral plots and summary statistics for each simulation batch.</li> </ul> <p><strong>Feasibility</strong></p> <p>The approach has already been adopted in the early days of GPT3. Sound experimental designs should today yield data with sufficient quality to stress-test experimental designs (prompt outliers, test unforeseen behavior under the designed rules). While LLMs can approximate (WEIRD-like) model-driven behavior, integrating explicit computational models and robust state management should be tested. Chain-of-Thoughts may prove useful for testing mentalization. Scalability and validation may become challenges. Human oversight is obviously critical.</p> <p><strong>Purpose</strong></p> <p>Automate the generation, auditing, and maintenance of research project documentation, including READMEs, metadata, and code annotations — to enhance reproducibility, usability, and transparency. The goal is to reduce documentation debt while supporting compliance with FAIR and open science standards.</p> <p><strong>Implications</strong>: Higher likelihood of reproducibility; better alignment with data-sharing mandates and publication requirements.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Ensures that outputs like documentation audits, metadata files, and README components follow structured, machine-parseable schemas.</li> <li> <strong>Static Code Parsers</strong>: Tools for extracting function signatures, comments, and code structure (e.g., Python’s ast, inspect).</li> <li> <strong>Natural Language Generators</strong>: Lightweight LLM wrappers for docstring generation and README drafting, fine-tuned for scientific language.</li> <li> <strong>Metadata Validators</strong>: Schema.org and JSON-LD validators, FAIR compliance checkers, and data catalog schema tools.</li> <li> <strong>Version Control Interfaces</strong>: Git hooks and commit parsers to trace documentation lineage and link documentation updates to code changes.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>README Drafting</strong>: Auto-generate a project README including installation, usage, citation, and licensing information.</li> <li> <strong>Function Annotation</strong>: Generate or verify docstrings for functions and classes based on code logic and structure.</li> <li> <strong>File Role Tagging</strong>: Classify source files by role (e.g., data preprocessing, model training) to enhance navigation and metadata clarity.</li> <li> <strong>Metadata Generation</strong>: Produce structured, schema.org-compliant JSON-LD metadata for datasets, workflows, and scripts.</li> <li> <strong>Documentation Consistency Audit</strong>: Evaluate generated documentation for internal consistency and alignment with code behavior or test cases.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>README Composer</strong>: Builds structured project overviews by extracting key project metadata, dependencies, and usage examples.</li> <li> <strong>Code Annotator</strong>: Adds or updates inline docstrings by analyzing code structure and function behavior.</li> <li> <strong>Metadata Generator</strong>: Produces JSON-LD metadata templates, ensuring fields for identifiers, licensing, access, and file provenance.</li> <li> <strong>Documentation Auditor</strong>: Checks whether documentation reflects actual code behavior and tests alignment through symbolic reasoning or simple unit tests.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>The project’s file structure and scripts are parsed to identify documentation targets.</li> <li>Function and module annotations are generated, prioritizing high-level coverage over exhaustive line-by-line summaries.</li> <li>Project-level metadata and READMEs are drafted based on detected scripts, packages, and prior author contributions.</li> <li>Audit agents scan outputs for inconsistencies, missing fields, or misalignments between documentation and code behavior.</li> <li>All outputs are validated using Pydantic schemas and flagged for optional human review before publishing.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Standardized README (Markdown)</strong>: With sections for project purpose, usage, dependencies, author attribution, and citations.</li> <li> <strong>Inline Docstrings</strong>: Inserted or updated within source files, providing structured summaries of function logic, inputs, and outputs.</li> <li> <strong>Metadata Files</strong>: JSON-LD records suitable for FAIR repositories (e.g., Zenodo, Dataverse), including schema.org-compliant descriptions.</li> <li> <strong>Documentation Audit Reports</strong>: Highlighting missing annotations, inconsistencies between code and documentation, or metadata gaps.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Contextual Limits of Automation</strong>: Generated docstrings and READMEs may miss project-specific logic, edge cases, or architectural rationale.</li> <li> <strong>Superficial Metadata</strong>: FAIR metadata often appears valid structurally but lacks meaningful semantics without human input (e.g., vague licensing terms).</li> <li> <strong>Workflow Brittleness</strong>: Without integration into CI/CD pipelines or developer routines, documentation can quickly become outdated as the codebase evolves.</li> <li> <strong>Semantic Inaccuracy</strong>: While schema validation confirms structure, it cannot verify whether documentation truthfully describes code behavior.</li> <li> <strong>Overgeneralization Risks</strong>: LLM-based generators may produce generic or misleading summaries if not fine-tuned on project-specific conventions.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Use Structured Schemas</strong>: All documentation artifacts (README, docstrings, metadata) should be generated to conform to strict templates (validated by Pydantic), enabling downstream reuse and integration into project management pipelines.</li> <li> <strong>Layer Human Review Where Needed</strong>: Use automation for first drafts, but route critical sections (e.g., licensing, ethical use, and dataset sensitivity) to human reviewers before finalization.</li> <li> <strong>Enhance Context Awareness</strong>: Improve accuracy of generated content by incorporating architectural overviews (e.g., flowcharts or design notes) and prior documentation history.</li> <li> <strong>Integrate Documentation with Git</strong>: Trigger doc updates or audits on key commits. Maintain traceability by linking doc artifacts to their source commits or branches.</li> <li> <strong>Validate Documentation with Code Behavior</strong>: Check for inconsistencies (e.g., returns string instead of number) between generated docs and function outputs.</li> <li> <strong>Leverage Provenance Tracking</strong>: Link documentation to specific project phases or data artifacts (e.g., outputs from modeling or preprocessing) using version history and content hashes.</li> </ul> <p><strong>Feasibility</strong><br/> Automating documentation should be highly feasible for standard research project layouts where functions and workflows follow consistent patterns. Most tools required already exist. Challenge is ensuring semantic fidelity and domain alignment. While not a substitute for thoughtful writing, automated drafting and auditing dramatically reduce time and effort. This makes the system well-suited for pre-publication QA checks, onboarding kits, and FAIR submission preparation, otherwise minimal corrections.</p> <p><strong>Purpose</strong></p> <p>Automate the evaluation of research artifacts — code, data, and metadata — for reproducibility, transparency, and FAIR compliance. The system supports researchers in identifying reproducibility gaps, generating repair suggestions, and improving data/code stewardship with minimal manual effort. It does not replace human review but accelerates it through structured, automatable checks.</p> <p><strong>Implications</strong>: Increased transparency of research pipelines; early detection of reproducibility risks; semi-automated compliance with open science mandates.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Enforces structured schemas for outputs such as reproducibility reports, metadata records, and environment specifications.</li> <li> <strong>Static Analysis Libraries</strong>: AST parsers, regex scanners, and linters (e.g., Python’s ast, flake8, black) for code auditing.</li> <li> <strong>Metadata Validators</strong>: JSON schema validators for checking FAIR-aligned metadata (e.g., schema.org, CEDAR templates).</li> <li> <strong>Environment Introspection Tools</strong>: Tools such as pipdeptree, conda list, or Dockerfile parsers for dependency validation.</li> <li> <strong>Command-line Utilities</strong>: Git history analyzers, file checksum tools, and shell scripts for file integrity and version tracing.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Documentation Generation:</strong> Generate READMEs from project structure and code. Auto-generate function-level docstrings. Tag file purposes and interconnections.</li> <li> <strong>Dependency Mapping</strong>: Parse and reconstruct the project’s dependency graph using static analysis of environment files, setup scripts, and imports.</li> <li> <strong>Code Integrity Scanning</strong>: Detect reproducibility risks such as missing seeds, hardcoded paths, non-reproducible code patterns, and undocumented workflows.</li> <li> <strong>Metadata Validation</strong>: Assess the structure, presence, and standard-compliance of metadata files (e.g., schema.org, CEDAR templates).</li> <li> <strong>Workflow Reconstruction</strong>: Extract and sequence computational steps from notebooks, scripts, or workflow managers (e.g., Snakemake), forming a pipeline graph.</li> <li> <strong>FAIR Assessment</strong>: Evaluate the degree of compliance with FAIR principles, including findability (identifiers), accessibility (license fields), and interoperability (linked vocabularies).</li> <li> <strong>Repair Suggestion Generation</strong>: Propose corrective artifacts like Dockerfiles, environment.yaml, or JSON-LD metadata files to address flagged issues and enhance reproducibility.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Static Code Auditor</strong>: Scans scripts for reproducibility anti-patterns (e.g., unseeded randomness, implicit dependencies, or non-deterministic file paths).</li> <li> <strong>Metadata Validator</strong>: Checks presence, structure, and compliance of metadata files with community-accepted schemas.</li> <li> <strong>Workflow Reconstructor</strong>: Parses pipeline scripts or notebooks to infer workflow structure and dependency order.</li> <li> <strong>Reproducibility Synthesizer</strong>: Aggregates findings into structured, human-readable reports with suggestions for repair and improvement.</li> <li> <strong>FAIR Compliance Auditor:</strong> Maps data and metadata to FAIR metrics (e.g., unique identifiers, license clarity).</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Initial file scan identifies code, metadata, and documentation artifacts.</li> <li>Static analysis tools evaluate code structure and dependency graphs.</li> <li>Metadata files are parsed and checked against community schemas.</li> <li>Agents work in sequence or in parallel, validating, flagging, and generating outputs for review.</li> <li>All outputs conform to Pydantic-based schemas, allowing integration with reproducibility dashboards, continuous integration pipelines, or project documentation workflows.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Structured Reports (PDF, Markdown): </strong>Issue summaries (e.g., missing seeds, inconsistent environment specs) and recommended fixes.</li> <li> <strong>FAIR Metadata Templates:</strong> Auto-generated JSON-LD or YAML files aligned with schema.org or CEDAR templates.</li> <li> <strong>Environment Specifications:</strong> Machine-readable requirements.txt, environment.yaml, or Dockerfiles with version-pinned dependencies.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Partial Automation:</strong> FAIR principles like accessibility and ethical reuse still require human interpretation (e.g., licensing, IRB approvals).</li> <li> <strong>Workflow Incompleteness:</strong> Scripts often omit manual preprocessing steps or runtime logic that static tools can’t infer.</li> <li> <strong>Semantic Gaps:</strong> Schema validation can verify structure but not meaning (e.g., a license field may be present but semantically incorrect).</li> <li> <strong>Dependency Resolution Errors:</strong> Without actual environment builds, version conflicts and runtime incompatibilities may go undetected.</li> <li> <strong>False Positives:</strong> Static scans may wrongly flag hardcoded values or randomness that are actually justified.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Use Structured Schemas (Pydantic)</strong>: Ensure all outputs (reports, metadata, environments) are structured and machine-parseable. This supports integration into continuous review pipelines and version tracking.</li> <li> <strong>Modular Agent Roles</strong>: Keep agent responsibilities narrow (e.g., metadata validation vs code structure parsing) to reduce debugging complexity and enable domain-specific extensions (e.g., BIDS for neuroimaging).</li> <li> <strong>Incorporate Human Review Layers</strong>: Supplement automated scans with human checks, especially for FAIR “Accessibility” and data governance requirements.</li> <li> <strong>Track Provenance</strong>: Integrate tools or scripts that log file creation/modification lineage, aiding in data and workflow transparency.</li> <li> <strong>Validate in Real Environments</strong>: Where possible, test environment specifications by building containers or virtual environments. Use their logs as diagnostic tools.</li> <li> <strong>Generate Repair Artifacts</strong>: Suggest corrections, not just critiques. Auto-generate Dockerfiles, README scaffolds, and metadata records aligned with community templates.</li> </ul> <p><strong>Feasibility</strong><br/> Most components are independently mature: code linters, metadata schemas, and environment validators already exist in the open-source ecosystem. The challenge lies in orchestrating them coherently via AI agents, with outputs structured and actionable. Semantic validation and runtime reproducibility remain hard problems, but static analysis alone can surface critical red flags early. Integration with internal reproducibility checklists and domain-specific standards (e.g., neuroimaging BIDS, computational modeling standards) would enhance performance. Usefulness is high even if the system is only partially automated.</p> <p><strong>Purpose</strong></p> <p>Maintain a shared, persistent memory of research progress, decisions, discussions, and evolving plans to enhance team coordination, transparency, and project continuity. This assistant would act as a centralized, accessible repository for all key research milestones, tasks, and updates.</p> <p><strong>Implications</strong>: Mitigating knowledge loss.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Ensures structured validation of input (e.g., meeting summaries, decision logs) and output data (e.g., changelogs, dashboards) using predefined schemas.</li> <li> <strong>Natural Language Processing (NLP) Tools</strong>: Used for extracting and summarizing key points from meeting transcripts, chat logs, and research documentation.</li> <li> <strong>Project Management Integration</strong>: Tools like GitHub, Slack, Notion, Microsoft Teams for integrating meeting notes, code commits, task status, and other project updates. Otherwise Markdown and Word documentation.</li> <li> <strong>Version Control</strong>: To track iterative changes across code, data, and hypotheses.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Communications Summarizer: </strong>Summarize team communications (e.g., meeting transcripts, chat logs, email summaries).</li> <li> <strong>Documentation Archiving: </strong>Archive project decisions, milestones, and revisions in a structured format.</li> <li> <strong>Documentation Updating: </strong>Update and maintain project timelines, research hypotheses, and data evolution records.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Meeting Summarizer:</strong> Extracts key actions, decisions, and unresolved points from transcripts or chat logs.</li> <li> <strong>Task Monitor:</strong> Tracks deliverables, deadlines, and milestone progress.</li> <li> <strong>Project Historian:</strong> Logs revisions and collaborative changes across datasets, code, and hypotheses.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Meeting transcripts, chat logs, and code commits are ingested into the system.</li> <li>Summarization models extract and synthesize key discussion points, decisions, and task updates.</li> <li>All data is structured and validated according to predefined schemas (e.g., MeetingSummary, MilestoneLog).</li> <li>Generated outputs are time-stamped and stored in a shared memory system, accessible through dashboards and changelog files.</li> <li>Critical decisions or ambiguous points are flagged for team member review to ensure accuracy.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Meeting Summaries</strong>: Markdown-based actionable, time-stamped points and decisions that are easy to reference for follow-up.</li> <li> <strong>Changelogs</strong>: Markdown-based changelogs capturing the evolution of hypotheses, datasets, and code changes over time.</li> <li> <strong>Structured Logs</strong>: Compliant with Pydantic schemas, offering machine-readable logs and summaries for automated or external use.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Context Loss in Summarization</strong>: Summarization models may overlook nuanced details, context, or domain-specific language, resulting in incomplete or misinterpreted action items.</li> <li> <strong>Dynamic Memory Management</strong>: Maintaining a coherent, up-to-date shared memory that accurately captures evolving decisions and hypotheses can be difficult to manage, especially as the project grows.</li> <li> <strong>Integration Complexity</strong>: Combining data from diverse sources (e.g., meeting transcripts, chat logs, commits) into a unified timeline requires robust tools and processes.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Human-in-the-Loop Validation</strong>: Incorporate a step for team members to review and refine AI-generated summaries and task updates to ensure accuracy. This can be achieved by flagging high-priority content for team review or approval.</li> <li> <strong>Domain-Specific Fine-Tuning</strong>: Fine-tune summarization models on project-specific data or domain-relevant terminology to improve contextual understanding and minimize errors from jargon or specialized language.</li> <li> <strong>State and Conflict Management</strong>: Implement versioning and conflict resolution mechanisms for the shared memory system. This includes tracking updates and ensuring that inconsistent or contradictory inputs are flagged for manual intervention.</li> <li> <strong>Seamless Tool Integration</strong>: Integrate with existing collaboration and version control tools (e.g., Slack, GitHub, Microsoft Teams) via APIs to minimize friction and ensure data consistency.</li> <li> <strong>Provenance and History Linking</strong>: Use version control systems and data lineage tracking to connect documentation, decisions, and code changes to relevant project phases or discussions.</li> </ul> <p><strong>Feasibility</strong><br/> This use case is highly feasible, especially considering the capabilities of existing AI tools for summarization and task tracking. Many of the core components (meeting transcription, summarization, task tracking, and changelog generation) are already mature in tools like Otter.ai, Microsoft Teams, Slack, or Zoom. Primary challenges lie in maintaining a coherent, evolving memory that reflects the project’s evolution while ensuring user trust in the system. A hybrid human-AI workflow, with strong integration into existing tools and a structured approach to validation and memory management, would help mitigate risks associated with incomplete or inaccurate documentation. Requires a strong foundation, especially to integrate version control practices.</p> <p>The integration of AI agents and multi-agent systems into workflows is obviously a paradigm shift. As AI agents’ capabilities scale, research in the academia will undergo qualitative leaps. First, it represents the opportunity to diminish the burden of repetitive tasks, and the ability for resource-constrained labs to operate on a more level playing field with well-funded teams.</p> <p>Mostly, the ability to iterate quickly, access operational support with ease, and avoid methodological errors enable more efficient, transparent, and reproducible research practices. In this context, AI agents themselves function as reproducible, standardized tools, making task execution more portable across machines, labs, and institutions. This standardization increases the potential for interdisciplinary collaboration by reducing the friction across domains.</p> <p>AI agents will inevitably become part of daily practices, just like LLMs became part of our daily life (deep research, chain-of-thought, documents summarization, emails generation, assisted writing). AI agents will eventually help us move past bottlenecks, automate tedious tasks, and ensure that our work remains consistent and reproducible. This is especially relevant as funding research may become challenged in the next few years, open data will yield a whole strand of research dedicated to replicating results or reusing data, and emerging countries will increasingly take part to modern research.</p> <p>AI agents were funky until last year; the possibilities they present are now no longer just speculative. Though, they require experimenting in the lab until toolboxes are available.</p> <p>To stay updated on future developments, connect with me on <a href="https://github.com/ValentinGuigon">GitHub</a> or reach out directly.</p> <p>From my row house in DC,</p> <p>A bientôt,</p> <p><strong>Author:</strong><em><br/> </em><a href="https://valentinguigon.github.io/"><em>Valentin Guigon</em></a><em>, PhD<br/> Postdoctoral researcher,<br/> </em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/> University of Maryland, College Park, MD, USA</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=529a12347269" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="academia"/><category term="experiment"/><category term="ai"/><category term="research"/></entry><entry><title type="html">The public sphere and the echo chambers</title><link href="https://valentinguigon.github.io/posts/2025-03-21-2025-03-21-the-public-sphere-and-the-echo-chambers/" rel="alternate" type="text/html" title="The public sphere and the echo chambers"/><published>2025-03-21T15:01:50+00:00</published><updated>2025-03-21T15:01:50+00:00</updated><id>https://valentinguigon.github.io/posts/2025-03-21-the-public-sphere-and-the-echo-chambers</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-03-21-2025-03-21-the-public-sphere-and-the-echo-chambers/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[On why the public sphere is not a collection of echo chambers and the causes of noise when listening to networks.]]></summary></entry><entry><title type="html">Feeling like it</title><link href="https://valentinguigon.github.io/posts/2025-03-20-2025-03-20-feeling-like-it/" rel="alternate" type="text/html" title="Feeling like it"/><published>2025-03-20T15:01:49+00:00</published><updated>2025-03-20T15:01:49+00:00</updated><id>https://valentinguigon.github.io/posts/2025-03-20-feeling-like-it</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-03-20-2025-03-20-feeling-like-it/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[I love writing, whether as part of a scientific process, to transmit how the world works or as a testimony on human experience. Here's what to expect from me.]]></summary></entry><entry><title type="html">Coming soon</title><link href="https://valentinguigon.github.io/posts/2025-03-19-2025-03-19-coming-soon/" rel="alternate" type="text/html" title="Coming soon"/><published>2025-03-19T14:58:19+00:00</published><updated>2025-03-19T14:58:19+00:00</updated><id>https://valentinguigon.github.io/posts/2025-03-19-coming-soon</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-03-19-2025-03-19-coming-soon/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[This is a place for my non peer-reviewed works.]]></summary></entry><entry><title type="html">Workflow for a Reproducible Research</title><link href="https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research/" rel="alternate" type="text/html" title="Workflow for a Reproducible Research"/><published>2024-11-03T16:59:37+00:00</published><updated>2024-11-03T16:59:37+00:00</updated><id>https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*psp9EjuuirWpr714dtCHpA.jpeg"/></figure> <p>This workflow is part of a guide about reproducibility in academic research, specifically focusing on computational analysis. It is based on my practices as a PhD and postdoctoral researcher in the academia and takes inspiration on DevOps and MLOps.</p> <p>It is a companion to the <a href="https://medium.com/@valentin.guigon/reproducibility-in-research-a-practical-guide-2-3-a-workflow-for-a-reproducible-research-f8d6c58e81cf">second part of a three-parts series about reproducibility in academic research</a>, specifically focusing on computational analysis.</p> <p>In the first part of this series, we identified two key pillars of reproducibility:</p> <ol> <li>Adhering to the FAIR principles — making data <strong>Findable</strong>, <strong>Accessible</strong>, <strong>Interoperable</strong>, and <strong>Reusable.</strong> </li> <li>Constructing stable computational environments to maintain consistency in methodology and conditions over time.</li> </ol> <p>Here we address the construction of a workflow, the third pillar of a reproducible research. Specifically, we aim to establish a systematic series of steps that serve as the foundation for any research project. This workflow is intended to facilitate the creation of a stable computational environment aligned with the FAIR principles and <strong>reusable</strong> across projects.</p> <p>The result should be a replication package that could be handled as is to reviewers.</p> <h4>I. Setup the <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">structure of the research project</a> </h4> <ol><li> <strong>Outline</strong> a consistent structure across projects</li></ol> <ul> <li>Folders<br/>— Data<br/>— Documents<br/>— Notebooks<br/>— Scripts</li> <li>Config files</li> <li><a href="http://README.md">README.md</a></li> </ul> <ol> <li> <strong>Outline</strong> the <a href="https://occasionaldivergences.com/posts/rep-env/">virtual environments</a> </li> <li> <strong>Build</strong> the directory with a service standardization<br/>- <a href="https://github.com/ValentinGuigon/cookiecutter-neuro-research-project">Cookiecutter</a> </li> </ol> <h4>II. Implement version control by initializing a project as a git repository</h4> <h4>III. Set up your code management and data management conventions</h4> <ul> <li>Define a <a href="https://laneguides.stanford.edu/DataManagement/Organizing">coding naming organization</a> </li> <li>Define a <a href="https://www.loc.gov/preservation/resources/rfs/format-pref-summary.html">data saving organization</a> </li> </ul> <h4>I. Load and work within your virtual environments</h4> <h4>II. Start by using computational notebooks</h4> <ol> <li>First, outline the code in pseudocode</li> <li>Then use notebooks as playground</li> </ol> <h4>III. Then switch to scripts</h4> <ol><li> <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md">Refactor notebooks into scripts</a><br/>— Processing scripts<br/>— Analysis scripts<br/>— Data appendix scripts</li></ol> <p>2. <a href="https://medium.com/data-science-at-microsoft/testing-practices-for-data-science-applications-using-python-71c271cd8b5e">Test your scripts</a></p> <h4>I. Follow a scripting Workflow</h4> <ol> <li>Start with notebooks as playground then <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md">refactor into scripts</a> </li> <li>Follow and maintain code good practices<br/>- Follow <a href="https://gist.github.com/wojteklu/73c6914cc446146b8b533c0988cf8d29">clean code general rules</a><br/>- Stick to <a href="https://dev.to/gervaisamoah/a-guide-to-clean-code-the-power-of-good-names-3f6i">conventions</a> when naming things<br/>- If you copy/paste, use templates/boilerplates (<a href="https://medium.com/@bluucaterpilla/a-data-science-boilerplate-%E0%B2%A0%E1%B4%97%E0%B2%A0-ff1fd5cfe84e">examples here</a>)</li> </ol> <p>3. Generate automatic reports (results reports into .html with <em>.ipynb</em> (Python) or <a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html"><em>nb.html</em> (R)</a>)</p> <p>4. Maintain version control</p> <p>5. Use automatic code formatter &amp; linter for your IDE</p> <p>6. Orchestrate the execution of the computations with a batch/workflow</p> <h4>II. Document the code and the data</h4> <ol><li><strong>Maintain up-to-date Documentation</strong></li></ol> <ul> <li>Project-level <a href="http://README.md">README.md</a> </li> <li>Subfolder-level <a href="http://README.md">README.md</a> </li> <li>Metadata/Cards:<br/>— <a href="https://huggingface.co/docs/hub/model-cards">Model cards</a><br/>— <a href="https://huggingface.co/docs/hub/datasets-cards">Dataset cards</a><br/>— <a href="https://huggingface.co/docs/datasets/main/en/repository_structure">Directory cards</a> </li> <li><a href="https://egonw.github.io/cookbook-dev/content/recipes/interoperability/creating-data-dictionary.html#an-example-of-data-dictionary">Data dictionary</a></li> <li><a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html">Automatic reports</a></li> </ul> <h4>III. Maintain quality checks</h4> <p>From my row house in DC,</p> <p>Merci pour votre temps</p> <p>Et à bientôt,</p> <blockquote> <strong>Author:</strong><em><br/>Valentin Guigon, PhD<br/>Postdoctoral researcher,<br/></em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/>University of Maryland, College Park, MD, USA</em> </blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b91207fe5b14" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="research"/><category term="mlops"/><category term="academia"/><category term="open-science"/></entry><entry><title type="html">Reproducibility in Research, a practical guide (2/3): A workflow for a Reproducible Research</title><link href="https://valentinguigon.github.io/articles/2024-11-03-reproducibility-in-research-a-practical-guide-23-a-workflow-for-a-reproducible-research/" rel="alternate" type="text/html" title="Reproducibility in Research, a practical guide (2/3): A workflow for a Reproducible Research"/><published>2024-11-03T16:59:06+00:00</published><updated>2024-11-03T16:59:06+00:00</updated><id>https://valentinguigon.github.io/articles/2024-11-03-reproducibility-in-research-a-practical-guide-23-a-workflow-for-a-reproducible-research</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2024-11-03-reproducibility-in-research-a-practical-guide-23-a-workflow-for-a-reproducible-research/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qsrihqClibsQcO05"/><figcaption>Photo by <a href="https://unsplash.com/@karsten116?utm_source=medium&amp;utm_medium=referral">Karsten Winegeart</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>This guide took roots from the ethos of Open Science. In the first post of this three-part series on reproducibility in academic research, I argued that <a href="https://medium.com/@valentin.guigon/reproducibility-in-academic-research-1-3-key-components-of-a-reproducible-research-413f27cd0886">FAIR principles and stable computational environments are essential to a reproducible research</a>. The current post delves into building a practical research workflow that relies on stable computational environments and adheres to FAIR principles.</p> <p>As a disclaimer, I make a <a href="https://www.ncbi.nlm.nih.gov/books/NBK547531/">distinction between <em>reproducibility</em> and <em>replicability</em></a>. For a more precise look at what reproducibility means, Nature has dedicated in 2018 a special archive on the <a href="http://www.nature.com/news/reproducibility-1.17552">Challenges in Irreproducible Research</a>. For technical aspects, you can follow this <a href="https://lms.fun-mooc.fr/courses/course-v1:inria+41023+session01/info">very comprehensive online course from INRIA</a>. You can also read PlosOne <a href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1003285&amp;type=printable">10 simple rules for reproducible computational research</a>. <strong>For a more concise version, you can find the </strong><a href="https://medium.com/@valentin.guigon/workflow-for-a-reproducible-research-b91207fe5b14"><strong>blueprint of my current research workflow</strong></a><strong> as a companion post.</strong></p> <p>I’ve been writing this post to the best of my ability. Please feel free to reach for any reason.</p> <p><a href="https://medium.com/@valentin.guigon/reproducibility-in-academic-research-1-3-key-components-of-a-reproducible-research-413f27cd0886">In the first part of this series, we identified two key pillars of reproducibility</a>:</p> <ol> <li>Adhering to the FAIR principles — making data <strong>Findable</strong>, <strong>Accessible</strong>, <strong>Interoperable</strong>, and <strong>Reusable</strong> — which serve as essential guidelines for effective data management.</li> <li>Constructing stable computational environments to maintain consistency in methodology and conditions over time.</li> </ol> <p>The current blog post will address the construction of a workflow. Specifically, we aim to establish a systematic series of steps that serve as the foundation for our research. This workflow will facilitate the creation of a stable computational environment aligned with the FAIR principles and <strong>reusable</strong> across projects.</p> <p>The first layer of the workflow involves building the environment as a directory. Executing scripts within this directory is intended to recover the results of our computations. The directory is intended to be shared as is with, for instance, reviewers — a replication package.</p> <p>The second layer will focus on encapsulating the project directory within a container isolating the computational environment and controlling its dependencies.</p> <p>The third layer will address the automation of the container.</p> <p>Overall, we will examine four facets of reproducibility: <strong>Organization, Automation, Documentation.</strong> <strong>Dissemination </strong>is addressed as sharing a replication package.</p> <p>This post is extensive. You can find a concise summary below and the blueprint for my current workflow <a href="https://medium.com/@valentin.guigon/reproducibility-in-academic-research-1-3-key-components-of-a-reproducible-research-413f27cd0886">here</a>.</p> <ul> <li><strong>Initialize Project Structure:</strong></li> <li>— Use <a href="https://cookiecutter.readthedocs.io/en/latest/overview.html">Cookiecutter</a> to create a consistent project template.</li> <li>— Initialize a <a href="https://git-scm.com/">Git </a>repository with git init.</li> <li>— Initialize data management (e.g., <a href="https://git-annex.branchable.com/">Git Annex</a>, <a href="https://dvc.org/doc/start">DVC</a>).</li> <li><strong>Documentation:</strong></li> <li>— Write project-level documentation and file-level documentation (e.g., README.md, data dictionary, cards).</li> <li><strong>Development Process:</strong></li> <li>— Start coding in computational notebooks for exploration and prototyping.</li> <li>— Refactor notebooks into organized scripts</li> <li><strong>Reporting:</strong></li> <li>— Generate automatic output reports in HTML format from notebooks or scripts.</li> <li><strong>Execution:</strong></li> <li>— Write a master script to orchestrate the execution of the computational pipeline as a batch.</li> </ul> <ul> <li><strong>Functional Package Management:</strong></li> <li>— Manage dependencies using a functional package manager (e.g., <a href="https://nixos.org/download/">Nix</a>, <a href="https://guix.gnu.org/">Guix</a>).</li> <li><strong>Containerization:</strong></li> <li>— Package the project into a container (e.g., <a href="https://www.docker.com/">Docker</a>).</li> </ul> <ul> <li><strong>Establish Computational Workflow:</strong></li> <li>— Turn the master script into a workflow (e.g., <a href="https://makefiletutorial.com/">Makefile</a>, <a href="https://snakemake.readthedocs.io/en/stable/">SnakeMake</a>, <a href="https://github.com/resources/articles/devops/ci-cd">CI/CD</a>).</li> <li>— Write a recipe for the container that will: pull data from repository (e.g., <a href="https://figshare.com/">Figshare</a>, <a href="https://zenodo.org/">Zenodo</a>), pull code from repository (<a href="https://github.com/">GitHub </a>or <a href="https://about.gitlab.com/">GitLab</a>), execute the workflow.</li> </ul> <ul> <li>Create a project on a research management platform (e.g., <a href="http://osf.io/">OSF.io</a>)</li> <li>Associate links to the code repository (e.g., <a href="https://github.com/">GitHub</a>, <a href="https://about.gitlab.com/">GitLab</a>)</li> <li>Associate links to the data repository (e.g., <a href="https://figshare.com/">Zenodo</a>, <a href="https://zenodo.org/">Figshare</a>)</li> <li>In case of container:</li> <li>— Include the container recipe in the project on the research management platform</li> </ul> <ul> <li>Layer I. Organization / Set-up / outlining / only at the initiation of a project<br/>A. Outline a consistent structure across projects<br/>B. Outline virtual environments<br/>C. Use a service standardization for building the directory<br/>D. Implement version control by initializing a project as a git repository<br/>E. Set up your code management and data management conventions</li> <li>Layer I. Automation / New and recurrent processes<br/>A. Operate within virtual environments<br/>B. Start with Notebooks as Playground<br/>C. Refactor Notebooks into Scripts<br/>D. Use Appropriate Tools and Libraries to manipulating data by hand<br/>E. Follow good Coding Practices<br/>F. Maintain version control<br/>G. Use an automatic code formatter &amp; linter for your IDE<br/>H. Implement Testing</li> <li>Layer I. Document the code and the data<br/>A. What to document?<br/>B. How to document?</li> <li>Layer II. Isolation and containers<br/>A. Introduction to Containers<br/>B. Example: Docker: A Versatile Containerization Platform for Research</li> <li>Layer II. Functional package managers<br/>A. Introduction to Functional Package Managers</li> <li>Layer III. Workflows<br/>A. Introduction to Workflows<br/>B. Types of Workflow Systems<br/>C. Implementation Strategies</li> </ul> <p>This relates to organizing a project as a directory on the computer (<a href="https://laneguides.stanford.edu/DataManagement/Organizing">more here</a>). Be consistent, clear, structured in the creation and maintaining of directories, naming, files and file contents. Others and yourself should be able to navigate the project without a map.</p> <ol> <li>Create a <strong>main project directory</strong> with an informative name.</li> <li>Create <strong>subdirectories </strong>for different components (e.g., data, analysis, documents).<br/>a. Create a dedicated output subdirectory for <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/paper/">reports</a> and/or figures<br/>b. Create a <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/readme/">README</a> file describing the project and the directory<br/>c. Keep raw data separate from derived data.</li> <li>Make<strong> raw data read-only</strong> to prevent accidental modifications</li> <li>Keep <strong>code separate from data</strong>.</li> </ol> <p>This can be achieved by defining a directory structure at the start of the project. For instance:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*j7MSpkgOOMrjXNB-.png"/><figcaption>(Taken from the <a href="https://laneguides.stanford.edu/DataManagement/Organizing">Stanford edu</a>)</figcaption></figure> <p>A virtual environment is an isolated, self-contained directory that contains a specific version of a programming language and its associated libraries and dependencies. It allows you to create separate environments for different projects, each with its own set of installed packages and versions.</p> <p><a href="https://occasionaldivergences.com/posts/rep-env/"><strong>Why Use Virtual Environments?</strong></a></p> <ol> <li> <strong>Dependency isolation</strong>: Different projects require different versions of libraries/packages/dependencies. Virtual environments prevent conflicts between them.</li> <li> <strong>Reproducibility</strong>: Virtual environments make it easier to recreate the exact environment in which a project was developed, with one simple command.</li> <li> <strong>Clean system</strong>: They keep your system Python/R/else installation clean by installing project-specific packages in isolated environments.</li> <li> <strong>Version control</strong>: Virtual environments can be easily version-controlled, allowing you to track changes in project dependencies over time.</li> <li> <strong>Easier collaboration</strong>: They make it simpler for other developers to set up and run your project with the correct dependencies.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qepljIfEIsADLRPN"/><figcaption>Taken from <a href="https://www.linkedin.com/pulse/python-virtual-environments-tutorial-using-virtualenv-dmitriy-zub/"><strong>Dmitriy Zub</strong></a></figcaption></figure> <p>By restoring the virtual environment each time you want to work on your project, you keep the project clean an contained. Several solutions exist for Python, R and MATLAB:</p> <ul> <li> <strong>Python</strong> has built-in support for managing project-specific configurations and dependencies (i.e., virtual environment) through the <strong>venv</strong> module<br/><strong>- Initiate the </strong><strong>venv</strong> </li> <li> <strong>R</strong> manages project-specific configurations with the .Rprj project files (i.e., Select File &gt; New Project…) and uses for <strong>renv</strong> package for virtual environments<br/><strong>- Create the </strong><strong>.Rprj project<br/>- Install the </strong><strong>renv</strong> </li> <li> <strong>MATLAB</strong> manages project-specific configurations with the .Prj project files (i.e., New &gt; Project &gt; Blank Project). Whereas it doesn't offer a true virtual environment, the workspace isolation via paths and projects allows to manage dependencies in a controlled way<br/><strong>- Create the </strong><strong>.Prj project</strong> </li> </ul> <p><a href="https://cookiecutter.readthedocs.io/en/stable/overview.html">Cookiecutter</a> is a wonderful solution to realize consistently and rapidly a project structure. Cookiecutter takes a <a href="https://www.cookiecutter.io/templates">template </a>file, prompts the user with few questions and outputs a standardized directory structure.</p> <p>Acting as an executable blueprint, Cookiecutter enforces standards across projects, making its components by essence Findable, Accessible, Interoperable and Reusable. Thanks to its automation, human errors are eliminated and projects get built with a higher velocity.</p> <p>If looking for authoritative arguments, here are <a href="https://gitlab.has-sante.fr/has-sante/public/cookiecutter-prod-python-has">France’s Haute Autorité de Santé template</a> and UK GOV’s <a href="https://dataingovernment.blog.gov.uk/2021/07/20/govcookiecutter-a-template-for-data-science-projects/">govcookiecutter</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Gr2vXc7bSZ8byydw.png"/><figcaption><a href="https://github.com/ukgovdatascience/govcookiecutter">Govcookiecutter</a></figcaption></figure> <p>My own template for cognitive neurosciences projects is available <a href="https://github.com/ValentinGuigon/cookiecutter-neuro-research-project">here</a>. It initializes a directory structure and, given prompts, initiates MATLAB/Python/R projects with renv, .Rprj, renv, .Prj .</p> <p><a href="https://about.gitlab.com/topics/version-control/">Version control is essential for tracking changes, collaborating with others, and maintaining the integrity of your research project.</a> It enables a secure, dynamic, and collaborative approach to developing code and managing research projects. You can consider it a backbone for reproducibility. <a href="https://git-scm.com/">Git </a>is the most widely used version control system. Importantly, code, data, model parameters and documentation can all be version controlled. You can find further motivation and a guide <a href="https://neurathsboat.blog/post/git-intro/">here</a>.</p> <p>A Version Control System (VCS) like Git allows to:</p> <ol> <li>Capture snapshots of your code/data/documentation/parameters as you develop, enabling you to revert to previous states if needed.</li> <li>Create branches to experiment with new ideas without risking your main codebase.</li> <li>Merge different versions of as code and documentation, facilitating the integration of new features or experiments.</li> <li>Collaborate effectively with other researchers by sharing and merging code changes.</li> <li>Track the evolution of your project over time, including who made specific changes and why.</li> </ol> <p>In other words, versioning help you freeze computational states in time, and revert from an ulterior state to an anterior state. <a href="https://www.codecademy.com/article/f1-u3-git-setup">A simple tutorial for associating a local directory with a remote repository hosted on GitHub can be found here</a>.</p> <p><em>File should:</em></p> <ul> <li> <strong>Be human readable</strong> — you should understand the content of the file from its name alone</li> <li> <strong>Be machine readable</strong> — don’t use spaces (_ is preferable), special characters or accents</li> </ul> <p>Organize file naming:</p> <ul> <li>Include information sufficient to easily locate a specific file.</li> <li>Make sure file names are unique, descriptive and meaningful.</li> </ul> <p>Organize files content</p> <ul><li>The principles of file naming and organization can also be applied within files, for naming variables, columns, rows, values</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/758/0*fm5YUkJ3O3JXQ3V-.png"/><figcaption>(Example from <a href="https://laneguides.stanford.edu/DataManagement/Organizing">Stanford edu</a>)</figcaption></figure> <p>When <strong>preserving/publishing</strong> digital content it’s essential to save the work in an appropriate file format to ensure long-term preservation and accessibility. It is also important to save twice: in a specific format for the scientific work and in open format for preserving.</p> <p>Privilege formats that are:</p> <ul> <li><strong>widely used within your discipline</strong></li> <li><strong>open, non-proprietary, unencrypted and uncompressed (except if lossless compression)</strong></li> <li> <strong>self-documenting</strong> (i.e. the file itself can include useful metadata)</li> <li> <strong>publicly documented</strong> (i.e. the complete file specification is publicly available) &amp; <strong>endorsed by standards agencies</strong> (e.g;, ISO standard)</li> </ul> <p>For instance, <strong>not .docx, .xls</strong>, etc. Rather, <strong>.csv for data</strong> and <strong>.tiff for images</strong>. <a href="https://www.loc.gov/preservation/resources/rfs/"><strong>Check the Library of Congress Recommended Formats list to see what is recommended for your file type.</strong></a></p> <p>By restoring the virtual environment each time you want to work on your project, you keep the project clean an contained.</p> <p>At the end/beginning on the day, close/load the virtual environments:</p> <ul> <li> <strong>On Python, load the </strong><strong>venv</strong> </li> <li> <strong>On R, load the </strong><strong>.Rprj project &amp; restore the </strong><strong>renv via </strong><strong>renv::restore()</strong> </li> <li> <strong>On MATLAB, load the </strong><strong>.Prj project</strong> </li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/612/0*GucROonzXE4aU9Lo.png"/><figcaption>3 simples commands to run a stable renv: init(), snapshot() and restore(). Taken from <a href="https://rstudio.github.io/renv/articles/renv.html">renv website</a>.</figcaption></figure> <ul> <li> <strong>Use computational notebooks</strong> (e.g., Jupyter Notebooks, R Markdown) <strong>for initial exploration</strong>.</li> <li>Document your dependencies, analyses, results, interpretations, comments.</li> <li>Keep notebooks organized and readable by:<br/>- Moving reusable code to external files.<br/>- Defining key variables at the top.<br/>- Restarting kernels periodically to ensure reproducibility.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UMSDmLDY3_U1cYJD.png"/><figcaption>How R notebooks operate: write scripts as chunks of code on the left; generate .hmtl report on the right. Taken from <a href="https://rmarkdown.rstudio.com/lesson-10.html">rmarkdown website</a>.</figcaption></figure> <p>Once you are done with exploration, convert your notebook code into robust, reusable scripts.</p> <p>Notebooks are very often messy and not optimized for executing code efficiently — unless moving source code into robust, reusable scripts. Consider notebooks in a final project for producing automated reports.</p> <p>To refactor notebooks into scripts, <a href="https://towardsdatascience.com/how-to-refactor-a-jupyter-notebook-ed531b6a17">follow a structured refactoring process:</a></p> <ul> <li>1. Ensure the notebook runs without errors.</li> <li>2. Identify code blocks that can be turned into functions.</li> <li>3. Write tests for these functions.</li> <li>4. Create separate Python/R modules for these functions.</li> <li>5. Replace notebook code with calls to these new functions.</li> <li>6. Verify the refactored notebook still produces the same results.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*kMwrtpwkqgSp0fDN.png"/><figcaption><a href="https://towardsdatascience.com/how-to-refactor-a-jupyter-notebook-ed531b6a17">David Tan’s guide on How to refactor a jupyter notebook</a></figcaption></figure> <ul> <li> <strong>For data manipulation</strong>: <a href="https://swcarpentry.github.io/r-novice-gapminder/12-dplyr.html">dplyr</a>, <a href="https://swcarpentry.github.io/r-novice-gapminder/13-tidyr.html">tidyr</a> (R); pandas, numpy (Python)</li> <li> <strong>For visualization</strong>: ggplot2 (R); matplotlib, seaborn (Python)</li> <li> <strong>For report generation</strong>: <a href="https://swcarpentry.github.io/r-novice-gapminder/14-knitr-markdown.html">R Markdown</a>, <a href="https://jupyter.org/">Jupyter Notebooks</a>, <a href="https://www.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script-or-function.html">MATLAB Live Scripts</a> </li> </ul> <ul> <li>Follow the DRY (<strong>Don’t Repeat Yourself</strong>) principle.</li> <li> <strong>Write modular</strong>, single-purpose functions.</li> <li> <strong>Use clear, descriptive names</strong> for variables and functions.</li> <li> <strong>Make code readable</strong> and include comments to tell what and why, not how</li> <li> <strong>Reduce complexity, </strong>break down problem into bite size pieces</li> <li>Know that your code is doing the right thing (<strong>test your functions</strong>)</li> <li> <strong>Use version control</strong> consistently.</li> </ul> <p><strong>First, a disclaimer on the uses of Git:<br/></strong>Git is not intended for handling large data files. Saving computational modelling output files (e.g., 50mb .RData models) in a git repository very often prompts Git to asks users for a different system called<a href="https://git-lfs.com/"> LFS (Large File Storage)</a>. <strong>GitHub and GitLab are NOT intended for storing large files</strong>, and doing so will cost personal account data storage, or money.</p> <p>To properly use and maintain version control without the research project:</p> <ol> <li><strong>Make sure you initialized a Git repository:</strong></li> <li> <strong>Version control your code, documentation, and configuration files:</strong><br/>- Use <a href="https://git-scm.com/">Git</a> to track changes in your source code, documentation, and configuration files.<br/>- Host your repository on platforms like GitHub or GitLab.</li> <li> <strong>Version control your datasets:</strong><br/>- Use <a href="https://git-annex.branchable.com/">Git Annex</a> to store files on data repositories and pointers to data on Git repositories.<strong><br/></strong>- Or use <a href="https://dvc.org/">DVC</a> to track changes in data when performing ML.<br/>- Host your data on secured platforms (<a href="https://zenodo.org/records/10651775">data repositories here</a>)</li> <li> <strong>Track your experiment parameters and results:</strong><br/>- Save scripts, environment configuration files, data, evaluation metrics, training parameters, model weights and visualizations used in ML with Experiment tracking tools (e.g., <a href="https://mlflow.org/">MLflow</a>, <a href="https://dvc.org/">DVC</a>, <a href="https://wandb.ai/site">Weights &amp; Biases</a>)</li> <li> <strong>Follow best practices:</strong><br/>- Commit frequently with meaningful commit messages.<br/>- Use .gitignore to exclude unnecessary files.<br/>- Tag important milestones or versions.<br/>- Use branches for different features or experiments.<br/>- Regularly push changes to remote repositories for backup and collaboration.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*PkBs71xJcbk9_sXt.png"/><figcaption>Git simplest workflow: a master (main) branch and a feature branch. Taken from <a href="https://medium.com/@abeythilakeudara3/version-control-system-cabd8d120986">Udara Abeythilake</a>.</figcaption></figure> <p>If you use an IDE (e.g., Visual Code Studio, RStudio), code formatters (e.g., <a href="https://prettier.io/">Prettier</a>, <a href="https://www.tidyverse.org/blog/2017/12/styler-1.0.0/">styler</a>) and code linters (e.g., <a href="https://www.pylint.org/">Pylint</a>) may be useful, as they will automatically check the code for errors and format the code in a readable and conventional format when hitting save.</p> <p>Choosing the appropriate formatter and a linter may depend on the language, framework, conventions decided for the project.</p> <p>Testing is often overlooked until conflicts and confusions emerge in data. Resolving these issues ex-post often requires reverse-engineering complex processes. To avoid this, implement testing from the start and make it a habit. Testing has the benefits of:</p> <ol> <li>Helping ensure that analyses produce correct and consistent outputs, reducing the risk of errors of data alterations or wrong conclusions.</li> <li>Acting as executable documentation, making it easier for others (or yourself in the future) to understand and replicate the analysis by reading the tests outputs.</li> <li>Providing a common reference point for understanding how code and analyses should behave, facilitating teamwork.</li> <li>Helping in refining methods over time, knowing that existing functionality remains intact.</li> <li>Leading to more modular and maintainable code.</li> </ol> <p><a href="https://medium.com/data-science-at-microsoft/testing-practices-for-data-science-applications-using-python-71c271cd8b5e">Examples</a>:</p> <ol> <li> <strong>Validation by Asserting</strong>: Condition the execution of steps in your script by implementing an assert test that throws an error if a condition is false.</li> <li> <strong>Unit Testing</strong>: Write tests for individual functions or components of your analysis. If you have a function that calculates a specific statistic, create a test that verifies it produces the correct output for known inputs.</li> <li> <strong>Integration Testing</strong>: Test how different parts of your analysis pipeline work together. Run your entire workflow from data input to final output, ensuring that the complete process produces expected results.</li> <li> <strong>Comparing Outputs</strong>: Run identical analyses in both notebook and script formats, then compare the results. This practice helps ensure that transitioning from exploratory notebooks to final scripts does not introduce errors or inconsistencies in your findings.</li> <li> <strong>Quality Checks</strong>: Verify the integrity of your data. For instance, check the content of a variable after altering the preprocessing. trigger a head() to check data format or output the first images of a brain scan.</li> </ol> <p>Proper documentation is crucial for reproducibility, collaboration, and long-term project maintenance.</p> <p>In general, <a href="https://laneguides.stanford.edu/DataManagement/Documenting">you should maintain documentation at both the <strong>project level</strong> and the <strong>file level</strong></a>.</p> <ul> <li> <strong>Project-level</strong> documentation includes information about the processes used throughout the project, including how you and your collaborators are collecting, organizing, and analyzing your data.</li> <li> <strong>File-level</strong> documentation includes details related to individual files.</li> </ul> <p>Documentation can be maintained in a variety of forms. Some common forms of documentation are:</p> <ul> <li> <strong>Readme</strong> — A <a href="http://Readme.md">Readme.md</a> file is a text file located in a project-related folder that describes the contents and structure of the folder and/or a dataset so that a researcher can locate the information they need. The .md extension refers to markdown, a text language easy to write and easy to interpret.</li> <li> <strong>Data Dictionary</strong> — Also known as a codebook, a data dictionary defines and describes the elements of a dataset so that it can be understood and used at a later date.</li> <li> <strong>Protocol</strong> — A protocol describes the procedure(s) or method(s) used in the implementation of a research project or experiment.</li> <li> <strong>Lab Notebook</strong> — For research groups that use them, lab notebooks are often the primary record of the research process. They are used to document hypotheses, experiments, analyses, and interpretations of experiments.</li> <li> <strong>Metadata</strong> — Metadata is data about data. Metadata often conforms to a specific scheme- a set of standardized rules about how the metadata is organized and used. There are different types of metadata:<br/>- descriptive metadata (information about the content of your data)<br/>- structural metadata (information about the physical structure of your data, including file format)<br/>- administrative metadata (information about how and when your data was created).</li> </ul> <p>There are a variety of ways to maintain documentation related to your research. It can be as straightforward as developing a regular practice of documenting your process in a Google Document or as formal as maintaining a formal lab notebook. <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">The TIER protocol</a> has a guideline for the location of documentation. Please check it for more details.</p> <p>Create a comprehensive <a href="http://README.md">README.md</a> file at the root of your project directory. This file should include:</p> <ul> <li>Project title and brief description</li> <li>Authors’ information and contact details</li> <li>Project goals and objectives</li> <li>Installation instructions and necessary software</li> <li>Usage guidelines</li> <li>Description of the computational environment</li> <li>Overview of the project structure</li> <li>Links to relevant publications or external resources</li> </ul> <p>Create <a href="http://README.md">README.md</a> files in each major subfolder (e.g., data/, scripts/, docs/) to provide specific information about the contents and purpose of that directory.</p> <p>Implement standardized metadata documentation for various components of your project:</p> <ol> <li> <a href="https://huggingface.co/docs/hub/model-cards">Model Cards</a>: For machine learning projects, create model cards that describe<br/>- Model architecture<br/>- Training data<br/>- Performance metrics<br/>- Intended use and limitations<br/>- Ethical considerations</li> <li> <a href="https://huggingface.co/docs/hub/datasets-cards">Dataset Cards</a>: Document your datasets with cards that include<br/>- Dataset description and purpose<br/>- Data collection methodology<br/>- Data structure and schema<br/>- Data quality and preprocessing steps<br/>- Ethical considerations and potential biases</li> <li> <a href="https://huggingface.co/docs/datasets/main/en/repository_structure">Repository Structure Cards</a>: Create a card that outlines the overall structure of your repository, including<br/>- Main directories and their purposes<br/>- Key files and their functions<br/>- Relationships between different components</li> </ol> <p>Create a comprehensive <a href="https://laneguides.stanford.edu/DataManagement/Documenting">data dictionary</a> that defines and describes all variables in your datasets. Include:</p> <ul> <li>Variable names</li> <li>Data types</li> <li>Units of measurement</li> <li>Possible values or ranges</li> <li>Description of what each variable represents</li> </ul> <p>Implement automatic report generation to document your analysis process and results:</p> <ul> <li>Use tools like <a href="https://jupyter.org/">Jupyter</a> notebooks or <a href="https://bookdown.org/yihui/rmarkdown/notebook.html">R Notebooks</a> to create reproducible reports</li> <li>Include data visualizations, statistical summaries, and interpretations</li> <li>Ensure reports are generated automatically as part of your workflow</li> <li>Store reports in a dedicated directory (e.g., /reports)</li> </ul> <p>Containers offer a lightweight, consistent environment for running applications across diverse computing systems, ensuring that software behaves identically regardless of where it’s deployed. This is a big step for research reproducibility, enabling exact replication of computational environments.</p> <p>While tools like virtual environments and version control support reproducibility, containerization goes further. It creates a self-contained setup that includes code, data, and all system dependencies, eliminating inconsistencies across systems. This consistency simplifies collaboration, eases deployment to different platforms, and aids in the long-term preservation of research work.</p> <p>For more information, check <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316">Nüst et al (2020) <strong>Ten simple rules for writing Dockerfiles for reproducible data science.</strong></a></p> <p><a href="https://www.docker.com/">Docker</a>, a popular containerization platform, is particularly advantageous for research. It offers cross-platform compatibility, enabling work across different operating systems. With a robust ecosystem and community support, Docker provides access to a vast array of pre-built images and tools. Compared to traditional virtual machines, Docker containers are more resource-efficient, making them ideal for intensive computations. By running analyses in identical environments, Docker enhances reproducibility, isolates projects, manages version control of environments, and facilitates collaboration through shareable container images.</p> <p>To start using Docker in your research:</p> <ol> <li>Install Docker Desktop.</li> <li>Create a Dockerfile by<br/>- Specifying the base image and project dependencies or<br/>- <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316">Following Nüst et al. recipe</a> </li> <li>Decide whether to copy your project into the container or set the container to pull from the project from the git remote repository.</li> <li>Build the Docker image via the command line.</li> <li>Run your analysis in the container for consistent, isolated execution.</li> <li>Share your image on Docker Hub or share the recipe on your research repository (e.g., <a href="http://OSF.io">OSF.io</a>).</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/320/0*tnK5ADbjlpVu83Ov"/><figcaption><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316">Nüst et al (2020) <strong>Ten simple rules for writing Dockerfiles for reproducible data science</strong></a></figcaption></figure> <p>Functional package managers offer a different approach to managing software environments. Unlike traditional package managers that construct environments through a sequence of steps, functional package managers use mathematical functions to define and build software environments.</p> <p><strong>Traditional package managers</strong> (e.g., Debian’s, <a href="https://wiki.debian.org/Apt">apt</a>, Windows’ <a href="https://chocolatey.org/">chocolatey</a>, macOS’ <a href="https://brew.sh/">homebrew</a>) build environments sequentially, making results order-dependent and often non-reproducible.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/492/0*uNpKzJJ3-QC3GUEM.png"/><figcaption>Python Environment — by xkcd.com</figcaption></figure> <p><strong>Functional package managers</strong> (e.g., <a href="https://guix.gnu.org/">Guix</a>, <a href="https://medium.com/@zmre/the-package-manager-to-rule-them-all-9a8829e4f392">Nix</a>) construct environments as mathematical functions, yielding reproducible builds and greater dependency control. As a consequence, they offer the possibility to rollback to previous versions of dependencies and provide access to the history of versions. Each version is pulled from a package repository called <a href="https://nixos.org/manual/nixpkgs/stable/">Nixpkgs</a>.</p> <p>Key concepts:</p> <ol> <li>Environments are defined as <a href="https://nixos.org/manual/nixpkgs/stable/"><strong>sums</strong></a> of packages</li> <li>Packages function as <strong>mathematical constructs</strong> based on source code, build environment, and scripts.</li> <li>Source code is precisely <strong>identified using cryptographic hashes</strong> </li> </ol> <p>This method addresses challenges such as:</p> <ul> <li>Ensuring precise references to software versions</li> <li>Managing complex dependency graphs</li> <li>Eliminating assumptions about standard software locations</li> </ul> <p>Nix for instance can be installed on the local machine or in a container such as Docker, for easier dependencies management.</p> <p>For more arguments in favor of functional package managers as an alternative to the many-managers situation, please refer to <a href="https://learninglab.gitlabpages.inria.fr/mooc-rr/mooc-rr2-ressources/module2/seq7-conclusion/unit1-lecture.html">this article</a>.</p> <p>Workflows bring structure and automation to complex computational processes in research, transforming scripts into reproducible, organized sequences of tasks. Benefits include:</p> <ol> <li> <strong>Automated task execution:</strong> Steps run automatically when a file hash changes, ensuring tasks are updated only when necessary.</li> <li> <strong>Clear dependencies and data flow:</strong> Each task and data movement is explicitly defined, making the process easy to understand and maintain.</li> <li> <strong>Enhanced reproducibility:</strong> Automated workflows create consistent outputs by reproducing the exact sequence of operations every time.</li> <li> <strong>Parallel processing and scalability:</strong> Tasks can be run in parallel or distributed across systems, accelerating data-intensive processes.</li> <li> <strong>Improved error handling:</strong> Automated workflows detect and isolate errors, allowing efficient recovery and debugging.</li> </ol> <p>Workflows can be seen as an evolution of computational notebooks, offering more structure and better handling of complex, data-intensive tasks.</p> <p>Workflow systems range from simple task automation to complex pipelines:</p> <ol> <li>Batch Processing:<br/>- Manually set up processing steps by specifying the modules to run, the data type, dependencies between modules, and their execution order (e.g., SPM batch for fMRI analysis).</li> <li> <strong>Task Automation:<br/>- </strong><em>Makefile:</em> Local automation for simple dependencies<br/>- <em>Snakemake:</em> Python-based, ideal for scientific workflows</li> <li> <strong>CI/CD Tools:<br/>- </strong><em>GitHub Actions:</em> YAML-based workflows integrated with GitHub<br/>- <em>CircleCI:</em> CI/CD with strong Docker support and customizable pipelines</li> <li> <strong>Data-Intensive and Specialized Workflow Managers:<br/>- </strong><em>Nextflow:</em> Parallel scientific workflows, supports cloud/HPC<br/>- <em>Cromwell, Luigi, Prefect:</em> Specialized for bioinformatics, genomics, data workflows</li> <li> <strong>Hybrid Notebook-Workflow Systems:<br/>- </strong><em>SOS-notebook:</em> Blends interactive notebooks with structured workflows</li> </ol> <p>Each tool offers different levels of complexity and scalability, chosen based on the project’s computational needs.</p> <p>Implementing workflows in research projects can be approached in several ways:</p> <ol> <li> <strong>From Scripts to Simple Workflows:<br/>- </strong>Use Make or Snakemake to automate tasks and define dependencies<br/>- Modularize scripts for clarity and scalability</li> <li> <strong>Adopt CI/CD Practices:<br/>- </strong>Use version control (e.g., Git)<br/>- Automate testing and deployment with tools like GitHub Actions</li> <li> <strong>Full Workflow Management:<br/>- </strong>Select a domain-specific workflow manager (e.g., Nextflow for bioinformatics)<br/>- Organize the research process into modules, orchestrated by the workflow tool</li> <li> <strong>Container-Based Workflows:<br/>- </strong>Start in a containerized environment (e.g., Docker)<br/>- Define the environment with a Dockerfile<br/>- Use tools like Docker Compose to manage multiple services and integrate with workflows for robust reproducibility.</li> </ol> <p>Appreciate the simplicity of recovering results with a few commands.</p> <p>From my row house in DC,</p> <p>Merci pour votre temps</p> <p>Et à bientôt,</p> <ul> <li>National Academies of Sciences, Policy, Global Affairs, Board on Research Data, Information, Division on Engineering, … &amp; Replicability in Science. (2019). <em>Reproducibility and replicability in science</em>. National Academies Press.</li> <li>Sandve, G. K., Nekrutenko, A., Taylor, J., &amp; Hovig, E. (2013). Ten simple rules for reproducible computational research. <em>PLoS computational biology</em>, <em>9</em>(10), e1003285.</li> <li>Nüst, D., Sochat, V., Marwick, B., Eglen, S. J., Head, T., Hirst, T., &amp; Evans, B. D. (2020). Ten simple rules for writing Dockerfiles for reproducible data science. <em>PLoS computational biology</em>, <em>16</em>(11), e1008316.</li> </ul> <blockquote> <strong>Author:</strong><em><br/>Valentin Guigon, PhD<br/>Postdoctoral researcher,<br/></em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/>University of Maryland, College Park, MD, USA</em> </blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f8d6c58e81cf" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="open-science"/><category term="academia"/><category term="mlops"/><category term="research"/></entry></feed>