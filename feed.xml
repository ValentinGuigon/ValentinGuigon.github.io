<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://valentinguigon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valentinguigon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-24T16:32:59+00:00</updated><id>https://valentinguigon.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Valentin Guigon. </subtitle><entry><title type="html">The Better Judgment Project: how AI forecasting will reshape human metacognition</title><link href="https://valentinguigon.github.io/posts/2026-02-23-2026-02-23-the-better-judgment-project-how-ai-forecasting-will-reshape-human-metacognition/" rel="alternate" type="text/html" title="The Better Judgment Project: how AI forecasting will reshape human metacognition"/><published>2026-02-23T22:05:15+00:00</published><updated>2026-02-23T22:05:15+00:00</updated><id>https://valentinguigon.github.io/posts/2026-02-23-the-better-judgment-project-how-ai-forecasting-will-reshape-human-metacognition</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2026-02-23-2026-02-23-the-better-judgment-project-how-ai-forecasting-will-reshape-human-metacognition/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[In 2027, is it still worth bothering with human judgment?]]></summary></entry><entry><title type="html">The Better Judgment Project: what good judgments are missing</title><link href="https://valentinguigon.github.io/posts/2026-02-21-2026-02-21-the-better-judgment-project-what-good-judgments-are-missing/" rel="alternate" type="text/html" title="The Better Judgment Project: what good judgments are missing"/><published>2026-02-21T22:08:41+00:00</published><updated>2026-02-21T22:08:41+00:00</updated><id>https://valentinguigon.github.io/posts/2026-02-21-the-better-judgment-project-what-good-judgments-are-missing</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2026-02-21-2026-02-21-the-better-judgment-project-what-good-judgments-are-missing/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[In 2026, are we still evaluating the world the right way?]]></summary></entry><entry><title type="html">Intelligence is easy; cognition is hard</title><link href="https://valentinguigon.github.io/posts/2026-01-24-2026-01-24-intelligence-is-easy-cognition-is-hard/" rel="alternate" type="text/html" title="Intelligence is easy; cognition is hard"/><published>2026-01-24T18:10:41+00:00</published><updated>2026-01-24T18:10:41+00:00</updated><id>https://valentinguigon.github.io/posts/2026-01-24-intelligence-is-easy-cognition-is-hard</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2026-01-24-2026-01-24-intelligence-is-easy-cognition-is-hard/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Distinguishing intelligent systems from cognitive systems]]></summary></entry><entry><title type="html">The mind, the brain and the network</title><link href="https://valentinguigon.github.io/posts/2025-10-14-2025-10-14-the-mind-the-brain-and-the-network/" rel="alternate" type="text/html" title="The mind, the brain and the network"/><published>2025-10-14T03:01:14+00:00</published><updated>2025-10-14T03:01:14+00:00</updated><id>https://valentinguigon.github.io/posts/2025-10-14-the-mind-the-brain-and-the-network</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-10-14-2025-10-14-the-mind-the-brain-and-the-network/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[What's modular? Learning under topological constraints]]></summary></entry><entry><title type="html">Can machines think?</title><link href="https://valentinguigon.github.io/posts/2025-09-06-2025-09-06-can-machines-think/" rel="alternate" type="text/html" title="Can machines think?"/><published>2025-09-06T20:36:59+00:00</published><updated>2025-09-06T20:36:59+00:00</updated><id>https://valentinguigon.github.io/posts/2025-09-06-can-machines-think</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-09-06-2025-09-06-can-machines-think/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Assessing machine vs human cognition]]></summary></entry><entry><title type="html">Some forecasting on the next decade of Neuroscience</title><link href="https://valentinguigon.github.io/posts/2025-08-27-2025-08-27-some-forecasting-on-the-next-decade-of-neuroscience/" rel="alternate" type="text/html" title="Some forecasting on the next decade of Neuroscience"/><published>2025-08-27T23:10:23+00:00</published><updated>2025-08-27T23:10:23+00:00</updated><id>https://valentinguigon.github.io/posts/2025-08-27-some-forecasting-on-the-next-decade-of-neuroscience</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-08-27-2025-08-27-some-forecasting-on-the-next-decade-of-neuroscience/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Based on The BRAIN Initiative 2025 Report and the BRAIN Initiative August 2025 Meeting]]></summary></entry><entry><title type="html">AI Agents in the Lab: Concept Paper</title><link href="https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper/" rel="alternate" type="text/html" title="AI Agents in the Lab: Concept Paper"/><published>2025-05-08T11:22:03+00:00</published><updated>2025-05-08T11:22:03+00:00</updated><id>https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper/"><![CDATA[<ul> <li>1. Why This Matters</li> <li>2. When Is It Time</li> <li>3. Blueprint</li> <li>4. Use Case 1: Research Discovery &amp; Critique</li> <li>5. Use Case 2: Multi-Agent Simulation Sandbox</li> <li>6. Use Case 3: Research Documentation</li> <li>7. Use Case 4: Reproducibility Audit &amp; Open Science Automation</li> <li>8. Use Case 5: Research Collaboration &amp; Memory Assistant</li> <li>9. What This Means for Research</li> <li>10. Now</li> </ul> <figure><img alt="There are 16 different small square images in this collage. Each of them have a grid background and different neon coloured square patterns in the squares. One looks like a flower, another looks like a random array of pixels, and the other is 4 blue squares." src="https://cdn-images-1.medium.com/max/1024/1*ThUOQjc9vYzhBkTlGomqMw.jpeg"/><figcaption>Elise Racine / <a href="https://betterimagesofai.org">https://betterimagesofai.org</a> / <a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></figcaption></figure> <p>Modern academic research faces structural inefficiencies that hinder quality, slow progress, and exacerbate inequities between large, well-funded labs and smaller, resource-constrained ones. Reproducibility remains a persistent crisis, while high cognitive load, bureaucratic demands, and fragmented tools siphon time away from high-level reasoning.</p> <p>As in any domain shaped by financial pressure, limited time, and high competition, AI agents have the potential to directly alleviate these structural burdens. At the same time, modern research demands constant high precision. Human error is the most common source of slowdown, inconsistency, and missed insight whenever repetition is at the heart of a process. On the other hand, <a href="https://arxiv.org/pdf/2412.14161">benchmarks</a> show how far from sufficient quality AI agents achieve on complex processes. Instead, by offloading non-critical, simple but essential tasks, agentic systems should increase both the clarity and the reliability of scientific output.</p> <p>Specifically, AI agents and multi-agent systems (MAS), when properly orchestrated, would optimize a meaningful portion of research pipelines, flag methodological flaws early, and promote consistent documentation and quality control.</p> <p>These systems would lower the barrier to high-quality science. Smaller labs and early-career researchers, who often lack institutional support or internal review infrastructure, would be able to compete more equitably in a publish-or-perish ecosystem. The end goal is to increase both speed and quality, while leaving the trade-off between them to humans : more precision, less friction, faster iteration.</p> <p>In this document I outline use cases for integrating AI agents and MAS into academic research workflows. Each use case is structured around functional value and estimated technical feasibility. Specifically, the use cases are grounded in the context of computational and cognitive neuroscience, where these perspectives are most familiar to me. These examples should be understood as <strong>conceptual designs</strong>, intended to guide real-world lab experimentation.</p> <p>While MAS are promising, their current capabilities come with limitations. They are not yet mature enough for full task automation. However, they can already provide valuable assistance, particularly in structured environments where feedback-based correction allows for progressive refinement or post-hoc corrections. In this role, they function as <strong>intelligent scaffolds</strong>: supportive systems that enhance repetitive work prior to human oversight.</p> <p>As emphasized on each use case feasibility, Now is time for small-scale use cases; Later is time for complex use cases.</p> <p>Each section below describes a use case, the corresponding implications, the agentic solution (tools, tasks, agents, workflow), sample outputs, estimated weaknesses and recommendations, and the estimated feasibility.</p> <p>Each use case follows a nine-part structure:</p> <ol> <li>Define a) research needs that can be addressed and b) their implications for the practices.</li> <li>Propose a high-level pipeline composed of a) agents, b) tools, c) decomposed tasks and d) workflows.</li> <li>Assess a) potential weaknesses and b) suggest recommendations.</li> <li>Estimate feasibility based on current tools.</li> </ol> <p>I emphasize that this is a tentative, conceptual work, meant to guide future experiments, and that my expertise lies primarily in cognitive and neurocomputational sciences.</p> <p>General recommendations</p> <ul> <li> <strong>Use Structured Schemas (Pydantic)</strong>: Ensure all outputs (reports, metadata, environments) are structured and machine-parseable. This supports integration into continuous review pipelines and version tracking.</li> <li> <strong>Robust Workflow Orchestration</strong><br/> Use DAG-style control flows (e.g., LangGraph) to manage iteration, branching, and error handling for complex crews.</li> <li> <strong>Human-in-the-Loop Validation</strong><br/> Periodically calibrate agent behavior against real subject data to avoid overfitting to flawed prompt assumptions.</li> </ul> <p><strong>Purpose</strong></p> <p>Automate high-quality literature discovery and structured critique of scientific papers — assessing theoretical alignment, modeling validity, methodological bias, funding relevance, and ethical risks. This system will be <strong>augmentative</strong>, not replacing human experts. <strong>Internal documentation</strong>, including scientific papers, must be heavily leveraged to ensure the critiques and evaluations align with established research standards.</p> <p><strong>Implications: </strong>Faster, structured literature reviews; improved reproducibility via consistent critique schemas; early detection of theoretical or methodological flaws; template-driven alignment and drafting support.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Enforces structured schemas for outputs.</li> <li> <strong>External APIs</strong>: SerpAPI, PubMed, CrossRef, Semantic Scholar API for multi-source academic retrieval.</li> <li> <strong>Internal Documentation</strong>: Internal scientific documents, guidelines, style guides and domain-specific knowledge bases to ground evaluations in domain expertise.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Search &amp; Retrieve</strong>: Query APIs and retrieve relevant papers using natural-language queries and filters.</li> <li> <strong>Indexing &amp; Semantic Search</strong>: Build vector indices from retrieved papers and perform RAG-based retrieval to enhance search relevance.</li> <li> <strong>Classification</strong>: Tag documents by field, method, population, and relevance.</li> <li> <strong>Summarization</strong>: Extract structured summaries with citations and metadata, ensuring completeness but with limitations.</li> <li> <strong>Multi-Lens Critique</strong>: Run modular evaluation passes (theoretical validity, compliance risks, modeling rigor, demographic bias, etc.).</li> <li> <strong>Funding Relevance</strong>: Assist in grant application preparations by matching paper topics to relevant funding calls using past grant data and basic NLP.</li> </ul> <p><strong>Agent</strong></p> <ul> <li> <strong>Semantic Retriever</strong>: Interfaces with APIs and vector stores to surface relevant literature.</li> <li> <strong>Classifier</strong>: Labels documents based on methodology, field, and population studied.</li> <li> <strong>Summarizer:</strong> Extracts structured summaries with citation metadata.</li> <li> <strong>Synthesizer</strong>: Generates structured outputs, focusing on summarizing themes, key findings, and proposal elements.</li> <li> <strong>Modeling Critic:</strong> Assesses alignment of model with theoretical hypotheses or inappropriate model usage (theoretical alignment checklists, not full critique).</li> <li> <strong>Bias Auditor</strong>: Recommends analysis strategies (e.g., simulations, effect size calculations, power analysis) to assess bias and improve model robustness.</li> <li> <strong>Ethics &amp; Compliance Critic:</strong> Flags potential IRB, GDPR, and dual-use risks based on internal policy documents and rule-based checks.</li> </ul> <p><strong>Workflow Structure</strong></p> <ul> <li>CrewAI delegates task-specific agents across a modular critique pipeline.</li> <li>Optional chaining (e.g., LangGraph) routes papers between agents based on content type.</li> <li>State memory passes outputs (e.g., summaries, classifications) across stages.</li> <li>Pydantic schemas enforce structure for all agent outputs.</li> <li>Outputs are reviewed and refined iteratively using feedback from researchers.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li>`.bib`: BibTeX with relevance tags and critique annotations.</li> <li>` md`: Structured markdown synthesis and summaries.</li> <li>`.json`: Machine-readable JSON objects of evaluations and labels.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Automated Critique Limitations</strong>: AI cannot yet replace domain experts in assessing deep theoretical soundness. Critiques may be shallow or inconsistent.</li> <li> <strong>Retrieval Limitations</strong>: External APIs have incomplete coverage and rate limits. Missing full texts can degrade critique quality.</li> <li> <strong>Classification Accuracy</strong>: Automated tagging can misclassify boundary cases.</li> <li> <strong>Workflow Complexity</strong>: Multi-agent memory and coordination can create brittle pipelines.</li> <li> <strong>Expectation Management</strong>: Risk of users assuming full automation rather than augmentation.</li> <li> <strong>Prompt Engineering &amp; Schema Validation</strong>: Improper prompt structure can lead to inconsistent outputs.</li> <li> <strong>Internal Document Grounding</strong>: Failure to integrate internal documentation may reduce context accuracy and increase hallucinations.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li>Use checklist-driven critique templates and flagging systems to maintain transparency and guide human review.</li> <li>Prioritize summaries from abstracts and metadata. Integrate internal documents to fill gaps and improve accuracy.</li> <li>Implement hierarchical taxonomies and confidence scoring in classification. Allow manual corrections.</li> <li>Modularize agent roles to localize failure. Implement error logging and fallback mechanisms.</li> <li>Set clear user expectations through documentation and UI cues about AI’s supportive role.</li> <li>Use Pydantic schemas with early validation checks to ensure format integrity.</li> <li>Align all evaluation prompts with internal documentation, policies, and domain knowledge to improve grounding and reduce drift.</li> </ul> <p><strong>Feasibility</strong></p> <p>Many components are already in use individually: semantic search, structured summarization, and rule-based bias detection. Integrating them into a modular, agent-based workflow is feasible with current frameworks (e.g., CrewAI, LangGraph) and good prompt/schema design. Success depends on well-grounded internal documentation, careful agent calibration, and realistic scoping of critique depth. While LLMs can flag obvious issues, domain-specific judgment and iterative refinement are essential to avoid overreliance on superficial outputs. Mostly, systems seem not mature enough to finalize into a full-scope workflow. The entire proposition has a high degree of complexity. Can be broken down into systems of small scope.</p> <p><strong>Purpose</strong></p> <p>Create an <em>in silico</em> environment to simulate subject-like behavior across experimental conditions. The sandbox enables researchers to test and benchmark task designs (decision-making paradigms, game-theoretic setups, social phenomena, maybe abstracted mechanisms) prior to data collection. Simulated data can aid in estimating effect sizes, stress-testing models, and refining behavioral hypotheses. Cognitive modeling could be combined with AI-driven behavior to generate realistic trial-level data.</p> <p><strong>Implications</strong>: Cheap pilot testing, design optimization.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Cognitive modeling libraries </strong>(e.g., custom Bayesian inference engines, reinforcement learning models).</li> <li> <strong>Data validation frameworks </strong>(e.g., Pydantic for schema enforcement).</li> <li> <strong>Data science and visualization </strong>(e.g., pandas, seaborn, matplotlib).</li> <li> <strong>Simulation state stores </strong>(e.g., Redis, or in-memory Python structures).</li> <li> <strong>Notebooks or dashboards </strong>for exploratory analysis and parameter tuning.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li>Generate cognitive profiles with varying priors, learning rates, and strategies informed by explicit computational models or prompt-driven approximations.</li> <li>Execute trial-based simulations where agents perform decisions, produce confidence ratings, reaction times (RTs), and outcomes according to task rules.</li> <li>Record detailed trial logs with structured data schemas.</li> <li>Aggregate and analyze population-level behavior, producing statistical summaries and visual reports.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Persona Generator: </strong>Samples parameter sets from computational models or approximates cognitive traits using structured prompts.</li> <li> <strong>Task Executor:</strong> Performs trial-level decisions using a hybrid approach combining explicit model computations and LLM-generated behavior constrained by structured prompts.</li> <li> <strong>Interaction Logger:</strong> Validates and stores each trial’s outputs and metadata using formal (e.g., Pydantic) schemas.</li> <li> <strong>Results Summarizer:</strong> Generates statistical summaries, behavior plots, and variability estimates.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Simulation is initialized with N synthetic agents.</li> <li>Each agent executes a set of trials under specific experimental conditions.</li> <li>Central state tracks agent properties, task variables, and simulation history.</li> <li>Data is validated and logged continuously.</li> <li>After simulations, analysis agents produce visualizations and reports.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Structured logs (CSV, JSON): </strong>Trial-by-trial behavior with metadata.</li> <li> <strong>Visualizations: </strong>Learning curves, RT distributions, choice matrices, etc.</li> <li> <strong>Summary notebooks: </strong>Auto-generated for post-hoc analysis and reproducibility.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Cognitive Modeling Limitations: </strong>LLMs alone cannot precisely execute complex cognitive models; they approximate behavior based on textual descriptions and tend to suffer from WEIRD biases. Pure prompt-driven agents may produce inconsistencies; detecting the source of variability may not be possible.</li> <li> <strong>State and Interaction Complexity: </strong>Managing multi-agent state across iterative trials is challenging and prone to errors without robust centralized state management. Complex workflows increase architectural overhead and debugging difficulty.</li> <li> <strong>Scalability Constraints: </strong>Large-scale simulations require parallelization and resource optimization not inherently provided by CrewAI or LangGraph.</li> </ul> <p>· <strong>Output Dependence on Model Fidelity: </strong>The quality of logs and analyses depends heavily on the realism of underlying agent behaviors.</p> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Hybrid Modeling Approach</strong><br/> Combine explicit computational cognitive models with LLM-driven behavior generation.</li> <li> <strong>Structured Prompt Engineering</strong><br/> Guide agent responses using prompts that request structured outputs (e.g., JSON), ensuring parsability and reproducibility across trials. Pydantic schemas validate trial logs and agent outputs.</li> <li> <strong>Modular Agent Roles</strong><br/> Maintain separation of concerns (i.e., persona generation, task execution, logging, summarization) to ease debugging and extensibility.</li> <li> <strong>Centralized State Management</strong><br/> Employ memory stores to persist simulation state, agent parameters, and historical trial outcomes across loops.</li> <li> <strong>Parallelize Simulations</strong><br/> Run agent trials in parallel where feasible to boost throughput and scalability.</li> <li> <strong>Automate Statistical Reporting</strong><br/> Leverage Python data science tools (e.g., seaborn, pandas, scipy) to auto-generate behavioral plots and summary statistics for each simulation batch.</li> </ul> <p><strong>Feasibility</strong></p> <p>The approach has already been adopted in the early days of GPT3. Sound experimental designs should today yield data with sufficient quality to stress-test experimental designs (prompt outliers, test unforeseen behavior under the designed rules). While LLMs can approximate (WEIRD-like) model-driven behavior, integrating explicit computational models and robust state management should be tested. Chain-of-Thoughts may prove useful for testing mentalization. Scalability and validation may become challenges. Human oversight is obviously critical.</p> <p><strong>Purpose</strong></p> <p>Automate the generation, auditing, and maintenance of research project documentation, including READMEs, metadata, and code annotations — to enhance reproducibility, usability, and transparency. The goal is to reduce documentation debt while supporting compliance with FAIR and open science standards.</p> <p><strong>Implications</strong>: Higher likelihood of reproducibility; better alignment with data-sharing mandates and publication requirements.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Ensures that outputs like documentation audits, metadata files, and README components follow structured, machine-parseable schemas.</li> <li> <strong>Static Code Parsers</strong>: Tools for extracting function signatures, comments, and code structure (e.g., Python’s ast, inspect).</li> <li> <strong>Natural Language Generators</strong>: Lightweight LLM wrappers for docstring generation and README drafting, fine-tuned for scientific language.</li> <li> <strong>Metadata Validators</strong>: Schema.org and JSON-LD validators, FAIR compliance checkers, and data catalog schema tools.</li> <li> <strong>Version Control Interfaces</strong>: Git hooks and commit parsers to trace documentation lineage and link documentation updates to code changes.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>README Drafting</strong>: Auto-generate a project README including installation, usage, citation, and licensing information.</li> <li> <strong>Function Annotation</strong>: Generate or verify docstrings for functions and classes based on code logic and structure.</li> <li> <strong>File Role Tagging</strong>: Classify source files by role (e.g., data preprocessing, model training) to enhance navigation and metadata clarity.</li> <li> <strong>Metadata Generation</strong>: Produce structured, schema.org-compliant JSON-LD metadata for datasets, workflows, and scripts.</li> <li> <strong>Documentation Consistency Audit</strong>: Evaluate generated documentation for internal consistency and alignment with code behavior or test cases.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>README Composer</strong>: Builds structured project overviews by extracting key project metadata, dependencies, and usage examples.</li> <li> <strong>Code Annotator</strong>: Adds or updates inline docstrings by analyzing code structure and function behavior.</li> <li> <strong>Metadata Generator</strong>: Produces JSON-LD metadata templates, ensuring fields for identifiers, licensing, access, and file provenance.</li> <li> <strong>Documentation Auditor</strong>: Checks whether documentation reflects actual code behavior and tests alignment through symbolic reasoning or simple unit tests.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>The project’s file structure and scripts are parsed to identify documentation targets.</li> <li>Function and module annotations are generated, prioritizing high-level coverage over exhaustive line-by-line summaries.</li> <li>Project-level metadata and READMEs are drafted based on detected scripts, packages, and prior author contributions.</li> <li>Audit agents scan outputs for inconsistencies, missing fields, or misalignments between documentation and code behavior.</li> <li>All outputs are validated using Pydantic schemas and flagged for optional human review before publishing.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Standardized README (Markdown)</strong>: With sections for project purpose, usage, dependencies, author attribution, and citations.</li> <li> <strong>Inline Docstrings</strong>: Inserted or updated within source files, providing structured summaries of function logic, inputs, and outputs.</li> <li> <strong>Metadata Files</strong>: JSON-LD records suitable for FAIR repositories (e.g., Zenodo, Dataverse), including schema.org-compliant descriptions.</li> <li> <strong>Documentation Audit Reports</strong>: Highlighting missing annotations, inconsistencies between code and documentation, or metadata gaps.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Contextual Limits of Automation</strong>: Generated docstrings and READMEs may miss project-specific logic, edge cases, or architectural rationale.</li> <li> <strong>Superficial Metadata</strong>: FAIR metadata often appears valid structurally but lacks meaningful semantics without human input (e.g., vague licensing terms).</li> <li> <strong>Workflow Brittleness</strong>: Without integration into CI/CD pipelines or developer routines, documentation can quickly become outdated as the codebase evolves.</li> <li> <strong>Semantic Inaccuracy</strong>: While schema validation confirms structure, it cannot verify whether documentation truthfully describes code behavior.</li> <li> <strong>Overgeneralization Risks</strong>: LLM-based generators may produce generic or misleading summaries if not fine-tuned on project-specific conventions.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Use Structured Schemas</strong>: All documentation artifacts (README, docstrings, metadata) should be generated to conform to strict templates (validated by Pydantic), enabling downstream reuse and integration into project management pipelines.</li> <li> <strong>Layer Human Review Where Needed</strong>: Use automation for first drafts, but route critical sections (e.g., licensing, ethical use, and dataset sensitivity) to human reviewers before finalization.</li> <li> <strong>Enhance Context Awareness</strong>: Improve accuracy of generated content by incorporating architectural overviews (e.g., flowcharts or design notes) and prior documentation history.</li> <li> <strong>Integrate Documentation with Git</strong>: Trigger doc updates or audits on key commits. Maintain traceability by linking doc artifacts to their source commits or branches.</li> <li> <strong>Validate Documentation with Code Behavior</strong>: Check for inconsistencies (e.g., returns string instead of number) between generated docs and function outputs.</li> <li> <strong>Leverage Provenance Tracking</strong>: Link documentation to specific project phases or data artifacts (e.g., outputs from modeling or preprocessing) using version history and content hashes.</li> </ul> <p><strong>Feasibility</strong><br/> Automating documentation should be highly feasible for standard research project layouts where functions and workflows follow consistent patterns. Most tools required already exist. Challenge is ensuring semantic fidelity and domain alignment. While not a substitute for thoughtful writing, automated drafting and auditing dramatically reduce time and effort. This makes the system well-suited for pre-publication QA checks, onboarding kits, and FAIR submission preparation, otherwise minimal corrections.</p> <p><strong>Purpose</strong></p> <p>Automate the evaluation of research artifacts — code, data, and metadata — for reproducibility, transparency, and FAIR compliance. The system supports researchers in identifying reproducibility gaps, generating repair suggestions, and improving data/code stewardship with minimal manual effort. It does not replace human review but accelerates it through structured, automatable checks.</p> <p><strong>Implications</strong>: Increased transparency of research pipelines; early detection of reproducibility risks; semi-automated compliance with open science mandates.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Enforces structured schemas for outputs such as reproducibility reports, metadata records, and environment specifications.</li> <li> <strong>Static Analysis Libraries</strong>: AST parsers, regex scanners, and linters (e.g., Python’s ast, flake8, black) for code auditing.</li> <li> <strong>Metadata Validators</strong>: JSON schema validators for checking FAIR-aligned metadata (e.g., schema.org, CEDAR templates).</li> <li> <strong>Environment Introspection Tools</strong>: Tools such as pipdeptree, conda list, or Dockerfile parsers for dependency validation.</li> <li> <strong>Command-line Utilities</strong>: Git history analyzers, file checksum tools, and shell scripts for file integrity and version tracing.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Documentation Generation:</strong> Generate READMEs from project structure and code. Auto-generate function-level docstrings. Tag file purposes and interconnections.</li> <li> <strong>Dependency Mapping</strong>: Parse and reconstruct the project’s dependency graph using static analysis of environment files, setup scripts, and imports.</li> <li> <strong>Code Integrity Scanning</strong>: Detect reproducibility risks such as missing seeds, hardcoded paths, non-reproducible code patterns, and undocumented workflows.</li> <li> <strong>Metadata Validation</strong>: Assess the structure, presence, and standard-compliance of metadata files (e.g., schema.org, CEDAR templates).</li> <li> <strong>Workflow Reconstruction</strong>: Extract and sequence computational steps from notebooks, scripts, or workflow managers (e.g., Snakemake), forming a pipeline graph.</li> <li> <strong>FAIR Assessment</strong>: Evaluate the degree of compliance with FAIR principles, including findability (identifiers), accessibility (license fields), and interoperability (linked vocabularies).</li> <li> <strong>Repair Suggestion Generation</strong>: Propose corrective artifacts like Dockerfiles, environment.yaml, or JSON-LD metadata files to address flagged issues and enhance reproducibility.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Static Code Auditor</strong>: Scans scripts for reproducibility anti-patterns (e.g., unseeded randomness, implicit dependencies, or non-deterministic file paths).</li> <li> <strong>Metadata Validator</strong>: Checks presence, structure, and compliance of metadata files with community-accepted schemas.</li> <li> <strong>Workflow Reconstructor</strong>: Parses pipeline scripts or notebooks to infer workflow structure and dependency order.</li> <li> <strong>Reproducibility Synthesizer</strong>: Aggregates findings into structured, human-readable reports with suggestions for repair and improvement.</li> <li> <strong>FAIR Compliance Auditor:</strong> Maps data and metadata to FAIR metrics (e.g., unique identifiers, license clarity).</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Initial file scan identifies code, metadata, and documentation artifacts.</li> <li>Static analysis tools evaluate code structure and dependency graphs.</li> <li>Metadata files are parsed and checked against community schemas.</li> <li>Agents work in sequence or in parallel, validating, flagging, and generating outputs for review.</li> <li>All outputs conform to Pydantic-based schemas, allowing integration with reproducibility dashboards, continuous integration pipelines, or project documentation workflows.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Structured Reports (PDF, Markdown): </strong>Issue summaries (e.g., missing seeds, inconsistent environment specs) and recommended fixes.</li> <li> <strong>FAIR Metadata Templates:</strong> Auto-generated JSON-LD or YAML files aligned with schema.org or CEDAR templates.</li> <li> <strong>Environment Specifications:</strong> Machine-readable requirements.txt, environment.yaml, or Dockerfiles with version-pinned dependencies.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Partial Automation:</strong> FAIR principles like accessibility and ethical reuse still require human interpretation (e.g., licensing, IRB approvals).</li> <li> <strong>Workflow Incompleteness:</strong> Scripts often omit manual preprocessing steps or runtime logic that static tools can’t infer.</li> <li> <strong>Semantic Gaps:</strong> Schema validation can verify structure but not meaning (e.g., a license field may be present but semantically incorrect).</li> <li> <strong>Dependency Resolution Errors:</strong> Without actual environment builds, version conflicts and runtime incompatibilities may go undetected.</li> <li> <strong>False Positives:</strong> Static scans may wrongly flag hardcoded values or randomness that are actually justified.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Use Structured Schemas (Pydantic)</strong>: Ensure all outputs (reports, metadata, environments) are structured and machine-parseable. This supports integration into continuous review pipelines and version tracking.</li> <li> <strong>Modular Agent Roles</strong>: Keep agent responsibilities narrow (e.g., metadata validation vs code structure parsing) to reduce debugging complexity and enable domain-specific extensions (e.g., BIDS for neuroimaging).</li> <li> <strong>Incorporate Human Review Layers</strong>: Supplement automated scans with human checks, especially for FAIR “Accessibility” and data governance requirements.</li> <li> <strong>Track Provenance</strong>: Integrate tools or scripts that log file creation/modification lineage, aiding in data and workflow transparency.</li> <li> <strong>Validate in Real Environments</strong>: Where possible, test environment specifications by building containers or virtual environments. Use their logs as diagnostic tools.</li> <li> <strong>Generate Repair Artifacts</strong>: Suggest corrections, not just critiques. Auto-generate Dockerfiles, README scaffolds, and metadata records aligned with community templates.</li> </ul> <p><strong>Feasibility</strong><br/> Most components are independently mature: code linters, metadata schemas, and environment validators already exist in the open-source ecosystem. The challenge lies in orchestrating them coherently via AI agents, with outputs structured and actionable. Semantic validation and runtime reproducibility remain hard problems, but static analysis alone can surface critical red flags early. Integration with internal reproducibility checklists and domain-specific standards (e.g., neuroimaging BIDS, computational modeling standards) would enhance performance. Usefulness is high even if the system is only partially automated.</p> <p><strong>Purpose</strong></p> <p>Maintain a shared, persistent memory of research progress, decisions, discussions, and evolving plans to enhance team coordination, transparency, and project continuity. This assistant would act as a centralized, accessible repository for all key research milestones, tasks, and updates.</p> <p><strong>Implications</strong>: Mitigating knowledge loss.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Ensures structured validation of input (e.g., meeting summaries, decision logs) and output data (e.g., changelogs, dashboards) using predefined schemas.</li> <li> <strong>Natural Language Processing (NLP) Tools</strong>: Used for extracting and summarizing key points from meeting transcripts, chat logs, and research documentation.</li> <li> <strong>Project Management Integration</strong>: Tools like GitHub, Slack, Notion, Microsoft Teams for integrating meeting notes, code commits, task status, and other project updates. Otherwise Markdown and Word documentation.</li> <li> <strong>Version Control</strong>: To track iterative changes across code, data, and hypotheses.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Communications Summarizer: </strong>Summarize team communications (e.g., meeting transcripts, chat logs, email summaries).</li> <li> <strong>Documentation Archiving: </strong>Archive project decisions, milestones, and revisions in a structured format.</li> <li> <strong>Documentation Updating: </strong>Update and maintain project timelines, research hypotheses, and data evolution records.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Meeting Summarizer:</strong> Extracts key actions, decisions, and unresolved points from transcripts or chat logs.</li> <li> <strong>Task Monitor:</strong> Tracks deliverables, deadlines, and milestone progress.</li> <li> <strong>Project Historian:</strong> Logs revisions and collaborative changes across datasets, code, and hypotheses.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Meeting transcripts, chat logs, and code commits are ingested into the system.</li> <li>Summarization models extract and synthesize key discussion points, decisions, and task updates.</li> <li>All data is structured and validated according to predefined schemas (e.g., MeetingSummary, MilestoneLog).</li> <li>Generated outputs are time-stamped and stored in a shared memory system, accessible through dashboards and changelog files.</li> <li>Critical decisions or ambiguous points are flagged for team member review to ensure accuracy.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Meeting Summaries</strong>: Markdown-based actionable, time-stamped points and decisions that are easy to reference for follow-up.</li> <li> <strong>Changelogs</strong>: Markdown-based changelogs capturing the evolution of hypotheses, datasets, and code changes over time.</li> <li> <strong>Structured Logs</strong>: Compliant with Pydantic schemas, offering machine-readable logs and summaries for automated or external use.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Context Loss in Summarization</strong>: Summarization models may overlook nuanced details, context, or domain-specific language, resulting in incomplete or misinterpreted action items.</li> <li> <strong>Dynamic Memory Management</strong>: Maintaining a coherent, up-to-date shared memory that accurately captures evolving decisions and hypotheses can be difficult to manage, especially as the project grows.</li> <li> <strong>Integration Complexity</strong>: Combining data from diverse sources (e.g., meeting transcripts, chat logs, commits) into a unified timeline requires robust tools and processes.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Human-in-the-Loop Validation</strong>: Incorporate a step for team members to review and refine AI-generated summaries and task updates to ensure accuracy. This can be achieved by flagging high-priority content for team review or approval.</li> <li> <strong>Domain-Specific Fine-Tuning</strong>: Fine-tune summarization models on project-specific data or domain-relevant terminology to improve contextual understanding and minimize errors from jargon or specialized language.</li> <li> <strong>State and Conflict Management</strong>: Implement versioning and conflict resolution mechanisms for the shared memory system. This includes tracking updates and ensuring that inconsistent or contradictory inputs are flagged for manual intervention.</li> <li> <strong>Seamless Tool Integration</strong>: Integrate with existing collaboration and version control tools (e.g., Slack, GitHub, Microsoft Teams) via APIs to minimize friction and ensure data consistency.</li> <li> <strong>Provenance and History Linking</strong>: Use version control systems and data lineage tracking to connect documentation, decisions, and code changes to relevant project phases or discussions.</li> </ul> <p><strong>Feasibility</strong><br/> This use case is highly feasible, especially considering the capabilities of existing AI tools for summarization and task tracking. Many of the core components (meeting transcription, summarization, task tracking, and changelog generation) are already mature in tools like Otter.ai, Microsoft Teams, Slack, or Zoom. Primary challenges lie in maintaining a coherent, evolving memory that reflects the project’s evolution while ensuring user trust in the system. A hybrid human-AI workflow, with strong integration into existing tools and a structured approach to validation and memory management, would help mitigate risks associated with incomplete or inaccurate documentation. Requires a strong foundation, especially to integrate version control practices.</p> <p>The integration of AI agents and multi-agent systems into workflows is obviously a paradigm shift. As AI agents’ capabilities scale, research in the academia will undergo qualitative leaps. First, it represents the opportunity to diminish the burden of repetitive tasks, and the ability for resource-constrained labs to operate on a more level playing field with well-funded teams.</p> <p>Mostly, the ability to iterate quickly, access operational support with ease, and avoid methodological errors enable more efficient, transparent, and reproducible research practices. In this context, AI agents themselves function as reproducible, standardized tools, making task execution more portable across machines, labs, and institutions. This standardization increases the potential for interdisciplinary collaboration by reducing the friction across domains.</p> <p>AI agents will inevitably become part of daily practices, just like LLMs became part of our daily life (deep research, chain-of-thought, documents summarization, emails generation, assisted writing). AI agents will eventually help us move past bottlenecks, automate tedious tasks, and ensure that our work remains consistent and reproducible. This is especially relevant as funding research may become challenged in the next few years, open data will yield a whole strand of research dedicated to replicating results or reusing data, and emerging countries will increasingly take part to modern research.</p> <p>AI agents were funky until last year; the possibilities they present are now no longer just speculative. Though, they require experimenting in the lab until toolboxes are available.</p> <p>To stay updated on future developments, connect with me on <a href="https://github.com/ValentinGuigon">GitHub</a> or reach out directly.</p> <p>From my row house in DC,</p> <p>A bientôt,</p> <p><strong>Author:</strong><em><br/> </em><a href="https://valentinguigon.github.io/"><em>Valentin Guigon</em></a><em>, PhD<br/> Postdoctoral researcher,<br/> </em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/> University of Maryland, College Park, MD, USA</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=529a12347269" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="academia"/><category term="experiment"/><category term="ai"/><category term="research"/></entry><entry><title type="html">The public sphere and the echo chambers</title><link href="https://valentinguigon.github.io/posts/2025-03-21-2025-03-21-the-public-sphere-and-the-echo-chambers/" rel="alternate" type="text/html" title="The public sphere and the echo chambers"/><published>2025-03-21T15:01:50+00:00</published><updated>2025-03-21T15:01:50+00:00</updated><id>https://valentinguigon.github.io/posts/2025-03-21-the-public-sphere-and-the-echo-chambers</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-03-21-2025-03-21-the-public-sphere-and-the-echo-chambers/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[On why the public sphere is not a collection of echo chambers and the causes of noise when listening to networks.]]></summary></entry><entry><title type="html">Coming soon</title><link href="https://valentinguigon.github.io/posts/2025-03-19-2025-03-19-coming-soon/" rel="alternate" type="text/html" title="Coming soon"/><published>2025-03-19T14:58:19+00:00</published><updated>2025-03-19T14:58:19+00:00</updated><id>https://valentinguigon.github.io/posts/2025-03-19-coming-soon</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2025-03-19-2025-03-19-coming-soon/"><![CDATA[]]></content><author><name></name></author><category term="articles"/><summary type="html"><![CDATA[Hi, I&#8217;m starting a Substack.]]></summary></entry><entry><title type="html">Workflow for a Reproducible Research</title><link href="https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research/" rel="alternate" type="text/html" title="Workflow for a Reproducible Research"/><published>2024-11-03T16:59:37+00:00</published><updated>2024-11-03T16:59:37+00:00</updated><id>https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*psp9EjuuirWpr714dtCHpA.jpeg"/></figure> <p>This workflow is part of a guide about reproducibility in academic research, specifically focusing on computational analysis. It is based on my practices as a PhD and postdoctoral researcher in the academia and takes inspiration on DevOps and MLOps.</p> <p>It is a companion to the <a href="https://medium.com/@valentin.guigon/reproducibility-in-research-a-practical-guide-2-3-a-workflow-for-a-reproducible-research-f8d6c58e81cf">second part of a three-parts series about reproducibility in academic research</a>, specifically focusing on computational analysis.</p> <p>In the first part of this series, we identified two key pillars of reproducibility:</p> <ol> <li>Adhering to the FAIR principles — making data <strong>Findable</strong>, <strong>Accessible</strong>, <strong>Interoperable</strong>, and <strong>Reusable.</strong> </li> <li>Constructing stable computational environments to maintain consistency in methodology and conditions over time.</li> </ol> <p>Here we address the construction of a workflow, the third pillar of a reproducible research. Specifically, we aim to establish a systematic series of steps that serve as the foundation for any research project. This workflow is intended to facilitate the creation of a stable computational environment aligned with the FAIR principles and <strong>reusable</strong> across projects.</p> <p>The result should be a replication package that could be handled as is to reviewers.</p> <h4>I. Setup the <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">structure of the research project</a> </h4> <ol><li> <strong>Outline</strong> a consistent structure across projects</li></ol> <ul> <li>Folders<br/>— Data<br/>— Documents<br/>— Notebooks<br/>— Scripts</li> <li>Config files</li> <li><a href="http://README.md">README.md</a></li> </ul> <ol> <li> <strong>Outline</strong> the <a href="https://occasionaldivergences.com/posts/rep-env/">virtual environments</a> </li> <li> <strong>Build</strong> the directory with a service standardization<br/>- <a href="https://github.com/ValentinGuigon/cookiecutter-neuro-research-project">Cookiecutter</a> </li> </ol> <h4>II. Implement version control by initializing a project as a git repository</h4> <h4>III. Set up your code management and data management conventions</h4> <ul> <li>Define a <a href="https://laneguides.stanford.edu/DataManagement/Organizing">coding naming organization</a> </li> <li>Define a <a href="https://www.loc.gov/preservation/resources/rfs/format-pref-summary.html">data saving organization</a> </li> </ul> <h4>I. Load and work within your virtual environments</h4> <h4>II. Start by using computational notebooks</h4> <ol> <li>First, outline the code in pseudocode</li> <li>Then use notebooks as playground</li> </ol> <h4>III. Then switch to scripts</h4> <ol><li> <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md">Refactor notebooks into scripts</a><br/>— Processing scripts<br/>— Analysis scripts<br/>— Data appendix scripts</li></ol> <p>2. <a href="https://medium.com/data-science-at-microsoft/testing-practices-for-data-science-applications-using-python-71c271cd8b5e">Test your scripts</a></p> <h4>I. Follow a scripting Workflow</h4> <ol> <li>Start with notebooks as playground then <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md">refactor into scripts</a> </li> <li>Follow and maintain code good practices<br/>- Follow <a href="https://gist.github.com/wojteklu/73c6914cc446146b8b533c0988cf8d29">clean code general rules</a><br/>- Stick to <a href="https://dev.to/gervaisamoah/a-guide-to-clean-code-the-power-of-good-names-3f6i">conventions</a> when naming things<br/>- If you copy/paste, use templates/boilerplates (<a href="https://medium.com/@bluucaterpilla/a-data-science-boilerplate-%E0%B2%A0%E1%B4%97%E0%B2%A0-ff1fd5cfe84e">examples here</a>)</li> </ol> <p>3. Generate automatic reports (results reports into .html with <em>.ipynb</em> (Python) or <a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html"><em>nb.html</em> (R)</a>)</p> <p>4. Maintain version control</p> <p>5. Use automatic code formatter &amp; linter for your IDE</p> <p>6. Orchestrate the execution of the computations with a batch/workflow</p> <h4>II. Document the code and the data</h4> <ol><li><strong>Maintain up-to-date Documentation</strong></li></ol> <ul> <li>Project-level <a href="http://README.md">README.md</a> </li> <li>Subfolder-level <a href="http://README.md">README.md</a> </li> <li>Metadata/Cards:<br/>— <a href="https://huggingface.co/docs/hub/model-cards">Model cards</a><br/>— <a href="https://huggingface.co/docs/hub/datasets-cards">Dataset cards</a><br/>— <a href="https://huggingface.co/docs/datasets/main/en/repository_structure">Directory cards</a> </li> <li><a href="https://egonw.github.io/cookbook-dev/content/recipes/interoperability/creating-data-dictionary.html#an-example-of-data-dictionary">Data dictionary</a></li> <li><a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html">Automatic reports</a></li> </ul> <h4>III. Maintain quality checks</h4> <p>From my row house in DC,</p> <p>Merci pour votre temps</p> <p>Et à bientôt,</p> <blockquote> <strong>Author:</strong><em><br/>Valentin Guigon, PhD<br/>Postdoctoral researcher,<br/></em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/>University of Maryland, College Park, MD, USA</em> </blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b91207fe5b14" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="research"/><category term="mlops"/><category term="academia"/><category term="open-science"/></entry></feed>