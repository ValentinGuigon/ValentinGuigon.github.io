<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://valentinguigon.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valentinguigon.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-01T23:36:43+00:00</updated><id>https://valentinguigon.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Valentin Guigon. </subtitle><entry><title type="html">AI Agents in the Lab: Concept Paper</title><link href="https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper/" rel="alternate" type="text/html" title="AI Agents in the Lab: Concept Paper"/><published>2025-05-08T11:22:03+00:00</published><updated>2025-05-08T11:22:03+00:00</updated><id>https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2025-05-08-ai-agents-in-the-lab-concept-paper/"><![CDATA[<figure><img alt="There are 16 different small square images in this collage. Each of them have a grid background and different neon coloured square patterns in the squares. One looks like a flower, another looks like a random array of pixels, and the other is 4 blue squares." src="https://cdn-images-1.medium.com/max/1024/1*ThUOQjc9vYzhBkTlGomqMw.jpeg"/><figcaption>Elise Racine / <a href="https://betterimagesofai.org">https://betterimagesofai.org</a> / <a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></figcaption></figure> <ul> <li>Why This Matters</li> <li>When Is It Time</li> <li>Blueprint</li> <li>Use Case 1: Research Discovery &amp; Critique</li> <li>Use Case 2: Multi-Agent Simulation Sandbox</li> <li>Use Case 3: Research Documentation</li> <li>Use Case 4: Reproducibility Audit &amp; Open Science Automation</li> <li>Use Case 5: Research Collaboration &amp; Memory Assistant</li> <li>What This Means for Research</li> <li>Now</li> </ul> <p>Modern academic research faces structural inefficiencies that hinder quality, slow progress, and exacerbate inequities between large, well-funded labs and smaller, resource-constrained ones. Reproducibility remains a persistent crisis, while high cognitive load, bureaucratic demands, and fragmented tools siphon time away from high-level reasoning.</p> <p>As in any domain shaped by financial pressure, limited time, and high competition, AI agents have the potential to directly alleviate these structural burdens. At the same time, modern research demands constant high precision. Human error is the most common source of slowdown, inconsistency, and missed insight whenever repetition is at the heart of a process. On the other hand, <a href="https://arxiv.org/pdf/2412.14161">benchmarks</a> show how far from sufficient quality AI agents achieve on complex processes. Instead, by offloading non-critical, simple but essential tasks, agentic systems should increase both the clarity and the reliability of scientific output.</p> <p>Specifically, AI agents and multi-agent systems (MAS), when properly orchestrated, would optimize a meaningful portion of research pipelines, flag methodological flaws early, and promote consistent documentation and quality control.</p> <p>These systems would lower the barrier to high-quality science. Smaller labs and early-career researchers, who often lack institutional support or internal review infrastructure, would be able to compete more equitably in a publish-or-perish ecosystem. The end goal is to increase both speed and quality, while leaving the trade-off between them to humans : more precision, less friction, faster iteration.</p> <p>In this document I outline use cases for integrating AI agents and MAS into academic research workflows. Each use case is structured around functional value and estimated technical feasibility. Specifically, the use cases are grounded in the context of computational and cognitive neuroscience, where these perspectives are most familiar to me. These examples should be understood as <strong>conceptual designs</strong>, intended to guide real-world lab experimentation.</p> <p>While MAS are promising, their current capabilities come with limitations. They are not yet mature enough for full task automation. However, they can already provide valuable assistance, particularly in structured environments where feedback-based correction allows for progressive refinement or post-hoc corrections. In this role, they function as <strong>intelligent scaffolds</strong>: supportive systems that enhance repetitive work prior to human oversight.</p> <p>As emphasized on each use case feasibility, Now is time for small-scale use cases; Later is time for complex use cases.</p> <p>Each section below describes a use case, the corresponding implications, the agentic solution (tools, tasks, agents, workflow), sample outputs, estimated weaknesses and recommendations, and the estimated feasibility.</p> <p>Each use case follows a nine-part structure:</p> <ol> <li>Define a) research needs that can be addressed and b) their implications for the practices.</li> <li>Propose a high-level pipeline composed of a) agents, b) tools, c) decomposed tasks and d) workflows.</li> <li>Assess a) potential weaknesses and b) suggest recommendations.</li> <li>Estimate feasibility based on current tools.</li> </ol> <p>I emphasize that this is a tentative, conceptual work, meant to guide future experiments, and that my expertise lies primarily in cognitive and neurocomputational sciences.</p> <p>General recommendations</p> <ul> <li> <strong>Use Structured Schemas (Pydantic)</strong>: Ensure all outputs (reports, metadata, environments) are structured and machine-parseable. This supports integration into continuous review pipelines and version tracking.</li> <li> <strong>Robust Workflow Orchestration</strong><br/> Use DAG-style control flows (e.g., LangGraph) to manage iteration, branching, and error handling for complex crews.</li> <li> <strong>Human-in-the-Loop Validation</strong><br/> Periodically calibrate agent behavior against real subject data to avoid overfitting to flawed prompt assumptions.</li> </ul> <p><strong>Purpose</strong></p> <p>Automate high-quality literature discovery and structured critique of scientific papers — assessing theoretical alignment, modeling validity, methodological bias, funding relevance, and ethical risks. This system will be <strong>augmentative</strong>, not replacing human experts. <strong>Internal documentation</strong>, including scientific papers, must be heavily leveraged to ensure the critiques and evaluations align with established research standards.</p> <p><strong>Implications: </strong>Faster, structured literature reviews; improved reproducibility via consistent critique schemas; early detection of theoretical or methodological flaws; template-driven alignment and drafting support.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Enforces structured schemas for outputs.</li> <li> <strong>External APIs</strong>: SerpAPI, PubMed, CrossRef, Semantic Scholar API for multi-source academic retrieval.</li> <li> <strong>Internal Documentation</strong>: Internal scientific documents, guidelines, style guides and domain-specific knowledge bases to ground evaluations in domain expertise.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Search &amp; Retrieve</strong>: Query APIs and retrieve relevant papers using natural-language queries and filters.</li> <li> <strong>Indexing &amp; Semantic Search</strong>: Build vector indices from retrieved papers and perform RAG-based retrieval to enhance search relevance.</li> <li> <strong>Classification</strong>: Tag documents by field, method, population, and relevance.</li> <li> <strong>Summarization</strong>: Extract structured summaries with citations and metadata, ensuring completeness but with limitations.</li> <li> <strong>Multi-Lens Critique</strong>: Run modular evaluation passes (theoretical validity, compliance risks, modeling rigor, demographic bias, etc.).</li> <li> <strong>Funding Relevance</strong>: Assist in grant application preparations by matching paper topics to relevant funding calls using past grant data and basic NLP.</li> </ul> <p><strong>Agent</strong></p> <ul> <li> <strong>Semantic Retriever</strong>: Interfaces with APIs and vector stores to surface relevant literature.</li> <li> <strong>Classifier</strong>: Labels documents based on methodology, field, and population studied.</li> <li> <strong>Summarizer:</strong> Extracts structured summaries with citation metadata.</li> <li> <strong>Synthesizer</strong>: Generates structured outputs, focusing on summarizing themes, key findings, and proposal elements.</li> <li> <strong>Modeling Critic:</strong> Assesses alignment of model with theoretical hypotheses or inappropriate model usage (theoretical alignment checklists, not full critique).</li> <li> <strong>Bias Auditor</strong>: Recommends analysis strategies (e.g., simulations, effect size calculations, power analysis) to assess bias and improve model robustness.</li> <li> <strong>Ethics &amp; Compliance Critic:</strong> Flags potential IRB, GDPR, and dual-use risks based on internal policy documents and rule-based checks.</li> </ul> <p><strong>Workflow Structure</strong></p> <ul> <li>CrewAI delegates task-specific agents across a modular critique pipeline.</li> <li>Optional chaining (e.g., LangGraph) routes papers between agents based on content type.</li> <li>State memory passes outputs (e.g., summaries, classifications) across stages.</li> <li>Pydantic schemas enforce structure for all agent outputs.</li> <li>Outputs are reviewed and refined iteratively using feedback from researchers.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li>`.bib`: BibTeX with relevance tags and critique annotations.</li> <li>` md`: Structured markdown synthesis and summaries.</li> <li>`.json`: Machine-readable JSON objects of evaluations and labels.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Automated Critique Limitations</strong>: AI cannot yet replace domain experts in assessing deep theoretical soundness. Critiques may be shallow or inconsistent.</li> <li> <strong>Retrieval Limitations</strong>: External APIs have incomplete coverage and rate limits. Missing full texts can degrade critique quality.</li> <li> <strong>Classification Accuracy</strong>: Automated tagging can misclassify boundary cases.</li> <li> <strong>Workflow Complexity</strong>: Multi-agent memory and coordination can create brittle pipelines.</li> <li> <strong>Expectation Management</strong>: Risk of users assuming full automation rather than augmentation.</li> <li> <strong>Prompt Engineering &amp; Schema Validation</strong>: Improper prompt structure can lead to inconsistent outputs.</li> <li> <strong>Internal Document Grounding</strong>: Failure to integrate internal documentation may reduce context accuracy and increase hallucinations.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li>Use checklist-driven critique templates and flagging systems to maintain transparency and guide human review.</li> <li>Prioritize summaries from abstracts and metadata. Integrate internal documents to fill gaps and improve accuracy.</li> <li>Implement hierarchical taxonomies and confidence scoring in classification. Allow manual corrections.</li> <li>Modularize agent roles to localize failure. Implement error logging and fallback mechanisms.</li> <li>Set clear user expectations through documentation and UI cues about AI’s supportive role.</li> <li>Use Pydantic schemas with early validation checks to ensure format integrity.</li> <li>Align all evaluation prompts with internal documentation, policies, and domain knowledge to improve grounding and reduce drift.</li> </ul> <p><strong>Feasibility</strong></p> <p>Many components are already in use individually: semantic search, structured summarization, and rule-based bias detection. Integrating them into a modular, agent-based workflow is feasible with current frameworks (e.g., CrewAI, LangGraph) and good prompt/schema design. Success depends on well-grounded internal documentation, careful agent calibration, and realistic scoping of critique depth. While LLMs can flag obvious issues, domain-specific judgment and iterative refinement are essential to avoid overreliance on superficial outputs. Mostly, systems seem not mature enough to finalize into a full-scope workflow. The entire proposition has a high degree of complexity. Can be broken down into systems of small scope.</p> <p><strong>Purpose</strong></p> <p>Create an <em>in silico</em> environment to simulate subject-like behavior across experimental conditions. The sandbox enables researchers to test and benchmark task designs (decision-making paradigms, game-theoretic setups, social phenomena, maybe abstracted mechanisms) prior to data collection. Simulated data can aid in estimating effect sizes, stress-testing models, and refining behavioral hypotheses. Cognitive modeling could be combined with AI-driven behavior to generate realistic trial-level data.</p> <p><strong>Implications</strong>: Cheap pilot testing, design optimization.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Cognitive modeling libraries </strong>(e.g., custom Bayesian inference engines, reinforcement learning models).</li> <li> <strong>Data validation frameworks </strong>(e.g., Pydantic for schema enforcement).</li> <li> <strong>Data science and visualization </strong>(e.g., pandas, seaborn, matplotlib).</li> <li> <strong>Simulation state stores </strong>(e.g., Redis, or in-memory Python structures).</li> <li> <strong>Notebooks or dashboards </strong>for exploratory analysis and parameter tuning.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li>Generate cognitive profiles with varying priors, learning rates, and strategies informed by explicit computational models or prompt-driven approximations.</li> <li>Execute trial-based simulations where agents perform decisions, produce confidence ratings, reaction times (RTs), and outcomes according to task rules.</li> <li>Record detailed trial logs with structured data schemas.</li> <li>Aggregate and analyze population-level behavior, producing statistical summaries and visual reports.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Persona Generator: </strong>Samples parameter sets from computational models or approximates cognitive traits using structured prompts.</li> <li> <strong>Task Executor:</strong> Performs trial-level decisions using a hybrid approach combining explicit model computations and LLM-generated behavior constrained by structured prompts.</li> <li> <strong>Interaction Logger:</strong> Validates and stores each trial’s outputs and metadata using formal (e.g., Pydantic) schemas.</li> <li> <strong>Results Summarizer:</strong> Generates statistical summaries, behavior plots, and variability estimates.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Simulation is initialized with N synthetic agents.</li> <li>Each agent executes a set of trials under specific experimental conditions.</li> <li>Central state tracks agent properties, task variables, and simulation history.</li> <li>Data is validated and logged continuously.</li> <li>After simulations, analysis agents produce visualizations and reports.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Structured logs (CSV, JSON): </strong>Trial-by-trial behavior with metadata.</li> <li> <strong>Visualizations: </strong>Learning curves, RT distributions, choice matrices, etc.</li> <li> <strong>Summary notebooks: </strong>Auto-generated for post-hoc analysis and reproducibility.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Cognitive Modeling Limitations: </strong>LLMs alone cannot precisely execute complex cognitive models; they approximate behavior based on textual descriptions and tend to suffer from WEIRD biases. Pure prompt-driven agents may produce inconsistencies; detecting the source of variability may not be possible.</li> <li> <strong>State and Interaction Complexity: </strong>Managing multi-agent state across iterative trials is challenging and prone to errors without robust centralized state management. Complex workflows increase architectural overhead and debugging difficulty.</li> <li> <strong>Scalability Constraints: </strong>Large-scale simulations require parallelization and resource optimization not inherently provided by CrewAI or LangGraph.</li> </ul> <p>· <strong>Output Dependence on Model Fidelity: </strong>The quality of logs and analyses depends heavily on the realism of underlying agent behaviors.</p> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Hybrid Modeling Approach</strong><br/> Combine explicit computational cognitive models with LLM-driven behavior generation.</li> <li> <strong>Structured Prompt Engineering</strong><br/> Guide agent responses using prompts that request structured outputs (e.g., JSON), ensuring parsability and reproducibility across trials. Pydantic schemas validate trial logs and agent outputs.</li> <li> <strong>Modular Agent Roles</strong><br/> Maintain separation of concerns (i.e., persona generation, task execution, logging, summarization) to ease debugging and extensibility.</li> <li> <strong>Centralized State Management</strong><br/> Employ memory stores to persist simulation state, agent parameters, and historical trial outcomes across loops.</li> <li> <strong>Parallelize Simulations</strong><br/> Run agent trials in parallel where feasible to boost throughput and scalability.</li> <li> <strong>Automate Statistical Reporting</strong><br/> Leverage Python data science tools (e.g., seaborn, pandas, scipy) to auto-generate behavioral plots and summary statistics for each simulation batch.</li> </ul> <p><strong>Feasibility</strong></p> <p>The approach has already been adopted in the early days of GPT3. Sound experimental designs should today yield data with sufficient quality to stress-test experimental designs (prompt outliers, test unforeseen behavior under the designed rules). While LLMs can approximate (WEIRD-like) model-driven behavior, integrating explicit computational models and robust state management should be tested. Chain-of-Thoughts may prove useful for testing mentalization. Scalability and validation may become challenges. Human oversight is obviously critical.</p> <p><strong>Purpose</strong></p> <p>Automate the generation, auditing, and maintenance of research project documentation, including READMEs, metadata, and code annotations — to enhance reproducibility, usability, and transparency. The goal is to reduce documentation debt while supporting compliance with FAIR and open science standards.</p> <p><strong>Implications</strong>: Higher likelihood of reproducibility; better alignment with data-sharing mandates and publication requirements.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Ensures that outputs like documentation audits, metadata files, and README components follow structured, machine-parseable schemas.</li> <li> <strong>Static Code Parsers</strong>: Tools for extracting function signatures, comments, and code structure (e.g., Python’s ast, inspect).</li> <li> <strong>Natural Language Generators</strong>: Lightweight LLM wrappers for docstring generation and README drafting, fine-tuned for scientific language.</li> <li> <strong>Metadata Validators</strong>: Schema.org and JSON-LD validators, FAIR compliance checkers, and data catalog schema tools.</li> <li> <strong>Version Control Interfaces</strong>: Git hooks and commit parsers to trace documentation lineage and link documentation updates to code changes.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>README Drafting</strong>: Auto-generate a project README including installation, usage, citation, and licensing information.</li> <li> <strong>Function Annotation</strong>: Generate or verify docstrings for functions and classes based on code logic and structure.</li> <li> <strong>File Role Tagging</strong>: Classify source files by role (e.g., data preprocessing, model training) to enhance navigation and metadata clarity.</li> <li> <strong>Metadata Generation</strong>: Produce structured, schema.org-compliant JSON-LD metadata for datasets, workflows, and scripts.</li> <li> <strong>Documentation Consistency Audit</strong>: Evaluate generated documentation for internal consistency and alignment with code behavior or test cases.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>README Composer</strong>: Builds structured project overviews by extracting key project metadata, dependencies, and usage examples.</li> <li> <strong>Code Annotator</strong>: Adds or updates inline docstrings by analyzing code structure and function behavior.</li> <li> <strong>Metadata Generator</strong>: Produces JSON-LD metadata templates, ensuring fields for identifiers, licensing, access, and file provenance.</li> <li> <strong>Documentation Auditor</strong>: Checks whether documentation reflects actual code behavior and tests alignment through symbolic reasoning or simple unit tests.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>The project’s file structure and scripts are parsed to identify documentation targets.</li> <li>Function and module annotations are generated, prioritizing high-level coverage over exhaustive line-by-line summaries.</li> <li>Project-level metadata and READMEs are drafted based on detected scripts, packages, and prior author contributions.</li> <li>Audit agents scan outputs for inconsistencies, missing fields, or misalignments between documentation and code behavior.</li> <li>All outputs are validated using Pydantic schemas and flagged for optional human review before publishing.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Standardized README (Markdown)</strong>: With sections for project purpose, usage, dependencies, author attribution, and citations.</li> <li> <strong>Inline Docstrings</strong>: Inserted or updated within source files, providing structured summaries of function logic, inputs, and outputs.</li> <li> <strong>Metadata Files</strong>: JSON-LD records suitable for FAIR repositories (e.g., Zenodo, Dataverse), including schema.org-compliant descriptions.</li> <li> <strong>Documentation Audit Reports</strong>: Highlighting missing annotations, inconsistencies between code and documentation, or metadata gaps.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Contextual Limits of Automation</strong>: Generated docstrings and READMEs may miss project-specific logic, edge cases, or architectural rationale.</li> <li> <strong>Superficial Metadata</strong>: FAIR metadata often appears valid structurally but lacks meaningful semantics without human input (e.g., vague licensing terms).</li> <li> <strong>Workflow Brittleness</strong>: Without integration into CI/CD pipelines or developer routines, documentation can quickly become outdated as the codebase evolves.</li> <li> <strong>Semantic Inaccuracy</strong>: While schema validation confirms structure, it cannot verify whether documentation truthfully describes code behavior.</li> <li> <strong>Overgeneralization Risks</strong>: LLM-based generators may produce generic or misleading summaries if not fine-tuned on project-specific conventions.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Use Structured Schemas</strong>: All documentation artifacts (README, docstrings, metadata) should be generated to conform to strict templates (validated by Pydantic), enabling downstream reuse and integration into project management pipelines.</li> <li> <strong>Layer Human Review Where Needed</strong>: Use automation for first drafts, but route critical sections (e.g., licensing, ethical use, and dataset sensitivity) to human reviewers before finalization.</li> <li> <strong>Enhance Context Awareness</strong>: Improve accuracy of generated content by incorporating architectural overviews (e.g., flowcharts or design notes) and prior documentation history.</li> <li> <strong>Integrate Documentation with Git</strong>: Trigger doc updates or audits on key commits. Maintain traceability by linking doc artifacts to their source commits or branches.</li> <li> <strong>Validate Documentation with Code Behavior</strong>: Check for inconsistencies (e.g., returns string instead of number) between generated docs and function outputs.</li> <li> <strong>Leverage Provenance Tracking</strong>: Link documentation to specific project phases or data artifacts (e.g., outputs from modeling or preprocessing) using version history and content hashes.</li> </ul> <p><strong>Feasibility</strong><br/> Automating documentation should be highly feasible for standard research project layouts where functions and workflows follow consistent patterns. Most tools required already exist. Challenge is ensuring semantic fidelity and domain alignment. While not a substitute for thoughtful writing, automated drafting and auditing dramatically reduce time and effort. This makes the system well-suited for pre-publication QA checks, onboarding kits, and FAIR submission preparation, otherwise minimal corrections.</p> <p><strong>Purpose</strong></p> <p>Automate the evaluation of research artifacts — code, data, and metadata — for reproducibility, transparency, and FAIR compliance. The system supports researchers in identifying reproducibility gaps, generating repair suggestions, and improving data/code stewardship with minimal manual effort. It does not replace human review but accelerates it through structured, automatable checks.</p> <p><strong>Implications</strong>: Increased transparency of research pipelines; early detection of reproducibility risks; semi-automated compliance with open science mandates.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Enforces structured schemas for outputs such as reproducibility reports, metadata records, and environment specifications.</li> <li> <strong>Static Analysis Libraries</strong>: AST parsers, regex scanners, and linters (e.g., Python’s ast, flake8, black) for code auditing.</li> <li> <strong>Metadata Validators</strong>: JSON schema validators for checking FAIR-aligned metadata (e.g., schema.org, CEDAR templates).</li> <li> <strong>Environment Introspection Tools</strong>: Tools such as pipdeptree, conda list, or Dockerfile parsers for dependency validation.</li> <li> <strong>Command-line Utilities</strong>: Git history analyzers, file checksum tools, and shell scripts for file integrity and version tracing.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Documentation Generation:</strong> Generate READMEs from project structure and code. Auto-generate function-level docstrings. Tag file purposes and interconnections.</li> <li> <strong>Dependency Mapping</strong>: Parse and reconstruct the project’s dependency graph using static analysis of environment files, setup scripts, and imports.</li> <li> <strong>Code Integrity Scanning</strong>: Detect reproducibility risks such as missing seeds, hardcoded paths, non-reproducible code patterns, and undocumented workflows.</li> <li> <strong>Metadata Validation</strong>: Assess the structure, presence, and standard-compliance of metadata files (e.g., schema.org, CEDAR templates).</li> <li> <strong>Workflow Reconstruction</strong>: Extract and sequence computational steps from notebooks, scripts, or workflow managers (e.g., Snakemake), forming a pipeline graph.</li> <li> <strong>FAIR Assessment</strong>: Evaluate the degree of compliance with FAIR principles, including findability (identifiers), accessibility (license fields), and interoperability (linked vocabularies).</li> <li> <strong>Repair Suggestion Generation</strong>: Propose corrective artifacts like Dockerfiles, environment.yaml, or JSON-LD metadata files to address flagged issues and enhance reproducibility.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Static Code Auditor</strong>: Scans scripts for reproducibility anti-patterns (e.g., unseeded randomness, implicit dependencies, or non-deterministic file paths).</li> <li> <strong>Metadata Validator</strong>: Checks presence, structure, and compliance of metadata files with community-accepted schemas.</li> <li> <strong>Workflow Reconstructor</strong>: Parses pipeline scripts or notebooks to infer workflow structure and dependency order.</li> <li> <strong>Reproducibility Synthesizer</strong>: Aggregates findings into structured, human-readable reports with suggestions for repair and improvement.</li> <li> <strong>FAIR Compliance Auditor:</strong> Maps data and metadata to FAIR metrics (e.g., unique identifiers, license clarity).</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Initial file scan identifies code, metadata, and documentation artifacts.</li> <li>Static analysis tools evaluate code structure and dependency graphs.</li> <li>Metadata files are parsed and checked against community schemas.</li> <li>Agents work in sequence or in parallel, validating, flagging, and generating outputs for review.</li> <li>All outputs conform to Pydantic-based schemas, allowing integration with reproducibility dashboards, continuous integration pipelines, or project documentation workflows.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Structured Reports (PDF, Markdown): </strong>Issue summaries (e.g., missing seeds, inconsistent environment specs) and recommended fixes.</li> <li> <strong>FAIR Metadata Templates:</strong> Auto-generated JSON-LD or YAML files aligned with schema.org or CEDAR templates.</li> <li> <strong>Environment Specifications:</strong> Machine-readable requirements.txt, environment.yaml, or Dockerfiles with version-pinned dependencies.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Partial Automation:</strong> FAIR principles like accessibility and ethical reuse still require human interpretation (e.g., licensing, IRB approvals).</li> <li> <strong>Workflow Incompleteness:</strong> Scripts often omit manual preprocessing steps or runtime logic that static tools can’t infer.</li> <li> <strong>Semantic Gaps:</strong> Schema validation can verify structure but not meaning (e.g., a license field may be present but semantically incorrect).</li> <li> <strong>Dependency Resolution Errors:</strong> Without actual environment builds, version conflicts and runtime incompatibilities may go undetected.</li> <li> <strong>False Positives:</strong> Static scans may wrongly flag hardcoded values or randomness that are actually justified.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Use Structured Schemas (Pydantic)</strong>: Ensure all outputs (reports, metadata, environments) are structured and machine-parseable. This supports integration into continuous review pipelines and version tracking.</li> <li> <strong>Modular Agent Roles</strong>: Keep agent responsibilities narrow (e.g., metadata validation vs code structure parsing) to reduce debugging complexity and enable domain-specific extensions (e.g., BIDS for neuroimaging).</li> <li> <strong>Incorporate Human Review Layers</strong>: Supplement automated scans with human checks, especially for FAIR “Accessibility” and data governance requirements.</li> <li> <strong>Track Provenance</strong>: Integrate tools or scripts that log file creation/modification lineage, aiding in data and workflow transparency.</li> <li> <strong>Validate in Real Environments</strong>: Where possible, test environment specifications by building containers or virtual environments. Use their logs as diagnostic tools.</li> <li> <strong>Generate Repair Artifacts</strong>: Suggest corrections, not just critiques. Auto-generate Dockerfiles, README scaffolds, and metadata records aligned with community templates.</li> </ul> <p><strong>Feasibility</strong><br/> Most components are independently mature: code linters, metadata schemas, and environment validators already exist in the open-source ecosystem. The challenge lies in orchestrating them coherently via AI agents, with outputs structured and actionable. Semantic validation and runtime reproducibility remain hard problems, but static analysis alone can surface critical red flags early. Integration with internal reproducibility checklists and domain-specific standards (e.g., neuroimaging BIDS, computational modeling standards) would enhance performance. Usefulness is high even if the system is only partially automated.</p> <p><strong>Purpose</strong></p> <p>Maintain a shared, persistent memory of research progress, decisions, discussions, and evolving plans to enhance team coordination, transparency, and project continuity. This assistant would act as a centralized, accessible repository for all key research milestones, tasks, and updates.</p> <p><strong>Implications</strong>: Mitigating knowledge loss.</p> <p><strong>Tools</strong></p> <ul> <li> <strong>Pydantic</strong>: Ensures structured validation of input (e.g., meeting summaries, decision logs) and output data (e.g., changelogs, dashboards) using predefined schemas.</li> <li> <strong>Natural Language Processing (NLP) Tools</strong>: Used for extracting and summarizing key points from meeting transcripts, chat logs, and research documentation.</li> <li> <strong>Project Management Integration</strong>: Tools like GitHub, Slack, Notion, Microsoft Teams for integrating meeting notes, code commits, task status, and other project updates. Otherwise Markdown and Word documentation.</li> <li> <strong>Version Control</strong>: To track iterative changes across code, data, and hypotheses.</li> </ul> <p><strong>Tasks</strong></p> <ul> <li> <strong>Communications Summarizer: </strong>Summarize team communications (e.g., meeting transcripts, chat logs, email summaries).</li> <li> <strong>Documentation Archiving: </strong>Archive project decisions, milestones, and revisions in a structured format.</li> <li> <strong>Documentation Updating: </strong>Update and maintain project timelines, research hypotheses, and data evolution records.</li> </ul> <p><strong>Agents</strong></p> <ul> <li> <strong>Meeting Summarizer:</strong> Extracts key actions, decisions, and unresolved points from transcripts or chat logs.</li> <li> <strong>Task Monitor:</strong> Tracks deliverables, deadlines, and milestone progress.</li> <li> <strong>Project Historian:</strong> Logs revisions and collaborative changes across datasets, code, and hypotheses.</li> </ul> <p><strong>Workflow</strong></p> <ul> <li>Meeting transcripts, chat logs, and code commits are ingested into the system.</li> <li>Summarization models extract and synthesize key discussion points, decisions, and task updates.</li> <li>All data is structured and validated according to predefined schemas (e.g., MeetingSummary, MilestoneLog).</li> <li>Generated outputs are time-stamped and stored in a shared memory system, accessible through dashboards and changelog files.</li> <li>Critical decisions or ambiguous points are flagged for team member review to ensure accuracy.</li> </ul> <p><strong>Outputs</strong></p> <ul> <li> <strong>Meeting Summaries</strong>: Markdown-based actionable, time-stamped points and decisions that are easy to reference for follow-up.</li> <li> <strong>Changelogs</strong>: Markdown-based changelogs capturing the evolution of hypotheses, datasets, and code changes over time.</li> <li> <strong>Structured Logs</strong>: Compliant with Pydantic schemas, offering machine-readable logs and summaries for automated or external use.</li> </ul> <p><strong>Weaknesses</strong></p> <ul> <li> <strong>Context Loss in Summarization</strong>: Summarization models may overlook nuanced details, context, or domain-specific language, resulting in incomplete or misinterpreted action items.</li> <li> <strong>Dynamic Memory Management</strong>: Maintaining a coherent, up-to-date shared memory that accurately captures evolving decisions and hypotheses can be difficult to manage, especially as the project grows.</li> <li> <strong>Integration Complexity</strong>: Combining data from diverse sources (e.g., meeting transcripts, chat logs, commits) into a unified timeline requires robust tools and processes.</li> </ul> <p><strong>Recommendations</strong></p> <ul> <li> <strong>Human-in-the-Loop Validation</strong>: Incorporate a step for team members to review and refine AI-generated summaries and task updates to ensure accuracy. This can be achieved by flagging high-priority content for team review or approval.</li> <li> <strong>Domain-Specific Fine-Tuning</strong>: Fine-tune summarization models on project-specific data or domain-relevant terminology to improve contextual understanding and minimize errors from jargon or specialized language.</li> <li> <strong>State and Conflict Management</strong>: Implement versioning and conflict resolution mechanisms for the shared memory system. This includes tracking updates and ensuring that inconsistent or contradictory inputs are flagged for manual intervention.</li> <li> <strong>Seamless Tool Integration</strong>: Integrate with existing collaboration and version control tools (e.g., Slack, GitHub, Microsoft Teams) via APIs to minimize friction and ensure data consistency.</li> <li> <strong>Provenance and History Linking</strong>: Use version control systems and data lineage tracking to connect documentation, decisions, and code changes to relevant project phases or discussions.</li> </ul> <p><strong>Feasibility</strong><br/> This use case is highly feasible, especially considering the capabilities of existing AI tools for summarization and task tracking. Many of the core components (meeting transcription, summarization, task tracking, and changelog generation) are already mature in tools like Otter.ai, Microsoft Teams, Slack, or Zoom. Primary challenges lie in maintaining a coherent, evolving memory that reflects the project’s evolution while ensuring user trust in the system. A hybrid human-AI workflow, with strong integration into existing tools and a structured approach to validation and memory management, would help mitigate risks associated with incomplete or inaccurate documentation. Requires a strong foundation, especially to integrate version control practices.</p> <p>The integration of AI agents and multi-agent systems into workflows is obviously a paradigm shift. As AI agents’ capabilities scale, research in the academia will undergo qualitative leaps. First, it represents the opportunity to diminish the burden of repetitive tasks, and the ability for resource-constrained labs to operate on a more level playing field with well-funded teams.</p> <p>Mostly, the ability to iterate quickly, access operational support with ease, and avoid methodological errors enable more efficient, transparent, and reproducible research practices. In this context, AI agents themselves function as reproducible, standardized tools, making task execution more portable across machines, labs, and institutions. This standardization increases the potential for interdisciplinary collaboration by reducing the friction across domains.</p> <p>AI agents will inevitably become part of daily practices, just like LLMs became part of our daily life (deep research, chain-of-thought, documents summarization, emails generation, assisted writing). AI agents will eventually help us move past bottlenecks, automate tedious tasks, and ensure that our work remains consistent and reproducible. This is especially relevant as funding research may become challenged in the next few years, open data will yield a whole strand of research dedicated to replicating results or reusing data, and emerging countries will increasingly take part to modern research.</p> <p>AI agents were funky until last year; the possibilities they present are now no longer just speculative. Though, they require experimenting in the lab until toolboxes are available.</p> <p>To stay updated on future developments, connect with me on <a href="https://github.com/ValentinGuigon">GitHub</a> or reach out directly.</p> <p>From my row house in DC,</p> <p>A bientôt,</p> <p><strong>Author:</strong><em><br/> </em><a href="https://valentinguigon.github.io/"><em>Valentin Guigon</em></a><em>, PhD<br/> Postdoctoral researcher,<br/> </em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/> University of Maryland, College Park, MD, USA</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=529a12347269" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="academia"/><category term="experiment"/><category term="ai"/><category term="research"/></entry><entry><title type="html">Workflow for a Reproducible Research</title><link href="https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research/" rel="alternate" type="text/html" title="Workflow for a Reproducible Research"/><published>2024-11-03T16:59:37+00:00</published><updated>2024-11-03T16:59:37+00:00</updated><id>https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2024-11-03-workflow-for-a-reproducible-research/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*psp9EjuuirWpr714dtCHpA.jpeg"/></figure> <p>This workflow is part of a guide about reproducibility in academic research, specifically focusing on computational analysis. It is based on my practices as a PhD and postdoctoral researcher in the academia and takes inspiration on DevOps and MLOps.</p> <p>It is a companion to the <a href="https://medium.com/@valentin.guigon/reproducibility-in-research-a-practical-guide-2-3-a-workflow-for-a-reproducible-research-f8d6c58e81cf">second part of a three-parts series about reproducibility in academic research</a>, specifically focusing on computational analysis.</p> <p>In the first part of this series, we identified two key pillars of reproducibility:</p> <ol> <li>Adhering to the FAIR principles — making data <strong>Findable</strong>, <strong>Accessible</strong>, <strong>Interoperable</strong>, and <strong>Reusable.</strong> </li> <li>Constructing stable computational environments to maintain consistency in methodology and conditions over time.</li> </ol> <p>Here we address the construction of a workflow, the third pillar of a reproducible research. Specifically, we aim to establish a systematic series of steps that serve as the foundation for any research project. This workflow is intended to facilitate the creation of a stable computational environment aligned with the FAIR principles and <strong>reusable</strong> across projects.</p> <p>The result should be a replication package that could be handled as is to reviewers.</p> <h4>I. Setup the <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">structure of the research project</a> </h4> <ol><li> <strong>Outline</strong> a consistent structure across projects</li></ol> <ul> <li>Folders<br/>— Data<br/>— Documents<br/>— Notebooks<br/>— Scripts</li> <li>Config files</li> <li><a href="http://README.md">README.md</a></li> </ul> <ol> <li> <strong>Outline</strong> the <a href="https://occasionaldivergences.com/posts/rep-env/">virtual environments</a> </li> <li> <strong>Build</strong> the directory with a service standardization<br/>- <a href="https://github.com/ValentinGuigon/cookiecutter-neuro-research-project">Cookiecutter</a> </li> </ol> <h4>II. Implement version control by initializing a project as a git repository</h4> <h4>III. Set up your code management and data management conventions</h4> <ul> <li>Define a <a href="https://laneguides.stanford.edu/DataManagement/Organizing">coding naming organization</a> </li> <li>Define a <a href="https://www.loc.gov/preservation/resources/rfs/format-pref-summary.html">data saving organization</a> </li> </ul> <h4>I. Load and work within your virtual environments</h4> <h4>II. Start by using computational notebooks</h4> <ol> <li>First, outline the code in pseudocode</li> <li>Then use notebooks as playground</li> </ol> <h4>III. Then switch to scripts</h4> <ol><li> <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md">Refactor notebooks into scripts</a><br/>— Processing scripts<br/>— Analysis scripts<br/>— Data appendix scripts</li></ol> <p>2. <a href="https://medium.com/data-science-at-microsoft/testing-practices-for-data-science-applications-using-python-71c271cd8b5e">Test your scripts</a></p> <h4>I. Follow a scripting Workflow</h4> <ol> <li>Start with notebooks as playground then <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md">refactor into scripts</a> </li> <li>Follow and maintain code good practices<br/>- Follow <a href="https://gist.github.com/wojteklu/73c6914cc446146b8b533c0988cf8d29">clean code general rules</a><br/>- Stick to <a href="https://dev.to/gervaisamoah/a-guide-to-clean-code-the-power-of-good-names-3f6i">conventions</a> when naming things<br/>- If you copy/paste, use templates/boilerplates (<a href="https://medium.com/@bluucaterpilla/a-data-science-boilerplate-%E0%B2%A0%E1%B4%97%E0%B2%A0-ff1fd5cfe84e">examples here</a>)</li> </ol> <p>3. Generate automatic reports (results reports into .html with <em>.ipynb</em> (Python) or <a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html"><em>nb.html</em> (R)</a>)</p> <p>4. Maintain version control</p> <p>5. Use automatic code formatter &amp; linter for your IDE</p> <p>6. Orchestrate the execution of the computations with a batch/workflow</p> <h4>II. Document the code and the data</h4> <ol><li><strong>Maintain up-to-date Documentation</strong></li></ol> <ul> <li>Project-level <a href="http://README.md">README.md</a> </li> <li>Subfolder-level <a href="http://README.md">README.md</a> </li> <li>Metadata/Cards:<br/>— <a href="https://huggingface.co/docs/hub/model-cards">Model cards</a><br/>— <a href="https://huggingface.co/docs/hub/datasets-cards">Dataset cards</a><br/>— <a href="https://huggingface.co/docs/datasets/main/en/repository_structure">Directory cards</a> </li> <li><a href="https://egonw.github.io/cookbook-dev/content/recipes/interoperability/creating-data-dictionary.html#an-example-of-data-dictionary">Data dictionary</a></li> <li><a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html">Automatic reports</a></li> </ul> <h4>III. Maintain quality checks</h4> <p>From my row house in DC,</p> <p>Merci pour votre temps</p> <p>Et à bientôt,</p> <blockquote> <strong>Author:</strong><em><br/>Valentin Guigon, PhD<br/>Postdoctoral researcher,<br/></em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/>University of Maryland, College Park, MD, USA</em> </blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b91207fe5b14" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="research"/><category term="mlops"/><category term="academia"/><category term="open-science"/></entry><entry><title type="html">Reproducibility in Research, a practical guide (2/3): A workflow for a Reproducible Research</title><link href="https://valentinguigon.github.io/articles/2024-11-03-reproducibility-in-research-a-practical-guide-23-a-workflow-for-a-reproducible-research/" rel="alternate" type="text/html" title="Reproducibility in Research, a practical guide (2/3): A workflow for a Reproducible Research"/><published>2024-11-03T16:59:06+00:00</published><updated>2024-11-03T16:59:06+00:00</updated><id>https://valentinguigon.github.io/articles/2024-11-03-reproducibility-in-research-a-practical-guide-23-a-workflow-for-a-reproducible-research</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2024-11-03-reproducibility-in-research-a-practical-guide-23-a-workflow-for-a-reproducible-research/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qsrihqClibsQcO05"/><figcaption>Photo by <a href="https://unsplash.com/@karsten116?utm_source=medium&amp;utm_medium=referral">Karsten Winegeart</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>This guide took roots from the ethos of Open Science. In the first post of this three-part series on reproducibility in academic research, I argued that <a href="https://medium.com/@valentin.guigon/reproducibility-in-academic-research-1-3-key-components-of-a-reproducible-research-413f27cd0886">FAIR principles and stable computational environments are essential to a reproducible research</a>. The current post delves into building a practical research workflow that relies on stable computational environments and adheres to FAIR principles.</p> <p>As a disclaimer, I make a <a href="https://www.ncbi.nlm.nih.gov/books/NBK547531/">distinction between <em>reproducibility</em> and <em>replicability</em></a>. For a more precise look at what reproducibility means, Nature has dedicated in 2018 a special archive on the <a href="http://www.nature.com/news/reproducibility-1.17552">Challenges in Irreproducible Research</a>. For technical aspects, you can follow this <a href="https://lms.fun-mooc.fr/courses/course-v1:inria+41023+session01/info">very comprehensive online course from INRIA</a>. You can also read PlosOne <a href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1003285&amp;type=printable">10 simple rules for reproducible computational research</a>. <strong>For a more concise version, you can find the </strong><a href="https://medium.com/@valentin.guigon/workflow-for-a-reproducible-research-b91207fe5b14"><strong>blueprint of my current research workflow</strong></a><strong> as a companion post.</strong></p> <p>I’ve been writing this post to the best of my ability. Please feel free to reach for any reason.</p> <p><a href="https://medium.com/@valentin.guigon/reproducibility-in-academic-research-1-3-key-components-of-a-reproducible-research-413f27cd0886">In the first part of this series, we identified two key pillars of reproducibility</a>:</p> <ol> <li>Adhering to the FAIR principles — making data <strong>Findable</strong>, <strong>Accessible</strong>, <strong>Interoperable</strong>, and <strong>Reusable</strong> — which serve as essential guidelines for effective data management.</li> <li>Constructing stable computational environments to maintain consistency in methodology and conditions over time.</li> </ol> <p>The current blog post will address the construction of a workflow. Specifically, we aim to establish a systematic series of steps that serve as the foundation for our research. This workflow will facilitate the creation of a stable computational environment aligned with the FAIR principles and <strong>reusable</strong> across projects.</p> <p>The first layer of the workflow involves building the environment as a directory. Executing scripts within this directory is intended to recover the results of our computations. The directory is intended to be shared as is with, for instance, reviewers — a replication package.</p> <p>The second layer will focus on encapsulating the project directory within a container isolating the computational environment and controlling its dependencies.</p> <p>The third layer will address the automation of the container.</p> <p>Overall, we will examine four facets of reproducibility: <strong>Organization, Automation, Documentation.</strong> <strong>Dissemination </strong>is addressed as sharing a replication package.</p> <p>This post is extensive. You can find a concise summary below and the blueprint for my current workflow <a href="https://medium.com/@valentin.guigon/reproducibility-in-academic-research-1-3-key-components-of-a-reproducible-research-413f27cd0886">here</a>.</p> <ul> <li><strong>Initialize Project Structure:</strong></li> <li>— Use <a href="https://cookiecutter.readthedocs.io/en/latest/overview.html">Cookiecutter</a> to create a consistent project template.</li> <li>— Initialize a <a href="https://git-scm.com/">Git </a>repository with git init.</li> <li>— Initialize data management (e.g., <a href="https://git-annex.branchable.com/">Git Annex</a>, <a href="https://dvc.org/doc/start">DVC</a>).</li> <li><strong>Documentation:</strong></li> <li>— Write project-level documentation and file-level documentation (e.g., README.md, data dictionary, cards).</li> <li><strong>Development Process:</strong></li> <li>— Start coding in computational notebooks for exploration and prototyping.</li> <li>— Refactor notebooks into organized scripts</li> <li><strong>Reporting:</strong></li> <li>— Generate automatic output reports in HTML format from notebooks or scripts.</li> <li><strong>Execution:</strong></li> <li>— Write a master script to orchestrate the execution of the computational pipeline as a batch.</li> </ul> <ul> <li><strong>Functional Package Management:</strong></li> <li>— Manage dependencies using a functional package manager (e.g., <a href="https://nixos.org/download/">Nix</a>, <a href="https://guix.gnu.org/">Guix</a>).</li> <li><strong>Containerization:</strong></li> <li>— Package the project into a container (e.g., <a href="https://www.docker.com/">Docker</a>).</li> </ul> <ul> <li><strong>Establish Computational Workflow:</strong></li> <li>— Turn the master script into a workflow (e.g., <a href="https://makefiletutorial.com/">Makefile</a>, <a href="https://snakemake.readthedocs.io/en/stable/">SnakeMake</a>, <a href="https://github.com/resources/articles/devops/ci-cd">CI/CD</a>).</li> <li>— Write a recipe for the container that will: pull data from repository (e.g., <a href="https://figshare.com/">Figshare</a>, <a href="https://zenodo.org/">Zenodo</a>), pull code from repository (<a href="https://github.com/">GitHub </a>or <a href="https://about.gitlab.com/">GitLab</a>), execute the workflow.</li> </ul> <ul> <li>Create a project on a research management platform (e.g., <a href="http://osf.io/">OSF.io</a>)</li> <li>Associate links to the code repository (e.g., <a href="https://github.com/">GitHub</a>, <a href="https://about.gitlab.com/">GitLab</a>)</li> <li>Associate links to the data repository (e.g., <a href="https://figshare.com/">Zenodo</a>, <a href="https://zenodo.org/">Figshare</a>)</li> <li>In case of container:</li> <li>— Include the container recipe in the project on the research management platform</li> </ul> <ul> <li>Layer I. Organization / Set-up / outlining / only at the initiation of a project<br/>A. Outline a consistent structure across projects<br/>B. Outline virtual environments<br/>C. Use a service standardization for building the directory<br/>D. Implement version control by initializing a project as a git repository<br/>E. Set up your code management and data management conventions</li> <li>Layer I. Automation / New and recurrent processes<br/>A. Operate within virtual environments<br/>B. Start with Notebooks as Playground<br/>C. Refactor Notebooks into Scripts<br/>D. Use Appropriate Tools and Libraries to manipulating data by hand<br/>E. Follow good Coding Practices<br/>F. Maintain version control<br/>G. Use an automatic code formatter &amp; linter for your IDE<br/>H. Implement Testing</li> <li>Layer I. Document the code and the data<br/>A. What to document?<br/>B. How to document?</li> <li>Layer II. Isolation and containers<br/>A. Introduction to Containers<br/>B. Example: Docker: A Versatile Containerization Platform for Research</li> <li>Layer II. Functional package managers<br/>A. Introduction to Functional Package Managers</li> <li>Layer III. Workflows<br/>A. Introduction to Workflows<br/>B. Types of Workflow Systems<br/>C. Implementation Strategies</li> </ul> <p>This relates to organizing a project as a directory on the computer (<a href="https://laneguides.stanford.edu/DataManagement/Organizing">more here</a>). Be consistent, clear, structured in the creation and maintaining of directories, naming, files and file contents. Others and yourself should be able to navigate the project without a map.</p> <ol> <li>Create a <strong>main project directory</strong> with an informative name.</li> <li>Create <strong>subdirectories </strong>for different components (e.g., data, analysis, documents).<br/>a. Create a dedicated output subdirectory for <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/paper/">reports</a> and/or figures<br/>b. Create a <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/readme/">README</a> file describing the project and the directory<br/>c. Keep raw data separate from derived data.</li> <li>Make<strong> raw data read-only</strong> to prevent accidental modifications</li> <li>Keep <strong>code separate from data</strong>.</li> </ol> <p>This can be achieved by defining a directory structure at the start of the project. For instance:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*j7MSpkgOOMrjXNB-.png"/><figcaption>(Taken from the <a href="https://laneguides.stanford.edu/DataManagement/Organizing">Stanford edu</a>)</figcaption></figure> <p>A virtual environment is an isolated, self-contained directory that contains a specific version of a programming language and its associated libraries and dependencies. It allows you to create separate environments for different projects, each with its own set of installed packages and versions.</p> <p><a href="https://occasionaldivergences.com/posts/rep-env/"><strong>Why Use Virtual Environments?</strong></a></p> <ol> <li> <strong>Dependency isolation</strong>: Different projects require different versions of libraries/packages/dependencies. Virtual environments prevent conflicts between them.</li> <li> <strong>Reproducibility</strong>: Virtual environments make it easier to recreate the exact environment in which a project was developed, with one simple command.</li> <li> <strong>Clean system</strong>: They keep your system Python/R/else installation clean by installing project-specific packages in isolated environments.</li> <li> <strong>Version control</strong>: Virtual environments can be easily version-controlled, allowing you to track changes in project dependencies over time.</li> <li> <strong>Easier collaboration</strong>: They make it simpler for other developers to set up and run your project with the correct dependencies.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qepljIfEIsADLRPN"/><figcaption>Taken from <a href="https://www.linkedin.com/pulse/python-virtual-environments-tutorial-using-virtualenv-dmitriy-zub/"><strong>Dmitriy Zub</strong></a></figcaption></figure> <p>By restoring the virtual environment each time you want to work on your project, you keep the project clean an contained. Several solutions exist for Python, R and MATLAB:</p> <ul> <li> <strong>Python</strong> has built-in support for managing project-specific configurations and dependencies (i.e., virtual environment) through the <strong>venv</strong> module<br/><strong>- Initiate the </strong><strong>venv</strong> </li> <li> <strong>R</strong> manages project-specific configurations with the .Rprj project files (i.e., Select File &gt; New Project…) and uses for <strong>renv</strong> package for virtual environments<br/><strong>- Create the </strong><strong>.Rprj project<br/>- Install the </strong><strong>renv</strong> </li> <li> <strong>MATLAB</strong> manages project-specific configurations with the .Prj project files (i.e., New &gt; Project &gt; Blank Project). Whereas it doesn't offer a true virtual environment, the workspace isolation via paths and projects allows to manage dependencies in a controlled way<br/><strong>- Create the </strong><strong>.Prj project</strong> </li> </ul> <p><a href="https://cookiecutter.readthedocs.io/en/stable/overview.html">Cookiecutter</a> is a wonderful solution to realize consistently and rapidly a project structure. Cookiecutter takes a <a href="https://www.cookiecutter.io/templates">template </a>file, prompts the user with few questions and outputs a standardized directory structure.</p> <p>Acting as an executable blueprint, Cookiecutter enforces standards across projects, making its components by essence Findable, Accessible, Interoperable and Reusable. Thanks to its automation, human errors are eliminated and projects get built with a higher velocity.</p> <p>If looking for authoritative arguments, here are <a href="https://gitlab.has-sante.fr/has-sante/public/cookiecutter-prod-python-has">France’s Haute Autorité de Santé template</a> and UK GOV’s <a href="https://dataingovernment.blog.gov.uk/2021/07/20/govcookiecutter-a-template-for-data-science-projects/">govcookiecutter</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Gr2vXc7bSZ8byydw.png"/><figcaption><a href="https://github.com/ukgovdatascience/govcookiecutter">Govcookiecutter</a></figcaption></figure> <p>My own template for cognitive neurosciences projects is available <a href="https://github.com/ValentinGuigon/cookiecutter-neuro-research-project">here</a>. It initializes a directory structure and, given prompts, initiates MATLAB/Python/R projects with renv, .Rprj, renv, .Prj .</p> <p><a href="https://about.gitlab.com/topics/version-control/">Version control is essential for tracking changes, collaborating with others, and maintaining the integrity of your research project.</a> It enables a secure, dynamic, and collaborative approach to developing code and managing research projects. You can consider it a backbone for reproducibility. <a href="https://git-scm.com/">Git </a>is the most widely used version control system. Importantly, code, data, model parameters and documentation can all be version controlled. You can find further motivation and a guide <a href="https://neurathsboat.blog/post/git-intro/">here</a>.</p> <p>A Version Control System (VCS) like Git allows to:</p> <ol> <li>Capture snapshots of your code/data/documentation/parameters as you develop, enabling you to revert to previous states if needed.</li> <li>Create branches to experiment with new ideas without risking your main codebase.</li> <li>Merge different versions of as code and documentation, facilitating the integration of new features or experiments.</li> <li>Collaborate effectively with other researchers by sharing and merging code changes.</li> <li>Track the evolution of your project over time, including who made specific changes and why.</li> </ol> <p>In other words, versioning help you freeze computational states in time, and revert from an ulterior state to an anterior state. <a href="https://www.codecademy.com/article/f1-u3-git-setup">A simple tutorial for associating a local directory with a remote repository hosted on GitHub can be found here</a>.</p> <p><em>File should:</em></p> <ul> <li> <strong>Be human readable</strong> — you should understand the content of the file from its name alone</li> <li> <strong>Be machine readable</strong> — don’t use spaces (_ is preferable), special characters or accents</li> </ul> <p>Organize file naming:</p> <ul> <li>Include information sufficient to easily locate a specific file.</li> <li>Make sure file names are unique, descriptive and meaningful.</li> </ul> <p>Organize files content</p> <ul><li>The principles of file naming and organization can also be applied within files, for naming variables, columns, rows, values</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/758/0*fm5YUkJ3O3JXQ3V-.png"/><figcaption>(Example from <a href="https://laneguides.stanford.edu/DataManagement/Organizing">Stanford edu</a>)</figcaption></figure> <p>When <strong>preserving/publishing</strong> digital content it’s essential to save the work in an appropriate file format to ensure long-term preservation and accessibility. It is also important to save twice: in a specific format for the scientific work and in open format for preserving.</p> <p>Privilege formats that are:</p> <ul> <li><strong>widely used within your discipline</strong></li> <li><strong>open, non-proprietary, unencrypted and uncompressed (except if lossless compression)</strong></li> <li> <strong>self-documenting</strong> (i.e. the file itself can include useful metadata)</li> <li> <strong>publicly documented</strong> (i.e. the complete file specification is publicly available) &amp; <strong>endorsed by standards agencies</strong> (e.g;, ISO standard)</li> </ul> <p>For instance, <strong>not .docx, .xls</strong>, etc. Rather, <strong>.csv for data</strong> and <strong>.tiff for images</strong>. <a href="https://www.loc.gov/preservation/resources/rfs/"><strong>Check the Library of Congress Recommended Formats list to see what is recommended for your file type.</strong></a></p> <p>By restoring the virtual environment each time you want to work on your project, you keep the project clean an contained.</p> <p>At the end/beginning on the day, close/load the virtual environments:</p> <ul> <li> <strong>On Python, load the </strong><strong>venv</strong> </li> <li> <strong>On R, load the </strong><strong>.Rprj project &amp; restore the </strong><strong>renv via </strong><strong>renv::restore()</strong> </li> <li> <strong>On MATLAB, load the </strong><strong>.Prj project</strong> </li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/612/0*GucROonzXE4aU9Lo.png"/><figcaption>3 simples commands to run a stable renv: init(), snapshot() and restore(). Taken from <a href="https://rstudio.github.io/renv/articles/renv.html">renv website</a>.</figcaption></figure> <ul> <li> <strong>Use computational notebooks</strong> (e.g., Jupyter Notebooks, R Markdown) <strong>for initial exploration</strong>.</li> <li>Document your dependencies, analyses, results, interpretations, comments.</li> <li>Keep notebooks organized and readable by:<br/>- Moving reusable code to external files.<br/>- Defining key variables at the top.<br/>- Restarting kernels periodically to ensure reproducibility.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UMSDmLDY3_U1cYJD.png"/><figcaption>How R notebooks operate: write scripts as chunks of code on the left; generate .hmtl report on the right. Taken from <a href="https://rmarkdown.rstudio.com/lesson-10.html">rmarkdown website</a>.</figcaption></figure> <p>Once you are done with exploration, convert your notebook code into robust, reusable scripts.</p> <p>Notebooks are very often messy and not optimized for executing code efficiently — unless moving source code into robust, reusable scripts. Consider notebooks in a final project for producing automated reports.</p> <p>To refactor notebooks into scripts, <a href="https://towardsdatascience.com/how-to-refactor-a-jupyter-notebook-ed531b6a17">follow a structured refactoring process:</a></p> <ul> <li>1. Ensure the notebook runs without errors.</li> <li>2. Identify code blocks that can be turned into functions.</li> <li>3. Write tests for these functions.</li> <li>4. Create separate Python/R modules for these functions.</li> <li>5. Replace notebook code with calls to these new functions.</li> <li>6. Verify the refactored notebook still produces the same results.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/0*kMwrtpwkqgSp0fDN.png"/><figcaption><a href="https://towardsdatascience.com/how-to-refactor-a-jupyter-notebook-ed531b6a17">David Tan’s guide on How to refactor a jupyter notebook</a></figcaption></figure> <ul> <li> <strong>For data manipulation</strong>: <a href="https://swcarpentry.github.io/r-novice-gapminder/12-dplyr.html">dplyr</a>, <a href="https://swcarpentry.github.io/r-novice-gapminder/13-tidyr.html">tidyr</a> (R); pandas, numpy (Python)</li> <li> <strong>For visualization</strong>: ggplot2 (R); matplotlib, seaborn (Python)</li> <li> <strong>For report generation</strong>: <a href="https://swcarpentry.github.io/r-novice-gapminder/14-knitr-markdown.html">R Markdown</a>, <a href="https://jupyter.org/">Jupyter Notebooks</a>, <a href="https://www.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script-or-function.html">MATLAB Live Scripts</a> </li> </ul> <ul> <li>Follow the DRY (<strong>Don’t Repeat Yourself</strong>) principle.</li> <li> <strong>Write modular</strong>, single-purpose functions.</li> <li> <strong>Use clear, descriptive names</strong> for variables and functions.</li> <li> <strong>Make code readable</strong> and include comments to tell what and why, not how</li> <li> <strong>Reduce complexity, </strong>break down problem into bite size pieces</li> <li>Know that your code is doing the right thing (<strong>test your functions</strong>)</li> <li> <strong>Use version control</strong> consistently.</li> </ul> <p><strong>First, a disclaimer on the uses of Git:<br/></strong>Git is not intended for handling large data files. Saving computational modelling output files (e.g., 50mb .RData models) in a git repository very often prompts Git to asks users for a different system called<a href="https://git-lfs.com/"> LFS (Large File Storage)</a>. <strong>GitHub and GitLab are NOT intended for storing large files</strong>, and doing so will cost personal account data storage, or money.</p> <p>To properly use and maintain version control without the research project:</p> <ol> <li><strong>Make sure you initialized a Git repository:</strong></li> <li> <strong>Version control your code, documentation, and configuration files:</strong><br/>- Use <a href="https://git-scm.com/">Git</a> to track changes in your source code, documentation, and configuration files.<br/>- Host your repository on platforms like GitHub or GitLab.</li> <li> <strong>Version control your datasets:</strong><br/>- Use <a href="https://git-annex.branchable.com/">Git Annex</a> to store files on data repositories and pointers to data on Git repositories.<strong><br/></strong>- Or use <a href="https://dvc.org/">DVC</a> to track changes in data when performing ML.<br/>- Host your data on secured platforms (<a href="https://zenodo.org/records/10651775">data repositories here</a>)</li> <li> <strong>Track your experiment parameters and results:</strong><br/>- Save scripts, environment configuration files, data, evaluation metrics, training parameters, model weights and visualizations used in ML with Experiment tracking tools (e.g., <a href="https://mlflow.org/">MLflow</a>, <a href="https://dvc.org/">DVC</a>, <a href="https://wandb.ai/site">Weights &amp; Biases</a>)</li> <li> <strong>Follow best practices:</strong><br/>- Commit frequently with meaningful commit messages.<br/>- Use .gitignore to exclude unnecessary files.<br/>- Tag important milestones or versions.<br/>- Use branches for different features or experiments.<br/>- Regularly push changes to remote repositories for backup and collaboration.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*PkBs71xJcbk9_sXt.png"/><figcaption>Git simplest workflow: a master (main) branch and a feature branch. Taken from <a href="https://medium.com/@abeythilakeudara3/version-control-system-cabd8d120986">Udara Abeythilake</a>.</figcaption></figure> <p>If you use an IDE (e.g., Visual Code Studio, RStudio), code formatters (e.g., <a href="https://prettier.io/">Prettier</a>, <a href="https://www.tidyverse.org/blog/2017/12/styler-1.0.0/">styler</a>) and code linters (e.g., <a href="https://www.pylint.org/">Pylint</a>) may be useful, as they will automatically check the code for errors and format the code in a readable and conventional format when hitting save.</p> <p>Choosing the appropriate formatter and a linter may depend on the language, framework, conventions decided for the project.</p> <p>Testing is often overlooked until conflicts and confusions emerge in data. Resolving these issues ex-post often requires reverse-engineering complex processes. To avoid this, implement testing from the start and make it a habit. Testing has the benefits of:</p> <ol> <li>Helping ensure that analyses produce correct and consistent outputs, reducing the risk of errors of data alterations or wrong conclusions.</li> <li>Acting as executable documentation, making it easier for others (or yourself in the future) to understand and replicate the analysis by reading the tests outputs.</li> <li>Providing a common reference point for understanding how code and analyses should behave, facilitating teamwork.</li> <li>Helping in refining methods over time, knowing that existing functionality remains intact.</li> <li>Leading to more modular and maintainable code.</li> </ol> <p><a href="https://medium.com/data-science-at-microsoft/testing-practices-for-data-science-applications-using-python-71c271cd8b5e">Examples</a>:</p> <ol> <li> <strong>Validation by Asserting</strong>: Condition the execution of steps in your script by implementing an assert test that throws an error if a condition is false.</li> <li> <strong>Unit Testing</strong>: Write tests for individual functions or components of your analysis. If you have a function that calculates a specific statistic, create a test that verifies it produces the correct output for known inputs.</li> <li> <strong>Integration Testing</strong>: Test how different parts of your analysis pipeline work together. Run your entire workflow from data input to final output, ensuring that the complete process produces expected results.</li> <li> <strong>Comparing Outputs</strong>: Run identical analyses in both notebook and script formats, then compare the results. This practice helps ensure that transitioning from exploratory notebooks to final scripts does not introduce errors or inconsistencies in your findings.</li> <li> <strong>Quality Checks</strong>: Verify the integrity of your data. For instance, check the content of a variable after altering the preprocessing. trigger a head() to check data format or output the first images of a brain scan.</li> </ol> <p>Proper documentation is crucial for reproducibility, collaboration, and long-term project maintenance.</p> <p>In general, <a href="https://laneguides.stanford.edu/DataManagement/Documenting">you should maintain documentation at both the <strong>project level</strong> and the <strong>file level</strong></a>.</p> <ul> <li> <strong>Project-level</strong> documentation includes information about the processes used throughout the project, including how you and your collaborators are collecting, organizing, and analyzing your data.</li> <li> <strong>File-level</strong> documentation includes details related to individual files.</li> </ul> <p>Documentation can be maintained in a variety of forms. Some common forms of documentation are:</p> <ul> <li> <strong>Readme</strong> — A <a href="http://Readme.md">Readme.md</a> file is a text file located in a project-related folder that describes the contents and structure of the folder and/or a dataset so that a researcher can locate the information they need. The .md extension refers to markdown, a text language easy to write and easy to interpret.</li> <li> <strong>Data Dictionary</strong> — Also known as a codebook, a data dictionary defines and describes the elements of a dataset so that it can be understood and used at a later date.</li> <li> <strong>Protocol</strong> — A protocol describes the procedure(s) or method(s) used in the implementation of a research project or experiment.</li> <li> <strong>Lab Notebook</strong> — For research groups that use them, lab notebooks are often the primary record of the research process. They are used to document hypotheses, experiments, analyses, and interpretations of experiments.</li> <li> <strong>Metadata</strong> — Metadata is data about data. Metadata often conforms to a specific scheme- a set of standardized rules about how the metadata is organized and used. There are different types of metadata:<br/>- descriptive metadata (information about the content of your data)<br/>- structural metadata (information about the physical structure of your data, including file format)<br/>- administrative metadata (information about how and when your data was created).</li> </ul> <p>There are a variety of ways to maintain documentation related to your research. It can be as straightforward as developing a regular practice of documenting your process in a Google Document or as formal as maintaining a formal lab notebook. <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">The TIER protocol</a> has a guideline for the location of documentation. Please check it for more details.</p> <p>Create a comprehensive <a href="http://README.md">README.md</a> file at the root of your project directory. This file should include:</p> <ul> <li>Project title and brief description</li> <li>Authors’ information and contact details</li> <li>Project goals and objectives</li> <li>Installation instructions and necessary software</li> <li>Usage guidelines</li> <li>Description of the computational environment</li> <li>Overview of the project structure</li> <li>Links to relevant publications or external resources</li> </ul> <p>Create <a href="http://README.md">README.md</a> files in each major subfolder (e.g., data/, scripts/, docs/) to provide specific information about the contents and purpose of that directory.</p> <p>Implement standardized metadata documentation for various components of your project:</p> <ol> <li> <a href="https://huggingface.co/docs/hub/model-cards">Model Cards</a>: For machine learning projects, create model cards that describe<br/>- Model architecture<br/>- Training data<br/>- Performance metrics<br/>- Intended use and limitations<br/>- Ethical considerations</li> <li> <a href="https://huggingface.co/docs/hub/datasets-cards">Dataset Cards</a>: Document your datasets with cards that include<br/>- Dataset description and purpose<br/>- Data collection methodology<br/>- Data structure and schema<br/>- Data quality and preprocessing steps<br/>- Ethical considerations and potential biases</li> <li> <a href="https://huggingface.co/docs/datasets/main/en/repository_structure">Repository Structure Cards</a>: Create a card that outlines the overall structure of your repository, including<br/>- Main directories and their purposes<br/>- Key files and their functions<br/>- Relationships between different components</li> </ol> <p>Create a comprehensive <a href="https://laneguides.stanford.edu/DataManagement/Documenting">data dictionary</a> that defines and describes all variables in your datasets. Include:</p> <ul> <li>Variable names</li> <li>Data types</li> <li>Units of measurement</li> <li>Possible values or ranges</li> <li>Description of what each variable represents</li> </ul> <p>Implement automatic report generation to document your analysis process and results:</p> <ul> <li>Use tools like <a href="https://jupyter.org/">Jupyter</a> notebooks or <a href="https://bookdown.org/yihui/rmarkdown/notebook.html">R Notebooks</a> to create reproducible reports</li> <li>Include data visualizations, statistical summaries, and interpretations</li> <li>Ensure reports are generated automatically as part of your workflow</li> <li>Store reports in a dedicated directory (e.g., /reports)</li> </ul> <p>Containers offer a lightweight, consistent environment for running applications across diverse computing systems, ensuring that software behaves identically regardless of where it’s deployed. This is a big step for research reproducibility, enabling exact replication of computational environments.</p> <p>While tools like virtual environments and version control support reproducibility, containerization goes further. It creates a self-contained setup that includes code, data, and all system dependencies, eliminating inconsistencies across systems. This consistency simplifies collaboration, eases deployment to different platforms, and aids in the long-term preservation of research work.</p> <p>For more information, check <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316">Nüst et al (2020) <strong>Ten simple rules for writing Dockerfiles for reproducible data science.</strong></a></p> <p><a href="https://www.docker.com/">Docker</a>, a popular containerization platform, is particularly advantageous for research. It offers cross-platform compatibility, enabling work across different operating systems. With a robust ecosystem and community support, Docker provides access to a vast array of pre-built images and tools. Compared to traditional virtual machines, Docker containers are more resource-efficient, making them ideal for intensive computations. By running analyses in identical environments, Docker enhances reproducibility, isolates projects, manages version control of environments, and facilitates collaboration through shareable container images.</p> <p>To start using Docker in your research:</p> <ol> <li>Install Docker Desktop.</li> <li>Create a Dockerfile by<br/>- Specifying the base image and project dependencies or<br/>- <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316">Following Nüst et al. recipe</a> </li> <li>Decide whether to copy your project into the container or set the container to pull from the project from the git remote repository.</li> <li>Build the Docker image via the command line.</li> <li>Run your analysis in the container for consistent, isolated execution.</li> <li>Share your image on Docker Hub or share the recipe on your research repository (e.g., <a href="http://OSF.io">OSF.io</a>).</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/320/0*tnK5ADbjlpVu83Ov"/><figcaption><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316">Nüst et al (2020) <strong>Ten simple rules for writing Dockerfiles for reproducible data science</strong></a></figcaption></figure> <p>Functional package managers offer a different approach to managing software environments. Unlike traditional package managers that construct environments through a sequence of steps, functional package managers use mathematical functions to define and build software environments.</p> <p><strong>Traditional package managers</strong> (e.g., Debian’s, <a href="https://wiki.debian.org/Apt">apt</a>, Windows’ <a href="https://chocolatey.org/">chocolatey</a>, macOS’ <a href="https://brew.sh/">homebrew</a>) build environments sequentially, making results order-dependent and often non-reproducible.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/492/0*uNpKzJJ3-QC3GUEM.png"/><figcaption>Python Environment — by xkcd.com</figcaption></figure> <p><strong>Functional package managers</strong> (e.g., <a href="https://guix.gnu.org/">Guix</a>, <a href="https://medium.com/@zmre/the-package-manager-to-rule-them-all-9a8829e4f392">Nix</a>) construct environments as mathematical functions, yielding reproducible builds and greater dependency control. As a consequence, they offer the possibility to rollback to previous versions of dependencies and provide access to the history of versions. Each version is pulled from a package repository called <a href="https://nixos.org/manual/nixpkgs/stable/">Nixpkgs</a>.</p> <p>Key concepts:</p> <ol> <li>Environments are defined as <a href="https://nixos.org/manual/nixpkgs/stable/"><strong>sums</strong></a> of packages</li> <li>Packages function as <strong>mathematical constructs</strong> based on source code, build environment, and scripts.</li> <li>Source code is precisely <strong>identified using cryptographic hashes</strong> </li> </ol> <p>This method addresses challenges such as:</p> <ul> <li>Ensuring precise references to software versions</li> <li>Managing complex dependency graphs</li> <li>Eliminating assumptions about standard software locations</li> </ul> <p>Nix for instance can be installed on the local machine or in a container such as Docker, for easier dependencies management.</p> <p>For more arguments in favor of functional package managers as an alternative to the many-managers situation, please refer to <a href="https://learninglab.gitlabpages.inria.fr/mooc-rr/mooc-rr2-ressources/module2/seq7-conclusion/unit1-lecture.html">this article</a>.</p> <p>Workflows bring structure and automation to complex computational processes in research, transforming scripts into reproducible, organized sequences of tasks. Benefits include:</p> <ol> <li> <strong>Automated task execution:</strong> Steps run automatically when a file hash changes, ensuring tasks are updated only when necessary.</li> <li> <strong>Clear dependencies and data flow:</strong> Each task and data movement is explicitly defined, making the process easy to understand and maintain.</li> <li> <strong>Enhanced reproducibility:</strong> Automated workflows create consistent outputs by reproducing the exact sequence of operations every time.</li> <li> <strong>Parallel processing and scalability:</strong> Tasks can be run in parallel or distributed across systems, accelerating data-intensive processes.</li> <li> <strong>Improved error handling:</strong> Automated workflows detect and isolate errors, allowing efficient recovery and debugging.</li> </ol> <p>Workflows can be seen as an evolution of computational notebooks, offering more structure and better handling of complex, data-intensive tasks.</p> <p>Workflow systems range from simple task automation to complex pipelines:</p> <ol> <li>Batch Processing:<br/>- Manually set up processing steps by specifying the modules to run, the data type, dependencies between modules, and their execution order (e.g., SPM batch for fMRI analysis).</li> <li> <strong>Task Automation:<br/>- </strong><em>Makefile:</em> Local automation for simple dependencies<br/>- <em>Snakemake:</em> Python-based, ideal for scientific workflows</li> <li> <strong>CI/CD Tools:<br/>- </strong><em>GitHub Actions:</em> YAML-based workflows integrated with GitHub<br/>- <em>CircleCI:</em> CI/CD with strong Docker support and customizable pipelines</li> <li> <strong>Data-Intensive and Specialized Workflow Managers:<br/>- </strong><em>Nextflow:</em> Parallel scientific workflows, supports cloud/HPC<br/>- <em>Cromwell, Luigi, Prefect:</em> Specialized for bioinformatics, genomics, data workflows</li> <li> <strong>Hybrid Notebook-Workflow Systems:<br/>- </strong><em>SOS-notebook:</em> Blends interactive notebooks with structured workflows</li> </ol> <p>Each tool offers different levels of complexity and scalability, chosen based on the project’s computational needs.</p> <p>Implementing workflows in research projects can be approached in several ways:</p> <ol> <li> <strong>From Scripts to Simple Workflows:<br/>- </strong>Use Make or Snakemake to automate tasks and define dependencies<br/>- Modularize scripts for clarity and scalability</li> <li> <strong>Adopt CI/CD Practices:<br/>- </strong>Use version control (e.g., Git)<br/>- Automate testing and deployment with tools like GitHub Actions</li> <li> <strong>Full Workflow Management:<br/>- </strong>Select a domain-specific workflow manager (e.g., Nextflow for bioinformatics)<br/>- Organize the research process into modules, orchestrated by the workflow tool</li> <li> <strong>Container-Based Workflows:<br/>- </strong>Start in a containerized environment (e.g., Docker)<br/>- Define the environment with a Dockerfile<br/>- Use tools like Docker Compose to manage multiple services and integrate with workflows for robust reproducibility.</li> </ol> <p>Appreciate the simplicity of recovering results with a few commands.</p> <p>From my row house in DC,</p> <p>Merci pour votre temps</p> <p>Et à bientôt,</p> <ul> <li>National Academies of Sciences, Policy, Global Affairs, Board on Research Data, Information, Division on Engineering, … &amp; Replicability in Science. (2019). <em>Reproducibility and replicability in science</em>. National Academies Press.</li> <li>Sandve, G. K., Nekrutenko, A., Taylor, J., &amp; Hovig, E. (2013). Ten simple rules for reproducible computational research. <em>PLoS computational biology</em>, <em>9</em>(10), e1003285.</li> <li>Nüst, D., Sochat, V., Marwick, B., Eglen, S. J., Head, T., Hirst, T., &amp; Evans, B. D. (2020). Ten simple rules for writing Dockerfiles for reproducible data science. <em>PLoS computational biology</em>, <em>16</em>(11), e1008316.</li> </ul> <blockquote> <strong>Author:</strong><em><br/>Valentin Guigon, PhD<br/>Postdoctoral researcher,<br/></em><a href="https://sldlab.umd.edu/m/"><em>Social Learning and Decisions lab</em></a><em><br/>University of Maryland, College Park, MD, USA</em> </blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f8d6c58e81cf" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="open-science"/><category term="academia"/><category term="mlops"/><category term="research"/></entry><entry><title type="html">Towards learning in public</title><link href="https://valentinguigon.github.io/posts/2024-10-17-towards_public/" rel="alternate" type="text/html" title="Towards learning in public"/><published>2024-10-17T00:00:00+00:00</published><updated>2024-10-17T00:00:00+00:00</updated><id>https://valentinguigon.github.io/posts/towards_public</id><content type="html" xml:base="https://valentinguigon.github.io/posts/2024-10-17-towards_public/"><![CDATA[<h2 id="argument">Argument</h2> <p>As with a multi-disciplinary approach, continuous learning and creation has become integral to my work and to my life. This necessitates the organization of knowledge, the construction of conceptual relationships, and the implementation of experimental testing. The result is a corpus of knowledge that I perceive as valuable and undergoing continuous improvement. By contrast with both traditional scholarship (i.e., paper) and informational streams (i.e., web 2.0 and social medias feeds), and in line with computer and web practices (e.g., hypertextual web, versioning &amp; open source), this corpus is topographical – it belongs to a network of connected nodes, impermanent – subject to change, and multimedia – incorporates text, images, code and videos. I believe this body of work also holds collective value.</p> <p>The tendency to guard ideas to prevent theft or premature criticism is widespread, especially in the academia. I’m now grasping how sharing knowledge openly can yield significant benefits for both the individual and the broader community. This practice aligns with open science principles and contributes to the public knowledge base, while also serving as a valuable professional asset. Open sharing imposes a sense of responsibility for the quality of one’s work. It invites scrutiny and feedback. Knowledge also evolves rapidly, and its true value lies in the ongoing development of ideas rather than in static snapshots. Thus, it makes sense to publish ongoing work instead of waiting for polished pieces. It also means that any snapshot will inevitably be outdated. Hence, it advertises for its author rather than the snapshot itself.</p> <p>The /Posts section of this website serves as the platform for publishing content from this evolving corpus of knowledge. It embodies the practice of <a href="https://www.swyx.io/learn-in-public">learning in public</a>, where one shares their learning process in real-time. This approach holds the learner accountable and encourages the production of higher-quality content by putting one’s reputation on the line. It aligns perfectly with the ethos of open practices, including the open science principles I advocate for.</p> <p><a href="https://maggieappleton.com/garden-history">Digital gardening</a> offers an interesting philosophy for sharing such personal knowledge. At its core, digital gardening involves publishing imperfect, continuously revised, and interlinked personal knowledge on the web. This practice is inherently experimental, multi-disciplinary, and personal. Knowledge is shared at early-stage ideas, necessitating a commitment to transparency.</p> <p>In line with these principles, the /Posts section of the website comprises two categories: polished articles published on external platforms (e.g., medium.com) and non-definitive notes published locally. Each post is tagged with relevant concepts, allowing the section to evolve into a network of contextual relationships between ideas and content. To maintain transparency about each post’s status, I include creation dates, update information, status indicators, confidence levels, and effort estimates wherever possible. This approach is intended to reflect the developmental aspect of the posts. Specifically, status, confidence and effort are meant as categorical and continuous epistemic evaluations of content:</p> <ul> <li>Status: baby, child, adult</li> <li>Effort: low, moderate, high</li> <li>Confidence: from 1 to 100 (in %)</li> </ul> <p>Ultimately, this approach serves as a learning tool, a means to organize and develop thoughts, a showcase of skills, and a contribution to the broader community. With the goal of disseminating knowledge, advocating for cooperation and fostering trust among peers (See <a href="https://www.sciencedirect.com/science/article/abs/pii/S0065260122000144">Van Bavel, Pärnamets, Reinero and Packer, 2022</a>).</p> <p>Thank you for your time,</p> <hr/> <h2 id="bibliography">Bibliography:</h2> <ul> <li>Learning in public. Shawn Wang. https://www.swyx.io/learn-in-public</li> <li>A Brief History &amp; Ethos of the Digital Garden. Maggie Appleton. https://maggieappleton.com/garden-history</li> <li>Van Bavel, J. J., Pärnamets, P., Reinero, D. A., &amp; Packer, D. (2022). How neurons, norms, and institutions shape group cooperation. In Advances in experimental social psychology (Vol. 66, pp. 59-105). Academic Press.</li> </ul>]]></content><author><name></name></author><category term="notes"/><category term="open-science"/><category term="open data"/><category term="learning in public"/><summary type="html"><![CDATA[Why choosing to publish notes in public.]]></summary></entry><entry><title type="html">Reproducibility in academic research (1/3): Key components of a Reproducible Research</title><link href="https://valentinguigon.github.io/articles/2024-07-02-reproducibility-in-academic-research-13-key-components-of-a-reproducible-research/" rel="alternate" type="text/html" title="Reproducibility in academic research (1/3): Key components of a Reproducible Research"/><published>2024-07-02T00:07:25+00:00</published><updated>2024-07-02T00:07:25+00:00</updated><id>https://valentinguigon.github.io/articles/2024-07-02-reproducibility-in-academic-research-13-key-components-of-a-reproducible-research</id><content type="html" xml:base="https://valentinguigon.github.io/articles/2024-07-02-reproducibility-in-academic-research-13-key-components-of-a-reproducible-research/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*PiHLhQu3WMjpszrx"/><figcaption>Photo by <a href="https://unsplash.com/@lazycreekimages?utm_source=medium&amp;utm_medium=referral">Michael Dziedzic</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>This guide took roots from the ethos of Open Science. It is the first part of a three-parts series about reproducibility in academic research, specifically focusing on computational analysis. Any part is susceptible to changes and updates. It is based on my practices as a PhD and postdoctoral researcher in the academia and takes inspiration on DevOps and MLOps.</p> <p>As a disclaimer, I make a <a href="https://www.ncbi.nlm.nih.gov/books/NBK547531/">distinction between <em>reproducibility </em>and <em>replicability</em></a>. In my own understanding, replication is about testing the robustness of findings across different studies and contexts, addressing broader questions about the reliability of scientific knowledge. It is an epistemological problem. Reproducibility is about ensuring that the same computational results can be achieved using the same data and methods. It is an information technology problem. I won’t tackle the question of <em>replicability</em>, as it is a different conversation that has been addressed by people way more qualified than I am.</p> <p>For readers familiar with the points illustrated in the Table of contents but looking for a hands-on, I’d like to point to a great and <a href="https://rse.shef.ac.uk/blog/2022-05-05-concise-guide-to-reproducible-matlab/">concise guide to reproducible MATLAB projects</a> by David Wilby. Until the next post, in which we should focus on building a practical research workflow.</p> <p>I’ve been writing this post to the best of my ability. If you feel that are present mistakes, malpractices or imprecisions, or if you’d like to see something added, please feel free to comment or to contact me. If you are pleased with this post, please feel free to do as well.</p> <ul> <li>Argument</li> <li>What is Reproducibility</li> <li>FAIR principles</li> <li>Stable computational environments</li> <li>The example of Compute Capsules</li> <li>Key components of a Reproducible Research</li> <li>Reproducibility is attained at various levels</li> <li>Conclusion</li> <li>Bibliography</li> </ul> <p>Reproducibility is a cornerstone of scientific integrity, ensuring that research findings can be independently verified and built upon by other researchers. In recent years, the principles of <a href="https://www.phdata.io/blog/mlops-vs-devops-whats-the-difference/">DevOps and MLOps</a> have offered critical frameworks for deploying stable products in the industry. Coupled with the ethos of Open Science, I believe they offer valuable, if not crucial, frameworks for enhancing reproducibility in research. These frameworks emphasize automation, version control, continuous integration, and collaborative practices, which are crucial for managing the complexity of modern scientific research. The increasing <a href="https://about.gitlab.com/blog/2022/02/15/devops-and-the-scientific-process-a-perfect-pairing/">use of Git in the academia</a> shows that researchers are willing to incorporate these frameworks in their workflow.</p> <p>This blog post addresses the question of producing a reproducible research, outlines foundational principles, showcases an industry example, proposes a series of practices, and provides theoretical strategies for achieving it. By leveraging best practices from software development and data science, researchers can improve the reliability and transparency of their work. We will go over practical strategies in another blogpost.</p> <p>Reproducibility refers to the ability of a research study to be repeated with the same results by other researchers using the same methodology, data, and conditions. It is a critical aspect of the scientific process, providing a means to validate findings and build trust within the scientific community. The question of reproducibility seems to be mostly known nowadays through the “replicability crisis”. A term familiar to pretty much any researcher studying human and animal behavior and biological processes. As such, reproducibility is a critical component of the construction of knowledge.</p> <p>Reproducibility is also a critical components of opening the Science. Open Science requires a set of practices that are now part of global research policies and, as such, will become the standard in academic research. To quote the <a href="https://rea.ec.europa.eu/open-science_en">European Research Executive Agency</a>, “<em>Open Science is an approach to research based on open cooperative work that emphasizes the sharing of knowledge, results and tools as early and widely as possible.</em> <em>It is mandatory under Horizon Europe, and it operates on the principle of being ‘as open as possible, as closed as necessary’ ”. </em>Its purpose is <em>to foster greater transparency and trust for the benefit of scientific research and for the benefit of EU citizens .</em> More information on the 2020–2024 strategy <a href="https://research-and-innovation.ec.europa.eu/strategy/strategy-2020-2024/our-digital-future/open-science_en">here</a>. Among those practices, any Horizon Europe research project must manage open-access data and <a href="https://www.openaire.eu/how-to-comply-with-horizon-europe-mandate-for-rdm"><em>provide information about any research output or any other tools and instruments needed to re-use or validate the data</em></a>.</p> <p>To reproduce a research <em>using the same methodology, data and conditions</em> means having the means to freeze a research study in time. To reach such an ideal, data should not incur any loss or alteration between the original publication and the replication; methodology, including toolboxes/packages and scripts, should not change between the original publication and the replication; conditions, including the versions of softwares, the seeds and the computational operations, should be stable between the original publication and the replication. To stay close as much as possible to these ideals, FAIR principles offer theoretical guidelines. Together, they are the first pillar of a Reproducible Research.</p> <p>The FAIR principles originate from a paper published in 2016 on <a href="https://www.nature.com/articles/sdata201618">Nature</a> by a consortium of researchers. They provide guidelines for data management and stewardship, ensuring that data are:</p> <ol> <li> <strong>Findable</strong>: The first step in (re)using data is to find it! Descriptive metadata (information about the data such as keywords) is essential.</li> <li> <strong>Accessible</strong>: Once the user finds the data and software they need to know how to access it. Data could be openly available but it is also possible that authentication and authorisation procedures are necessary.</li> <li> <strong>Interoperable</strong>: Data needs to be integrated with other data and interoperate with applications or workflows.</li> <li> <strong>Reusable</strong>: Data should be well-described so that they can be used, combined, and extended in different settings.</li> </ol> <figure><img alt="Visual picture of the 4 FAIR principles." src="https://cdn-images-1.medium.com/max/768/1*lR4Nd5csy-ATh-H03LEzIg.jpeg"/><figcaption>Image taken from <a href="https://libereurope.eu/article/fairdataconsultation/">libereurope.eu</a>.</figcaption></figure> <p>To be more specific, FAIR principles is concerned by a protocol, its related data, the description of data by metadata, and on their indexing in a searchable source. According to their authors, “<em>the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals” </em>(<a href="https://www.nature.com/articles/sdata201618">Wilkinson et al., 2016</a>). As we rely on computational supports to deal with data, we need the capacity of computational systems to find, access, interoperate, and reuse data, with none or minimal human intervention. <a href="https://the-turing-way.netlify.app/reproducible-research/rdm/rdm-fair.html">More on that topic here.</a></p> <p>As taken from <a href="https://www.go-fair.org/fair-principles/">go-fair.org</a>, FAIR principles for data refer:</p> <h4>1. To be findable:</h4> <ul> <li>F1. (meta)data are assigned a globally unique and persistent identifier (PID)</li> <li>F2. data are described with rich metadata (defined by R1 below)</li> <li>F3. metadata clearly and explicitly include the identifier of the data it describes</li> <li>F4. (meta)data are registered or indexed in a searchable resource</li> </ul> <p>Assigning a persistent identifier and adding metadata to data can be achieved by depositing the data in a data repository such as <a href="https://figshare.com/">Figshare</a>, <a href="https://osf.io/">Open Science Framework</a> (OSF), <a href="https://zenodo.org/">Zenodo</a>, or else. Mind to look carefully into the advantages and limits of each repository as ethical or legal obligations may constraint to using a platform over another.</p> <h4>2. To be Accessible:</h4> <ul> <li>A1. (meta)data are retrievable by their persistent identifier (PID) using a standardized communications protocol</li> <li>A1.1 the protocol is open, free, and universally implementable</li> <li>A1.2 the protocol allows for an authentication and authorization procedure, where necessary</li> <li>A2. metadata are accessible, even when the data are no longer available</li> </ul> <p>Platforms such as <a href="https://figshare.com/">Figshare</a>, <a href="https://osf.io/">OSF</a>, <a href="https://zenodo.org/">Zenodo</a> provide storage for light datasets. For larger datasets, one could look for large file storage solutions (such as <a href="https://git-lfs.com/">LFS</a>, not recommended, <a href="https://dagshub.com/pricing">Dags Hub</a>, or <a href="https://lakefs.io/blog/dvc-vs-git-vs-dolt-vs-lakefs/">else</a>).</p> <h4>3. To be Interoperable:</h4> <ul> <li>I1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.</li> <li>I2. (meta)data use vocabularies that follow FAIR principles</li> <li>I3. (meta)data include qualified references to other (meta)data</li> </ul> <p>Resources such as <a href="http://fairsharing.org/">FAIRsharing.org</a>, the <a href="https://www.ebi.ac.uk/ols4">Ontology Lookup Service</a>, <a href="https://lov.linkeddata.es/dataset/lov">Linked Open Vocabularies</a> can be consulted to identify suitable standards. Standard data formats describe how the data needs to be formatted, thereby simplifying the use of data.</p> <h4>4. To be Reusable:</h4> <ul> <li>R1. meta(data) are richly described with a plurality of accurate and relevant attributes</li> <li>R1.1. (meta)data are released with a clear and accessible data usage license</li> <li>R1.2. (meta)data are associated with detailed provenance</li> <li>R1.3. (meta)data meet domain-relevant community standards</li> </ul> <p>Documentation and metadata will inform a potential user of that data on how, why, by whom and when the data were created, allowing the user to judge whether the data is relevant for the intended reuse.</p> <p>Great documentation on FAIR principles can be found on the dedicated <a href="https://www.ugent.be/en/research/openscience/datamanagement/after-research/fair-data.htm">Ugent.be</a> webpages. In particular, they insist on the FAIR principles as something we should aim for permanently: “<em>Just like there are various degrees of data sharing, FAIR is also a spectrum. In other words, data can be FAIR to a greater or lesser extent.</em>”</p> <p><a href="http://This%20structure%20is%20adapted%20from%20the%20TIER%20protocol%204.0%20(https://www.projecttier.org/tier-protocol/protocol-4-0/root/).">Projecttier.org</a> provides some guidelines on how to structure a project (folders, files, metadata) respecting the FAIR principles.</p> <p>The second pillar of a Reproducible Research is a stable computational environment. To maintain data that incurred no loss nor alteration, to operate with the same methodology and to work in the same conditions across time, one should establish a stable computational environment. Ideally, this is achieved by building a contained and self-sufficient project and working within snapshots.</p> <p>Many tools and operations are readily available to researchers to build those environments. We will go over some of those in the next section. Leveraging these tools and operations require a thoughtful process, that may appear costly at first. However, scientific journals often require now to submit what could be understood as a <em>replication package</em>. For instance, as put by Psychological Science on their <a href="https://www.psychologicalscience.org/publications/psychological_science/ps-submissions#comp">submission guidelines</a>, “<em>all manuscripts submitted to Psychological Science are expected to be computationally reproducible. That is, a reader should be able to run the authors’ code on the authors’ data and reproduce the results reported in the manuscript, tables, and figures”.</em></p> <p>Data scientists and the industry already tackled the many problems of reproducing outputs by running scripts across different OS, IDEs, versions, persons, times, etc. This is what we refer to as DevOps — and MLOps in the specific case of Machine Learning. Solutions for researchers piecing together every key ingredients of reproducibility practices have not generalized yet. Likely one better change its own practices now. Nonetheless, because many of those tools and operations limit human error and operate on standardization and automatization, the long-term gains are not only virtually higher citations but also lower time consumption. By building a sound foundation for your current or your next research project, you will obtain a layout for future research and reduce its complexity.</p> <p>To give a sense of what it would look like as a single package, I’d like to present <a href="http://codeocean.com">Code Ocean</a>, an example of a market solution to ensure Reproducible Research, tailored for computational scientists in biotechnology. Their solution exemplifies the key ingredients of a Reproducible Research. Called <em>Compute Capsules</em>, their solution relies on 3 cornerstones: Sharability, Traceability and Reproducibility. They recently partnered with Nature and spotlight capsules, as available <a href="https://nature.codeocean.com/">here</a>.</p> <figure><img alt="Diagram of a depiction of a Compute Capsule by Codeocean" src="https://cdn-images-1.medium.com/max/700/1*x34qpfk8zxO3dE0eEgtH3Q.jpeg"/><figcaption>Diagram taken from <a href="https://codeocean.com/resources/nature-partnership.">codeocean.com</a>.</figcaption></figure> <p>Code Ocean provides a platform with a container solution in harmony with FAIR principles and the Open Science ethos<strong>.</strong> Each container is a self-contained version-controlled computational triplet Environment-Code-Data. As <a href="https://codeocean.com/blog/addressing-the-next-big-bottleneck-in-computational-research">Code Ocean puts it</a>, “<em>Compute Capsules are computational “apps” that enable seamless collaboration by allowing researchers to package code, data, the computing environment, and the results. These self-contained executable software packages can then be easily shared with and used by others without the need to install operating systems, libraries, and application packages in the correct versions on all computing machines. The Capsule tracks each element of a computational experiment to ensure full traceability and reproducibility of any experimental result so that you can trust your results, every time.</em>”</p> <p>A more detailed explanation in the video below:</p> <iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F8g9jGtFsFBY%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D8g9jGtFsFBY&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F8g9jGtFsFBY%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/4841d0effeb4c6f8d3be57292ba9426f/href">https://medium.com/media/4841d0effeb4c6f8d3be57292ba9426f/href</a></iframe> <p>As show the <em>Compute Capsules</em>, achieving reproducibility involves several key components, each addressing different aspects of the research process. These components include data management, version control, continuous integration/continuous deployment (CI/CD), documentation, all through adherence to FAIR principles.</p> <p>I propose below a series of components that allows one to build its own stable computational environment, to maintain code and data in compliance with FAIR principles and to align with the ethos of Open Science. As outlined by the <em>Compute Capsule</em>, Code, Data and Environment constitute the essential triplet of computational analysis. This triplet can be represented by your .R scripts, your .csv datafiles, and your RStudio window with which you load the .csv and execute the .R. Or, to go back to our first analogy, our <em>methodology, data and conditions. </em>Each component often address multiple elements of the triplet.</p> <h4>Data Management</h4> <p>FAIR principles merge with Data planning frameworks (data needs, data collection, data documentation, data organization, data storage, data access and data sharing). This is why effective data management is the foundation of reproducible research. It involves organizing, storing, and sharing data in a way that ensures it can be easily accessed by others.</p> <ul> <li> <strong>Data Organization</strong>: Structuring data in a logical and consistent manner, such as suggests <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">Tier Protocol 4.0</a>.</li> <li> <strong>Data Storage</strong>: Using secure and reliable storage solutions to preserve data integrity, as enforced by <a href="https://gdpr-info.eu/">GDPR </a>and <a href="https://www.hhs.gov/hipaa/for-professionals/index.html">HIPAA</a>.</li> <li> <strong>Data Sharing</strong>: Making data available through repositories, ensuring it is accessible for validation and reuse, such as <a href="http://osf.io">OSF</a> for project-level management or <a href="https://figshare.com/">figshare</a> or <a href="https://dagshub.com/">Dags Hub</a> for data-level management.</li> </ul> <h4>Project Organization</h4> <p>Organizing projects in a systematic way helps manage complexity and readability. Operating with a project logic enhances reproducibility across machines or across people. Beyond organizing one’s repository, constructing <em>projects </em>or <em>virtual environments</em> helps one setting up a self-contained environment that can easily be closed and restored, and that can operate with toolboxes/packages versions built from snapshots. Python virtual environments propose both; Matlab only proposes projects, R has both the project building tool and the package renv. I especially recommend reading the <a href="https://rstudio.github.io/renv/articles/renv.html">introduction to renv</a> to understand the importance of these tools.</p> <ul> <li> <strong>Structured Directory Layout</strong>: Using a clear and consistent directory structure, such as suggests <a href="https://www.projecttier.org/tier-protocol/protocol-4-0/root/">Tier Protocol 4.0</a>, with automation tools such as <a href="https://cookiecutter.readthedocs.io/en/latest/overview.html"><em>cookiecutter</em></a><em>.</em> </li> <li> <strong>Project Templates</strong>: Employing templates for common project types to standardize workflows.</li> <li> <strong>Configurable Parameters</strong>: Keeping parameters and settings configurable and documented. For instance, build <a href="https://medium.com/@pawan329/what-is-virtual-environment-in-python-a6a3b46e0515">virtual environments</a> (Python <a href="https://mindthevirt.com/venv-vs-conda-choosing-the-right-python-environment-manager-for-you/">venv or conda</a>, R <a href="https://rstudio.github.io/renv/articles/renv.html">renv</a>) or projects (<a href="https://www.mathworks.com/help/matlab/matlab_prog/create-projects.html">MATLAB .prj</a>, <a href="https://communicate-data-with-r.netlify.app/docs/baser/workingprojects/">R .RProj</a>).</li> </ul> <h4>Coding Practices</h4> <p>Adhering to best coding practices is project organization on the script-level. It ensures that code is readable, maintainable, and reusable. This includes following a consistent coding style, modularizing code, and providing thorough code-level documentation. In particular, one would want to start coding with notebooks as playgrounds and progressively reduce complexity until obtaining a coherent project as multiple scripts.</p> <ul> <li> <strong>Readability</strong>: Writing clear and understandable code.</li> <li> <strong>Notebooks as playgrounds:</strong> Notebooks are nice for exploration and prototype but no one wants to read the whole code and rewrite it.</li> <li> <a href="https://github.com/davified/clean-code-ml/blob/master/docs/refactoring-process.md"><strong>Refactoring the notebook</strong></a><strong>:</strong> Once the prototype is satisfying, abstracting code into functions and classes to compartmentalise the complexity.</li> <li> <a href="https://towardsdatascience.com/modularise-your-notebook-into-scripts-5d5ccaf3f4f3"><strong>Modularity</strong></a>: Breaking down code into reusable modules or functions.</li> <li> <strong>Testing:</strong> Writing code that checks variables behavior and coding errors.</li> <li> <strong>Documentation</strong>: Commenting on code and providing detailed usage examples.</li> </ul> <h4>Project-level documentation</h4> <p>Comprehensive documentation on the level of a project is vital for reproducibility. It includes recording methodologies, parameters, software versions, and any other relevant details that allow others to understand and replicate the study.</p> <ul> <li> <a href="https://huggingface.co/docs/hub/model-cards"><strong>Model cards</strong></a><strong>, </strong><a href="https://huggingface.co/docs/hub/datasets-cards"><strong>dataset cards</strong></a><strong> and </strong><a href="https://huggingface.co/docs/datasets/main/en/repository_structure"><strong>repository structure cards</strong></a><strong>: </strong>Creating cards that document the naming of variables, the components of models and the hierarchical structure of the project.</li> <li> <strong>Generating Documentation</strong>: Automating the creation of documentation, such as <a href="https://swcarpentry.github.io/r-novice-gapminder/15-knitr-markdown.html">results reports</a> by knitting notebooks <em>.ipynb</em> (Python) or <em>nb.html</em> (R) into <em>.html</em>, to reduce human error.</li> <li> <strong>Maintaining Up-to-Date Records</strong>: Keeping documentation current with the latest changes in the project.</li> </ul> <h4>Version Control</h4> <p><a href="https://about.gitlab.com/topics/version-control/">Version control systems</a> like Git are essential for tracking changes in code, data, and documentation. They enable researchers to manage different versions of their work, collaborate effectively, and maintain a complete history of their project. The most used systems are <a href="https://github.com/about">GitHub </a>and <a href="https://about.gitlab.com/">GitLab</a>.</p> <ul> <li> <strong>Tracking Changes</strong>: Recording every modification to the codebase or dataset.</li> <li> <strong>Collaboration</strong>: Facilitating teamwork without conflicts or data loss.</li> <li> <strong>Reverting Changes</strong>: Allowing easy rollback to previous versions if needed.</li> </ul> <h4>Experiment Tracking and Data Version Control</h4> <p>Tracking experiments and maintaining version control for data are crucial for managing the iterative nature of scientific research. To track experiment parameters and metrics, one usually uses <em>.txt</em>, <em>.ppt</em> or <em>.csv</em> tables. For model-based academic research, tools from the MLOps can be used as well. Data versioning is Git applied to datasets. Git was designed for versioning text/code files and is not meant to handle large files, but <a href="https://medium.com/@sachinsoni600517/understanding-dvc-a-practical-guide-to-data-version-control-04c105413ab4">specific solutions such as DVC</a> exist.</p> <ul> <li> <strong>Experiment Tracking Tools</strong>: Using tools like <a href="https://mlflow.org/">MLflow</a> or <a href="https://dagshub.com/pages/fds">other pipelines</a> to log experiments, track metrics, and manage models.</li> <li> <strong>Data Version Control</strong>: Implementing version control for datasets to track changes and ensure data integrity, with tools like <a href="https://dvc.org/">DVC (Data Version Control</a> — a nice and simple tutorial can be found <a href="https://www.youtube.com/watch?v=YbAgxiU1tW0">here</a>).</li> </ul> <h4>Continuous Integration/Continuous Deployment (CI/CD)</h4> <p>CI/CD pipelines automate the testing and deployment of code, ensuring that changes are integrated smoothly and that the codebase remains functional. It is in particular used when pushing code modifications or data modifications on Git. CI/CD is especially useful when new data is regularly added to a database, with a need to automatize pre-processing or processing, to automatize code-testing after code alterations, and to trigger specific workflows of actions. In particular, <a href="https://docs.github.com/en/actions">GitHub</a> has its own tool for triggering workflows (free for public repositories), and <a href="https://www.srcmake.com/home/circleci">Circle CI</a> offers a free solution that integrates well with Git.</p> <ul> <li> <strong>Automated Testing</strong>: Running tests automatically to catch code errors early.</li> <li> <strong>Automated Processing</strong>: Pre-processing/processing new data automatically when pushed to a repository.</li> <li> <strong>Consistent Deployment</strong>: Ensuring that code is <em>deployed</em> in a consistent manner.</li> <li> <strong>Monitoring and Feedback</strong>: Providing immediate feedback on the impact of changes.</li> </ul> <p>Reproducibility requires standardization for the many levels and phases of scientific research. Just like FAIR as a spectrum, a research can be reproducible to a greater or lesser extent<em>. </em>Research publication practices changed the last decade to incorporate transparency practices such as pre-registration, pre-prints and open-source projects. We can go farther on the reproducibility spectrum by practicing at every step of a research project:</p> <ul> <li>FAIR principles for data (data reusability);</li> <li>Containers, version control and documentation (computational environments);</li> <li>Code healthy practices (verifications);</li> <li>Open science attitude such as pre-registration and open publishing (transparency).</li> </ul> <p>You can find a library of resources regarding every step at the <a href="https://www.bitss.org/resource-library/">Berkeley Initiative for Transparency in the Social Sciences</a>.</p> <figure><img alt="Screenshot depicting the research phases, as taken from the Berkeley Initiative for Transparency in the Social Sciences, illustrating in what phases can standardization enforce reproducibility." src="https://cdn-images-1.medium.com/max/1012/1*5vVieHyaDJMLY_4rbpgKNw.jpeg"/><figcaption>Illustration of the many research phases that can benefit from standardization from the <a href="https://www.bitss.org/resource-library/">Berkeley Initiative for Transparency in the Social Sciences</a>.</figcaption></figure> <p>This series of posts originate from the discrepancy I observed between the industry and the academia regarding information technology practices. Plenty resources and tutorials exist for good coding practices, DevOps and MLOps, including here on Medium and on Towards Data Science. However it somehow didn’t disseminate, in my own opinion, to the researchers in the academia. My goal is to help the dissemination by offering a theoretical and a practical workflow.</p> <p>To conclude, I’d like to remind that besides participating to Open Science, tools for a Reproducible Research also help reducing computational costs, storage costs and cognitive costs — reducing the impact of research on the planet and mental health. The final word will go to <a href="https://arxiv.org/pdf/2302.01048">Lanubile, Martinez-Fernandez and Quaranta, 2023</a> with a table excerpted from their pre-print. The authors offer in their pre-print a synthesis of MLOps tools as taught in a program for the Higher Education, i.e., for computational analysis in the academia. The table synthetizes many helpful tools, some of which we addressed in this post; others we’ll address in the next posts. Until then,</p> <figure><img alt="Table from Lanubile et al., representing MLOps contents, practices and tools presented as milestones for teaching MLOps to high education." src="https://cdn-images-1.medium.com/max/995/1*JYrbrdpSTkNZzbcn7Wp7Ow.jpeg"/><figcaption>Table 1 from <a href="https://arxiv.org/pdf/2302.01048">Lanubile, Martinez-Fernandez and Quaranta, 2023</a>.</figcaption></figure> <p>From my row house in DC,</p> <p>A bientôt,</p> <ul> <li>National Academies of Sciences, Policy, Global Affairs, Board on Research Data, Information, Division on Engineering, … &amp; Replicability in Science. (2019). <em>Reproducibility and replicability in science</em>. National Academies Press.</li> <li> <em>Open science</em>. European Research Executive Agency. <a href="https://rea.ec.europa.eu/open-science_en">https://rea.ec.europa.eu/open-science_en</a> </li> <li> <em>How to comply with Horizon Europe Mandate for RDM</em>. OpenAIRE. <a href="https://www.openaire.eu/how-to-comply-with-horizon-europe-mandate-for-rdm">https://www.openaire.eu/how-to-comply-with-horizon-europe-mandate-for-rdm</a> </li> <li>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., … &amp; Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. <em>Scientific data</em>, <em>3</em>(1), 1–9.</li> <li> <em>Fair Data</em>. Universiteit Gent. (2024, January 11). <a href="https://www.ugent.be/en/research/openscience/datamanagement/after-research/fair-data.htm">https://www.ugent.be/en/research/openscience/datamanagement/after-research/fair-data.htm</a> </li> <li> <em>Psychological science submission guidelines</em>. Association for Psychological Science — APS. (2023, December 27). <a href="https://www.psychologicalscience.org/publications/psychological_science/ps-submissions#comp">https://www.psychologicalscience.org/publications/psychological_science/ps-submissions#comp</a> </li> <li>Code Ocean. (2024, June 7). <em>Addressing the next big bottleneck in computational research</em>. Computational science software for biology. <a href="https://codeocean.com/blog/addressing-the-next-big-bottleneck-in-computational-research">https://codeocean.com/blog/addressing-the-next-big-bottleneck-in-computational-research</a> </li> <li>Lanubile, F., Martínez-Fernández, S., &amp; Quaranta, L. (2023, May). Teaching MLOps in higher education through project-based learning. In <em>2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)</em> (pp. 95–100). IEEE.</li> </ul> <blockquote> <strong><em>Author:</em></strong><br/>Valentin Guigon, PhD <br/>Postdoctoral researcher, <br/><a href="https://sldlab.umd.edu/m/">Social Learning and Decisions lab</a> <br/>University of Maryland, College Park, MD, USA</blockquote> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=413f27cd0886" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="articles"/><category term="research"/><category term="academia"/><category term="open-science"/><category term="mlops"/></entry></feed>